{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "363db8bc",
   "metadata": {},
   "source": [
    "## Model build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7de7a6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import os\n",
    "from torch import tensor\n",
    "# from torchmetrics.audio import ScaleInvariantSignalDistortionRatio\n",
    "import concurrent.futures\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bb59076",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv2D,Conv2DTranspose, LeakyReLU, Dropout, concatenate, Multiply\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7047f2bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU for Training\n"
     ]
    }
   ],
   "source": [
    "if tf.config.list_physical_devices('GPU'):\n",
    "    print(\"Using GPU for Training\")\n",
    "    physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0],True)\n",
    "else:\n",
    "    print(\"Using CPU for Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38c3c76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "#definig the convolution block that consist of 2 conv layers.\n",
    "\n",
    "def conv_block(input,no_filter,f_size,BN = False,drop_out = 0):\n",
    "    x = tf.keras.layers.Conv2D(no_filter,f_size, padding = \"same\")(input)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    if BN:\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "    \n",
    "    residual = x\n",
    "    x = tf.keras.layers.Conv2D(no_filter,f_size,padding = \"same\")(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    \n",
    "    if BN :\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "    \n",
    "    if drop_out > 0 :\n",
    "        x = tf.keras.layers.Dropout(drop_out)(x)\n",
    "    x = tf.keras.layers.Add()([x, residual])\n",
    "    return x\n",
    "\n",
    "\n",
    "def att_block(x, g, desired_dimensionality):\n",
    "    x_shape = x.shape\n",
    "    g_shape = g.shape\n",
    "    \n",
    "    \n",
    "    # strides for xl should be 2 to equal the shapes before addition\n",
    "    xl = tf.keras.layers.Conv2D(desired_dimensionality, (1, 1),strides=(2,2), padding=\"same\")(x)\n",
    "    xl = LeakyReLU(alpha=0.2)(xl)\n",
    "    \n",
    "    gl = tf.keras.layers.Conv2D(desired_dimensionality, (1, 1), padding=\"same\")(g)\n",
    "    gl = LeakyReLU(alpha=0.2)(gl)\n",
    "    \n",
    "    xg = tf.keras.layers.Add()([xl, gl])\n",
    "    xg = LeakyReLU(alpha=0.2)(xg)\n",
    "    xg = tf.keras.layers.Conv2D(1, (1, 1), activation=\"sigmoid\", padding=\"same\")(xg)\n",
    "    xg_shape = xg.shape\n",
    "    # Transpose xg to match the shape of x\n",
    "    xg = tf.keras.layers.Conv2DTranspose(1, (x_shape[1], x_shape[2]), strides=(x_shape[1] // xg_shape[1], x_shape[2] // xg_shape[2]), padding='same')(xg)\n",
    "    \n",
    "    output = tf.keras.layers.Multiply()([xg, x])\n",
    "    return output\n",
    "\n",
    "\n",
    "def build_generator(no_filter, input_shape=(224, 224, 3)):\n",
    "    # down-sampling process\n",
    "    inputs = tf.keras.layers.Input(input_shape, dtype=tf.float32)\n",
    "    x1 = conv_block(inputs, no_filter, (3,3), BN=True, drop_out=0.5)\n",
    "    pool1 = tf.keras.layers.MaxPooling2D(2, 2)(x1)\n",
    "\n",
    "    x2 = conv_block(pool1, 2 * no_filter, (3,3), BN=True, drop_out=0.5)\n",
    "    pool2 = tf.keras.layers.MaxPooling2D(2, 2)(x2)\n",
    "\n",
    "    x3 = conv_block(pool2, 4 * no_filter, (3,3), BN=True, drop_out=0.5)\n",
    "    pool3 = tf.keras.layers.MaxPooling2D(2, 2)(x3)\n",
    "\n",
    "    x4 = conv_block(pool3, 8 * no_filter, (3,3), BN=True, drop_out=0.5)\n",
    "    pool4 = tf.keras.layers.MaxPooling2D(2, 2)(x4)\n",
    "\n",
    "    # bottle-neck\n",
    "    x5 = conv_block(pool4, 16 * no_filter, (3,3), BN=True, drop_out=0.5)\n",
    "\n",
    "    # up-sampling layers\n",
    "\n",
    "    x6 = att_block(x4, x5, no_filter * 2)\n",
    "    u6 = tf.keras.layers.Conv2DTranspose(8 * no_filter,(3,3),strides=(2,2) ,padding='same')(x5)\n",
    "    concate1 = tf.keras.layers.Concatenate()([x6, u6])\n",
    "    conv6 = conv_block(concate1, 8 * no_filter, (5,5), BN=True, drop_out=0.5)\n",
    "    \n",
    "    x7= att_block(x3,conv6, no_filter*2)\n",
    "    u7= tf.keras.layers.Conv2DTranspose(4 * no_filter,(3,3),strides=(2,2), padding='same')(conv6) \n",
    "    concate2 = tf.keras.layers.Concatenate()([x7,u7])\n",
    "    conv7 = conv_block(concate2,4*no_filter,(5,5),BN = True,drop_out = 0.5) \n",
    "\n",
    "\n",
    "    x8= att_block(x2,conv7, no_filter*2)\n",
    "    u8= tf.keras.layers.Conv2DTranspose(2 * no_filter,(3,3),strides=(2,2), padding='same')(conv7) \n",
    "    concate3 = tf.keras.layers.Concatenate()([x8,u8])\n",
    "    conv8 = conv_block(concate3,2*no_filter,(3,3),BN = True,drop_out = 0.5) \n",
    "\n",
    "\n",
    "\n",
    "    x9= att_block(x1,conv8, no_filter*2)\n",
    "    u9= tf.keras.layers.Conv2DTranspose(no_filter,(3,3),strides=(2,2),padding='same')(conv8) \n",
    "    concate4 = tf.keras.layers.Concatenate()([x9,u9])\n",
    "    conv9 = conv_block(concate4,no_filter,(3,3),BN = True,drop_out = 0.5) \n",
    "    \n",
    "\n",
    "\n",
    "    conv_final = tf.keras.layers.Conv2D(1, kernel_size=(4,4),dilation_rate=(2, 2), strides=(1,1),activation='sigmoid', padding='same')(conv8)\n",
    "    conv_final =tf.keras.layers.BatchNormalization(axis=3)(conv_final)\n",
    "    conv_final = tf.keras.layers.Conv2DTranspose(1,(3,3),strides=(2,2), activation=\"sigmoid\", padding='same')(conv_final)\n",
    "    \n",
    "    output = Multiply()([conv_final, inputs])\n",
    "\n",
    "    return tf.keras.Model(inputs, output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d59c8581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_generator(input_shape):\n",
    "#     # Encoder\n",
    "#     inputs = Input(input_shape)\n",
    "#     conv1 = Conv2D(16, (5, 5), strides=(2, 2), padding='same')(inputs)\n",
    "#     conv1 = LeakyReLU(alpha=0.2)(conv1)\n",
    "#     conv1 = Dropout(0.5)(conv1)\n",
    "\n",
    "#     conv2 = Conv2D(32, (5, 5), strides=(2, 2), padding='same')(conv1)\n",
    "#     conv2 = LeakyReLU(alpha=0.2)(conv2)\n",
    "#     conv2 = Dropout(0.5)(conv2)\n",
    "\n",
    "#     conv3 = Conv2D(64, (5, 5), strides=(2, 2), padding='same')(conv2)\n",
    "#     conv3 = LeakyReLU(alpha=0.2)(conv3)\n",
    "#     conv3 = Dropout(0.5)(conv3)\n",
    "\n",
    "#     conv4 = Conv2D(128, (5, 5), strides=(2, 2), padding='same')(conv3)\n",
    "#     conv4 = LeakyReLU(alpha=0.2)(conv4)\n",
    "#     conv4 = Dropout(0.5)(conv4)\n",
    "\n",
    "#     conv5 = Conv2D(256, (5, 5), strides=(2, 2), padding='same')(conv4)\n",
    "#     conv5 = LeakyReLU(alpha=0.2)(conv5)\n",
    "#     conv5 = Dropout(0.5)(conv5)\n",
    "\n",
    "#     conv6 = Conv2D(512, (5, 5), strides=(2, 2), padding='same')(conv5)\n",
    "#     conv6 = LeakyReLU(alpha=0.2)(conv6)\n",
    "#     conv6 = Dropout(0.5)(conv6)\n",
    "\n",
    "#     # Decoder\n",
    "#     up1 = Conv2DTranspose(256, (5, 5), strides=(2, 2), padding='same')(conv6)\n",
    "#     up1 = LeakyReLU(alpha=0.2)(up1)\n",
    "#     merge1 = concatenate([conv5, up1], axis=3)  # Changed axis to concatenate along the channel dimension\n",
    "#     merge1 = Dropout(0.5)(merge1)\n",
    "\n",
    "#     up2 = Conv2DTranspose(128, (5, 5), strides=(2, 2), padding='same')(merge1)\n",
    "#     up2 = LeakyReLU(alpha=0.2)(up2)\n",
    "#     merge2 = concatenate([conv4, up2], axis=3)  # Changed axis to concatenate along the channel dimension\n",
    "#     merge2 = Dropout(0.5)(merge2)\n",
    "\n",
    "#     up3 = Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same')(merge2)\n",
    "#     up3 = LeakyReLU(alpha=0.2)(up3)\n",
    "#     merge3 = concatenate([conv3, up3], axis=3)  # Changed axis to concatenate along the channel dimension\n",
    "#     merge3 = Dropout(0.5)(merge3)\n",
    "\n",
    "#     up4 = Conv2DTranspose(32, (5, 5), strides=(2, 2), padding='same')(merge3)\n",
    "#     up4 = LeakyReLU(alpha=0.2)(up4)\n",
    "#     merge4 = concatenate([conv2, up4], axis=3)  # Changed axis to concatenate along the channel dimension\n",
    "#     merge4 = Dropout(0.5)(merge4)\n",
    "\n",
    "#     up5 = Conv2DTranspose(16, (5, 5), strides=(2, 2), padding='same')(merge4)\n",
    "#     up5 = LeakyReLU(alpha=0.2)(up5)\n",
    "#     merge5 = concatenate([conv1, up5], axis=3)  # Changed axis to concatenate along the channel dimension\n",
    "#     merge5 = Dropout(0.5)(merge5)\n",
    "\n",
    "#     # Add the final convolution layer\n",
    "#     conv = Conv2D(1, (4, 4), dilation_rate=(2, 2), activation='sigmoid', padding='same')(merge5)\n",
    "\n",
    "#     output = Conv2DTranspose(1, (5, 5), strides=(2, 2), activation=\"sigmoid\", padding='same')(conv)\n",
    "#     output = Multiply()([output, inputs])\n",
    "\n",
    "#     model = Model(inputs=inputs, outputs=output)\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45718205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator\n",
    "def build_discriminator(input_shape):\n",
    "    model = models.Sequential()\n",
    "\n",
    "    model.add(layers.Conv2D(16, (3, 3), padding='same', input_shape=input_shape))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.ReLU())\n",
    "\n",
    "    model.add(layers.Conv2D(32, (3, 3), strides=(2, 2), padding='same'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.ReLU())\n",
    "\n",
    "    model.add(layers.Conv2D(64, (3, 3), strides=(2, 2), padding='same'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.ReLU())\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "345a74b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the input shape for only autoencoder\n",
    "# input_shape = (128, 64, 1)\n",
    "\n",
    "# # Create the U-Net model\n",
    "# model = build_generator(input_shape)\n",
    "\n",
    "\n",
    "# # Print the model summary\n",
    "# # model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25a44723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input shape for adversial autoencoder\n",
    "input_shape = (128, 64, 1)\n",
    "\n",
    "# Build generator and discriminator\n",
    "generator = build_generator(16,input_shape=(128,64,1) )\n",
    "discriminator = build_discriminator(input_shape)\n",
    "\n",
    "# Build GAN model\n",
    "discriminator.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy')\n",
    "generator.compile(optimizer=Adam(learning_rate=0.0001), loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f33ff08c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 128, 64, 1)  0           []                               \n",
      "                                ]                                                                 \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 128, 64, 16)  416         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " leaky_re_lu (LeakyReLU)        (None, 128, 64, 16)  0           ['conv2d[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 128, 64, 16)  64         ['leaky_re_lu[0][0]']            \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 128, 64, 16)  6416        ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " leaky_re_lu_1 (LeakyReLU)      (None, 128, 64, 16)  0           ['conv2d_1[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 128, 64, 16)  64         ['leaky_re_lu_1[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 128, 64, 16)  0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 128, 64, 16)  0           ['dropout[0][0]',                \n",
      "                                                                  'batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2D)   (None, 64, 32, 16)   0           ['add[0][0]']                    \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 64, 32, 32)   12832       ['max_pooling2d[0][0]']          \n",
      "                                                                                                  \n",
      " leaky_re_lu_2 (LeakyReLU)      (None, 64, 32, 32)   0           ['conv2d_2[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 64, 32, 32)  128         ['leaky_re_lu_2[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 64, 32, 32)   25632       ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " leaky_re_lu_3 (LeakyReLU)      (None, 64, 32, 32)   0           ['conv2d_3[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 64, 32, 32)  128         ['leaky_re_lu_3[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 64, 32, 32)   0           ['batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 64, 32, 32)   0           ['dropout_1[0][0]',              \n",
      "                                                                  'batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPooling2D)  (None, 32, 16, 32)  0           ['add_1[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 32, 16, 64)   51264       ['max_pooling2d_1[0][0]']        \n",
      "                                                                                                  \n",
      " leaky_re_lu_4 (LeakyReLU)      (None, 32, 16, 64)   0           ['conv2d_4[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 32, 16, 64)  256         ['leaky_re_lu_4[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 32, 16, 64)   102464      ['batch_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " leaky_re_lu_5 (LeakyReLU)      (None, 32, 16, 64)   0           ['conv2d_5[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 32, 16, 64)  256         ['leaky_re_lu_5[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 32, 16, 64)   0           ['batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 32, 16, 64)   0           ['dropout_2[0][0]',              \n",
      "                                                                  'batch_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " max_pooling2d_2 (MaxPooling2D)  (None, 16, 8, 64)   0           ['add_2[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 16, 8, 128)   204928      ['max_pooling2d_2[0][0]']        \n",
      "                                                                                                  \n",
      " leaky_re_lu_6 (LeakyReLU)      (None, 16, 8, 128)   0           ['conv2d_6[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 16, 8, 128)  512         ['leaky_re_lu_6[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 16, 8, 128)   409728      ['batch_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " leaky_re_lu_7 (LeakyReLU)      (None, 16, 8, 128)   0           ['conv2d_7[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 16, 8, 128)  512         ['leaky_re_lu_7[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " dropout_3 (Dropout)            (None, 16, 8, 128)   0           ['batch_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 16, 8, 128)   0           ['dropout_3[0][0]',              \n",
      "                                                                  'batch_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " max_pooling2d_3 (MaxPooling2D)  (None, 8, 4, 128)   0           ['add_3[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (None, 8, 4, 256)    819456      ['max_pooling2d_3[0][0]']        \n",
      "                                                                                                  \n",
      " leaky_re_lu_8 (LeakyReLU)      (None, 8, 4, 256)    0           ['conv2d_8[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_8 (BatchNo  (None, 8, 4, 256)   1024        ['leaky_re_lu_8[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)              (None, 8, 4, 256)    1638656     ['batch_normalization_8[0][0]']  \n",
      "                                                                                                  \n",
      " leaky_re_lu_9 (LeakyReLU)      (None, 8, 4, 256)    0           ['conv2d_9[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_9 (BatchNo  (None, 8, 4, 256)   1024        ['leaky_re_lu_9[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 8, 4, 256)    0           ['batch_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      " add_4 (Add)                    (None, 8, 4, 256)    0           ['dropout_4[0][0]',              \n",
      "                                                                  'batch_normalization_8[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)             (None, 8, 4, 32)     4128        ['add_3[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)             (None, 8, 4, 32)     8224        ['add_4[0][0]']                  \n",
      "                                                                                                  \n",
      " leaky_re_lu_10 (LeakyReLU)     (None, 8, 4, 32)     0           ['conv2d_10[0][0]']              \n",
      "                                                                                                  \n",
      " leaky_re_lu_11 (LeakyReLU)     (None, 8, 4, 32)     0           ['conv2d_11[0][0]']              \n",
      "                                                                                                  \n",
      " add_5 (Add)                    (None, 8, 4, 32)     0           ['leaky_re_lu_10[0][0]',         \n",
      "                                                                  'leaky_re_lu_11[0][0]']         \n",
      "                                                                                                  \n",
      " leaky_re_lu_12 (LeakyReLU)     (None, 8, 4, 32)     0           ['add_5[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)             (None, 8, 4, 1)      33          ['leaky_re_lu_12[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_transpose (Conv2DTransp  (None, 16, 8, 1)    129         ['conv2d_12[0][0]']              \n",
      " ose)                                                                                             \n",
      "                                                                                                  \n",
      " multiply (Multiply)            (None, 16, 8, 128)   0           ['conv2d_transpose[0][0]',       \n",
      "                                                                  'add_3[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_transpose_1 (Conv2DTran  (None, 16, 8, 128)  819328      ['add_4[0][0]']                  \n",
      " spose)                                                                                           \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 16, 8, 256)   0           ['multiply[0][0]',               \n",
      "                                                                  'conv2d_transpose_1[0][0]']     \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)             (None, 16, 8, 128)   819328      ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " leaky_re_lu_13 (LeakyReLU)     (None, 16, 8, 128)   0           ['conv2d_13[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_10 (BatchN  (None, 16, 8, 128)  512         ['leaky_re_lu_13[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)             (None, 16, 8, 128)   409728      ['batch_normalization_10[0][0]'] \n",
      "                                                                                                  \n",
      " leaky_re_lu_14 (LeakyReLU)     (None, 16, 8, 128)   0           ['conv2d_14[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_11 (BatchN  (None, 16, 8, 128)  512         ['leaky_re_lu_14[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 16, 8, 128)   0           ['batch_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      " add_6 (Add)                    (None, 16, 8, 128)   0           ['dropout_5[0][0]',              \n",
      "                                                                  'batch_normalization_10[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_15 (Conv2D)             (None, 16, 8, 32)    2080        ['add_2[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_16 (Conv2D)             (None, 16, 8, 32)    4128        ['add_6[0][0]']                  \n",
      "                                                                                                  \n",
      " leaky_re_lu_15 (LeakyReLU)     (None, 16, 8, 32)    0           ['conv2d_15[0][0]']              \n",
      "                                                                                                  \n",
      " leaky_re_lu_16 (LeakyReLU)     (None, 16, 8, 32)    0           ['conv2d_16[0][0]']              \n",
      "                                                                                                  \n",
      " add_7 (Add)                    (None, 16, 8, 32)    0           ['leaky_re_lu_15[0][0]',         \n",
      "                                                                  'leaky_re_lu_16[0][0]']         \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " leaky_re_lu_17 (LeakyReLU)     (None, 16, 8, 32)    0           ['add_7[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_17 (Conv2D)             (None, 16, 8, 1)     33          ['leaky_re_lu_17[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_transpose_2 (Conv2DTran  (None, 32, 16, 1)   513         ['conv2d_17[0][0]']              \n",
      " spose)                                                                                           \n",
      "                                                                                                  \n",
      " multiply_1 (Multiply)          (None, 32, 16, 64)   0           ['conv2d_transpose_2[0][0]',     \n",
      "                                                                  'add_2[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_transpose_3 (Conv2DTran  (None, 32, 16, 64)  204864      ['add_6[0][0]']                  \n",
      " spose)                                                                                           \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 32, 16, 128)  0           ['multiply_1[0][0]',             \n",
      "                                                                  'conv2d_transpose_3[0][0]']     \n",
      "                                                                                                  \n",
      " conv2d_18 (Conv2D)             (None, 32, 16, 64)   204864      ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " leaky_re_lu_18 (LeakyReLU)     (None, 32, 16, 64)   0           ['conv2d_18[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_12 (BatchN  (None, 32, 16, 64)  256         ['leaky_re_lu_18[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv2d_19 (Conv2D)             (None, 32, 16, 64)   102464      ['batch_normalization_12[0][0]'] \n",
      "                                                                                                  \n",
      " leaky_re_lu_19 (LeakyReLU)     (None, 32, 16, 64)   0           ['conv2d_19[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_13 (BatchN  (None, 32, 16, 64)  256         ['leaky_re_lu_19[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 32, 16, 64)   0           ['batch_normalization_13[0][0]'] \n",
      "                                                                                                  \n",
      " add_8 (Add)                    (None, 32, 16, 64)   0           ['dropout_6[0][0]',              \n",
      "                                                                  'batch_normalization_12[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_20 (Conv2D)             (None, 32, 16, 32)   1056        ['add_1[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_21 (Conv2D)             (None, 32, 16, 32)   2080        ['add_8[0][0]']                  \n",
      "                                                                                                  \n",
      " leaky_re_lu_20 (LeakyReLU)     (None, 32, 16, 32)   0           ['conv2d_20[0][0]']              \n",
      "                                                                                                  \n",
      " leaky_re_lu_21 (LeakyReLU)     (None, 32, 16, 32)   0           ['conv2d_21[0][0]']              \n",
      "                                                                                                  \n",
      " add_9 (Add)                    (None, 32, 16, 32)   0           ['leaky_re_lu_20[0][0]',         \n",
      "                                                                  'leaky_re_lu_21[0][0]']         \n",
      "                                                                                                  \n",
      " leaky_re_lu_22 (LeakyReLU)     (None, 32, 16, 32)   0           ['add_9[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_22 (Conv2D)             (None, 32, 16, 1)    33          ['leaky_re_lu_22[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_transpose_4 (Conv2DTran  (None, 64, 32, 1)   2049        ['conv2d_22[0][0]']              \n",
      " spose)                                                                                           \n",
      "                                                                                                  \n",
      " multiply_2 (Multiply)          (None, 64, 32, 32)   0           ['conv2d_transpose_4[0][0]',     \n",
      "                                                                  'add_1[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_transpose_5 (Conv2DTran  (None, 64, 32, 32)  51232       ['add_8[0][0]']                  \n",
      " spose)                                                                                           \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 64, 32, 64)   0           ['multiply_2[0][0]',             \n",
      "                                                                  'conv2d_transpose_5[0][0]']     \n",
      "                                                                                                  \n",
      " conv2d_23 (Conv2D)             (None, 64, 32, 32)   51232       ['concatenate_2[0][0]']          \n",
      "                                                                                                  \n",
      " leaky_re_lu_23 (LeakyReLU)     (None, 64, 32, 32)   0           ['conv2d_23[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_14 (BatchN  (None, 64, 32, 32)  128         ['leaky_re_lu_23[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv2d_24 (Conv2D)             (None, 64, 32, 32)   25632       ['batch_normalization_14[0][0]'] \n",
      "                                                                                                  \n",
      " leaky_re_lu_24 (LeakyReLU)     (None, 64, 32, 32)   0           ['conv2d_24[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_15 (BatchN  (None, 64, 32, 32)  128         ['leaky_re_lu_24[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 64, 32, 32)   0           ['batch_normalization_15[0][0]'] \n",
      "                                                                                                  \n",
      " add_10 (Add)                   (None, 64, 32, 32)   0           ['dropout_7[0][0]',              \n",
      "                                                                  'batch_normalization_14[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_30 (Conv2D)             (None, 64, 32, 1)    513         ['add_10[0][0]']                 \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " batch_normalization_18 (BatchN  (None, 64, 32, 1)   4           ['conv2d_30[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv2d_transpose_8 (Conv2DTran  (None, 128, 64, 1)  26          ['batch_normalization_18[0][0]'] \n",
      " spose)                                                                                           \n",
      "                                                                                                  \n",
      " multiply_4 (Multiply)          (None, 128, 64, 1)   0           ['conv2d_transpose_8[0][0]',     \n",
      "                                                                  'input_1[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 5,991,253\n",
      "Trainable params: 5,988,371\n",
      "Non-trainable params: 2,882\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93b30fac",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_31 (Conv2D)          (None, 128, 64, 16)       160       \n",
      "                                                                 \n",
      " batch_normalization_19 (Bat  (None, 128, 64, 16)      64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " re_lu (ReLU)                (None, 128, 64, 16)       0         \n",
      "                                                                 \n",
      " conv2d_32 (Conv2D)          (None, 64, 32, 32)        4640      \n",
      "                                                                 \n",
      " batch_normalization_20 (Bat  (None, 64, 32, 32)       128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " re_lu_1 (ReLU)              (None, 64, 32, 32)        0         \n",
      "                                                                 \n",
      " conv2d_33 (Conv2D)          (None, 32, 16, 64)        18496     \n",
      "                                                                 \n",
      " batch_normalization_21 (Bat  (None, 32, 16, 64)       256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " re_lu_2 (ReLU)              (None, 32, 16, 64)        0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 32768)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 32769     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 56,513\n",
      "Trainable params: 56,289\n",
      "Non-trainable params: 224\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f4b645",
   "metadata": {},
   "source": [
    "## Data Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5d33221",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4213it [00:02, 1856.76it/s]\n"
     ]
    }
   ],
   "source": [
    "# Function to load data from a single .mat file\n",
    "def load_data(file_path):\n",
    "    loaded_mat = scipy.io.loadmat(file_path)\n",
    "    return loaded_mat['Segment_clean']  # Replace with the actual variable name\n",
    "\n",
    "# Define the folder containing the .mat files\n",
    "mat_folder = 'data/dataformodel_noaug/clean_stft_wgn0db'  # Replace with the actual path\n",
    "\n",
    "# Initialize an empty list to store loaded data\n",
    "loaded_data = []\n",
    "\n",
    "# List all .mat files in the folder\n",
    "mat_files = [os.path.join(mat_folder, filename) for filename in os.listdir(mat_folder) if filename.endswith('.mat')]\n",
    "\n",
    "# Use concurrent processing to load data from multiple files simultaneously\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    loaded_data = list(tqdm.tqdm(executor.map(load_data, mat_files)))\n",
    "\n",
    "# Concatenate all loaded data into a single variable\n",
    "clean_data = np.stack(loaded_data, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3cd7bcbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4213, 128, 64), 0.8289681539628687)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_data.shape,np.std(np.real(clean_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00db8894",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4213, 128, 64), (4213, 128, 64))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_real = np.real(clean_data)\n",
    "clean_img  = np.imag(clean_data)\n",
    "clean_real.shape,clean_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79980cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# original_shape_clean = clean_real.shape\n",
    "# clean_data_2d = clean_real.reshape(-1, original_shape_clean[-1])\n",
    "\n",
    "# # Initialize the StandardScaler\n",
    "# scaler_clean = StandardScaler()\n",
    "\n",
    "# # Fit the scaler on your data and transform it\n",
    "# scaled_cleandata_2d = scaler_clean.fit_transform(clean_data_2d)\n",
    "\n",
    "# # Reshape the scaled data back to its original shape\n",
    "# scaled_clean_data = scaled_cleandata_2d.reshape(original_shape_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf46c58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "166d6896",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4213it [00:02, 1810.57it/s]\n"
     ]
    }
   ],
   "source": [
    "# Function to load data from a single .mat file\n",
    "def load_data(file_path):\n",
    "    loaded_mat = scipy.io.loadmat(file_path)\n",
    "    return loaded_mat['Segment_noisy']  # Replace with the actual variable name\n",
    "\n",
    "# Define the folder containing the .mat file\n",
    "mat_folder = 'data/dataformodel_noaug/noisy_stft_wgn0db'  # Replace with the actual path\n",
    "\n",
    "# Initialize an empty list to store loaded data\n",
    "loaded_data = []\n",
    "\n",
    "# List all .mat files in the folder\n",
    "mat_files = [os.path.join(mat_folder, filename) for filename in os.listdir(mat_folder) if filename.endswith('.mat')]\n",
    "\n",
    "# Use concurrent processing to load data from multiple files simultaneously\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    loaded_data = list(tqdm.tqdm(executor.map(load_data, mat_files)))\n",
    "\n",
    "# Concatenate all loaded data into a single variable\n",
    "noisy_data = np.stack(loaded_data, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "39814b01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4213, 128, 64), 1.2157565236630237)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noisy_data.shape,np.std(np.real(noisy_data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "10595f08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4213, 128, 64), (4213, 128, 64))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noisy_real = np.real(noisy_data)\n",
    "noisy_img  = np.imag(noisy_data)\n",
    "noisy_real.shape,noisy_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f12f248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# original_shape = noisy_real.shape\n",
    "# noisy_data_2d = noisy_real.reshape(-1, original_shape[-1])\n",
    "\n",
    "# # Initialize the StandardScaler\n",
    "# scaler = StandardScaler()\n",
    "\n",
    "# # Fit the scaler on your data and transform it\n",
    "# scaled_data_2d = scaler.fit_transform(noisy_data_2d)\n",
    "\n",
    "# # Reshape the scaled data back to its original shape\n",
    "# scaled_data = scaled_data_2d.reshape(original_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "989464b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_trn, X_valid, y_trn, y_valid = train_test_split(noisy_real, clean_real, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "43ab835a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3791, 128, 64), (422, 128, 64), (3791, 128, 64), (422, 128, 64))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_trn.shape, X_valid.shape, y_trn.shape, y_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccad3837",
   "metadata": {},
   "source": [
    "## Model Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "875f794b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_noisy = X_trn\n",
    "y_train_clean =y_trn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "df70780b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3791, 128, 64), (3791, 128, 64))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_noisy.shape,y_train_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "817020ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3000, 60/60 - D Loss: 0.001561547358505777, G Loss: 0.4643370509147644 - 25.78 s\n",
      "Epoch 2/3000, 60/60 - D Loss: 0.0005540987840504386, G Loss: 0.43324682116508484 - 17.43 s\n",
      "Epoch 3/3000, 60/60 - D Loss: 0.0002852453053492354, G Loss: 0.4276280105113983 - 17.50 s\n",
      "Epoch 4/3000, 60/60 - D Loss: 0.00017424047837266698, G Loss: 0.42362621426582336 - 17.57 s\n",
      "Epoch 5/3000, 60/60 - D Loss: 0.00011885871572303586, G Loss: 0.4194992780685425 - 17.69 s\n",
      "Epoch 6/3000, 60/60 - D Loss: 8.539981990907108e-05, G Loss: 0.4159187376499176 - 17.87 s\n",
      "Epoch 7/3000, 60/60 - D Loss: 6.318226587609388e-05, G Loss: 0.4107545018196106 - 17.65 s\n",
      "Epoch 8/3000, 60/60 - D Loss: 4.9804860282165464e-05, G Loss: 0.4055953621864319 - 17.39 s\n",
      "Epoch 9/3000, 60/60 - D Loss: 4.006914105048054e-05, G Loss: 0.4029640853404999 - 17.48 s\n",
      "Epoch 10/3000, 60/60 - D Loss: 3.3823002013377845e-05, G Loss: 0.3979806900024414 - 17.69 s\n",
      "Epoch 11/3000, 60/60 - D Loss: 2.8674054647126468e-05, G Loss: 0.3952995538711548 - 17.60 s\n",
      "Epoch 12/3000, 60/60 - D Loss: 2.4704385396034922e-05, G Loss: 0.3916822671890259 - 17.62 s\n",
      "Epoch 13/3000, 60/60 - D Loss: 2.1524293515540194e-05, G Loss: 0.3886767029762268 - 17.65 s\n",
      "Epoch 14/3000, 60/60 - D Loss: 1.886543805085239e-05, G Loss: 0.38486263155937195 - 17.49 s\n",
      "Epoch 15/3000, 60/60 - D Loss: 1.661607348069083e-05, G Loss: 0.38168972730636597 - 17.61 s\n",
      "Epoch 16/3000, 60/60 - D Loss: 1.4947159797884524e-05, G Loss: 0.3796851336956024 - 17.67 s\n",
      "Epoch 17/3000, 60/60 - D Loss: 1.3426304121821886e-05, G Loss: 0.377132385969162 - 17.50 s\n",
      "Epoch 18/3000, 60/60 - D Loss: 1.2129117067161133e-05, G Loss: 0.3742152452468872 - 17.57 s\n",
      "Epoch 19/3000, 60/60 - D Loss: 1.0957748600048944e-05, G Loss: 0.37121549248695374 - 17.57 s\n",
      "Epoch 20/3000, 60/60 - D Loss: 1.0027611324403551e-05, G Loss: 0.37012335658073425 - 17.52 s\n",
      "Epoch 21/3000, 60/60 - D Loss: 9.082871201826492e-06, G Loss: 0.3658095598220825 - 17.89 s\n",
      "Epoch 22/3000, 60/60 - D Loss: 8.216355809054221e-06, G Loss: 0.36262086033821106 - 17.65 s\n",
      "Epoch 23/3000, 60/60 - D Loss: 7.528681180701824e-06, G Loss: 0.36030006408691406 - 17.43 s\n",
      "Epoch 24/3000, 60/60 - D Loss: 6.865573368486366e-06, G Loss: 0.35931649804115295 - 17.53 s\n",
      "Epoch 25/3000, 60/60 - D Loss: 6.275325176829938e-06, G Loss: 0.3553180694580078 - 17.52 s\n",
      "Epoch 26/3000, 60/60 - D Loss: 5.747644991060952e-06, G Loss: 0.3555053174495697 - 17.74 s\n",
      "Epoch 27/3000, 60/60 - D Loss: 5.291905381454853e-06, G Loss: 0.35276126861572266 - 17.61 s\n",
      "Epoch 28/3000, 60/60 - D Loss: 4.838056725020579e-06, G Loss: 0.3505938649177551 - 17.47 s\n",
      "Epoch 29/3000, 60/60 - D Loss: 4.472660066312528e-06, G Loss: 0.3496594727039337 - 17.68 s\n",
      "Epoch 30/3000, 60/60 - D Loss: 4.132277240387339e-06, G Loss: 0.3462292551994324 - 17.81 s\n",
      "Epoch 31/3000, 60/60 - D Loss: 3.881220663970453e-06, G Loss: 0.34594011306762695 - 17.60 s\n",
      "Epoch 32/3000, 60/60 - D Loss: 3.5567495615396183e-06, G Loss: 0.3404143154621124 - 17.66 s\n",
      "Epoch 33/3000, 60/60 - D Loss: 3.298971250842442e-06, G Loss: 0.34230783581733704 - 17.56 s\n",
      "Epoch 34/3000, 60/60 - D Loss: 3.07521418108081e-06, G Loss: 0.3387259244918823 - 17.59 s\n",
      "Epoch 35/3000, 60/60 - D Loss: 2.7693650963556138e-06, G Loss: 0.3399505019187927 - 17.64 s\n",
      "Epoch 36/3000, 60/60 - D Loss: 2.5812896069510316e-06, G Loss: 0.33440837264060974 - 18.08 s\n",
      "Epoch 37/3000, 60/60 - D Loss: 2.4346489340132393e-06, G Loss: 0.3329513967037201 - 17.54 s\n",
      "Epoch 38/3000, 60/60 - D Loss: 2.261297368022497e-06, G Loss: 0.33305901288986206 - 17.64 s\n",
      "Epoch 39/3000, 60/60 - D Loss: 2.082724279262038e-06, G Loss: 0.33012324571609497 - 17.67 s\n",
      "Epoch 40/3000, 60/60 - D Loss: 1.924990328916465e-06, G Loss: 0.33044061064720154 - 17.56 s\n",
      "Epoch 41/3000, 60/60 - D Loss: 1.7950015376300144e-06, G Loss: 0.3266798257827759 - 17.64 s\n",
      "Epoch 42/3000, 60/60 - D Loss: 1.680387811120454e-06, G Loss: 0.32477474212646484 - 17.77 s\n",
      "Epoch 43/3000, 60/60 - D Loss: 1.5608227954544418e-06, G Loss: 0.3239516019821167 - 17.55 s\n",
      "Epoch 44/3000, 60/60 - D Loss: 1.4682639744023618e-06, G Loss: 0.323781818151474 - 17.61 s\n",
      "Epoch 45/3000, 60/60 - D Loss: 1.363443004720466e-06, G Loss: 0.31959521770477295 - 17.55 s\n",
      "Epoch 46/3000, 60/60 - D Loss: 1.2546469463359244e-06, G Loss: 0.3195057511329651 - 17.73 s\n",
      "Epoch 47/3000, 60/60 - D Loss: 1.2139593366100598e-06, G Loss: 0.3180788457393646 - 17.51 s\n",
      "Epoch 48/3000, 60/60 - D Loss: 1.132732990072327e-06, G Loss: 0.3183130919933319 - 17.51 s\n",
      "Epoch 49/3000, 60/60 - D Loss: 1.057254308989286e-06, G Loss: 0.3182869553565979 - 17.40 s\n",
      "Epoch 50/3000, 60/60 - D Loss: 9.774646514415508e-07, G Loss: 0.3145553469657898 - 17.54 s\n",
      "Epoch 51/3000, 60/60 - D Loss: 9.322904190867121e-07, G Loss: 0.3133363723754883 - 17.53 s\n",
      "Epoch 52/3000, 60/60 - D Loss: 8.569856504436757e-07, G Loss: 0.31175532937049866 - 17.90 s\n",
      "Epoch 53/3000, 60/60 - D Loss: 8.093515475593449e-07, G Loss: 0.3104441463947296 - 17.64 s\n",
      "Epoch 54/3000, 60/60 - D Loss: 7.518586357946333e-07, G Loss: 0.30793264508247375 - 17.68 s\n",
      "Epoch 55/3000, 60/60 - D Loss: 7.122340264231752e-07, G Loss: 0.31018584966659546 - 17.64 s\n",
      "Epoch 56/3000, 60/60 - D Loss: 6.654087769675243e-07, G Loss: 0.3068902790546417 - 17.57 s\n",
      "Epoch 57/3000, 60/60 - D Loss: 6.262135912038502e-07, G Loss: 0.30670759081840515 - 17.61 s\n",
      "Epoch 58/3000, 60/60 - D Loss: 5.801636149271872e-07, G Loss: 0.30229949951171875 - 17.61 s\n",
      "Epoch 59/3000, 60/60 - D Loss: 5.440771388975918e-07, G Loss: 0.3032906949520111 - 17.66 s\n",
      "Epoch 60/3000, 60/60 - D Loss: 5.10142683651793e-07, G Loss: 0.30138736963272095 - 17.80 s\n",
      "Epoch 61/3000, 60/60 - D Loss: 4.746022312929199e-07, G Loss: 0.29966312646865845 - 17.28 s\n",
      "Epoch 62/3000, 60/60 - D Loss: 4.423048522994577e-07, G Loss: 0.3011884093284607 - 16.62 s\n",
      "Epoch 63/3000, 60/60 - D Loss: 4.092631797902868e-07, G Loss: 0.2999376058578491 - 17.50 s\n",
      "Epoch 64/3000, 60/60 - D Loss: 3.8407210922741797e-07, G Loss: 0.2963367998600006 - 17.44 s\n",
      "Epoch 65/3000, 60/60 - D Loss: 3.610495582506701e-07, G Loss: 0.29613786935806274 - 17.57 s\n",
      "Epoch 66/3000, 60/60 - D Loss: 3.356340982918482e-07, G Loss: 0.29335224628448486 - 17.48 s\n",
      "Epoch 67/3000, 60/60 - D Loss: 3.1651350695938163e-07, G Loss: 0.29813796281814575 - 17.95 s\n",
      "Epoch 68/3000, 60/60 - D Loss: 2.9486653829735587e-07, G Loss: 0.2941017150878906 - 17.70 s\n",
      "Epoch 69/3000, 60/60 - D Loss: 2.7031718019543405e-07, G Loss: 0.2921140789985657 - 17.67 s\n",
      "Epoch 70/3000, 60/60 - D Loss: 2.5764111910575593e-07, G Loss: 0.29545992612838745 - 17.73 s\n",
      "Epoch 71/3000, 60/60 - D Loss: 2.429649725854688e-07, G Loss: 0.2907261252403259 - 17.77 s\n",
      "Epoch 72/3000, 60/60 - D Loss: 2.2528962517753826e-07, G Loss: 0.28889521956443787 - 17.59 s\n",
      "Epoch 73/3000, 60/60 - D Loss: 2.08179059768554e-07, G Loss: 0.2871330976486206 - 17.67 s\n",
      "Epoch 74/3000, 60/60 - D Loss: 1.965816736060333e-07, G Loss: 0.2889018952846527 - 17.44 s\n",
      "Epoch 75/3000, 60/60 - D Loss: 1.8043655813926307e-07, G Loss: 0.2851511836051941 - 17.76 s\n",
      "Epoch 76/3000, 60/60 - D Loss: 1.6871332064738453e-07, G Loss: 0.287227600812912 - 17.58 s\n",
      "Epoch 77/3000, 60/60 - D Loss: 1.57723434313084e-07, G Loss: 0.2817671000957489 - 17.48 s\n",
      "Epoch 78/3000, 60/60 - D Loss: 1.4876116338768952e-07, G Loss: 0.2851015329360962 - 17.67 s\n",
      "Epoch 79/3000, 60/60 - D Loss: 1.36757137170207e-07, G Loss: 0.2822871208190918 - 17.69 s\n",
      "Epoch 80/3000, 60/60 - D Loss: 1.289015969518914e-07, G Loss: 0.28021907806396484 - 17.67 s\n",
      "Epoch 81/3000, 60/60 - D Loss: 1.174343395859978e-07, G Loss: 0.28573793172836304 - 17.69 s\n",
      "Epoch 82/3000, 60/60 - D Loss: 1.1072518546484389e-07, G Loss: 0.27949270606040955 - 17.56 s\n",
      "Epoch 83/3000, 60/60 - D Loss: 1.0280010798169315e-07, G Loss: 0.2788408100605011 - 17.32 s\n",
      "Epoch 84/3000, 60/60 - D Loss: 9.529821198839272e-08, G Loss: 0.2791387140750885 - 17.24 s\n",
      "Epoch 85/3000, 60/60 - D Loss: 8.90515430285177e-08, G Loss: 0.27924805879592896 - 17.44 s\n",
      "Epoch 86/3000, 60/60 - D Loss: 8.434990306227519e-08, G Loss: 0.27796614170074463 - 17.44 s\n",
      "Epoch 87/3000, 60/60 - D Loss: 7.831531334545616e-08, G Loss: 0.27762502431869507 - 17.68 s\n",
      "Epoch 88/3000, 60/60 - D Loss: 7.28554105933199e-08, G Loss: 0.27786365151405334 - 17.53 s\n",
      "Epoch 89/3000, 60/60 - D Loss: 6.833234245107178e-08, G Loss: 0.2723652124404907 - 17.69 s\n",
      "Epoch 90/3000, 60/60 - D Loss: 6.290084897386805e-08, G Loss: 0.2744385600090027 - 17.28 s\n",
      "Epoch 91/3000, 60/60 - D Loss: 5.919210366300831e-08, G Loss: 0.27542999386787415 - 17.43 s\n",
      "Epoch 92/3000, 60/60 - D Loss: 5.4754730527406537e-08, G Loss: 0.2706703543663025 - 17.68 s\n",
      "Epoch 93/3000, 60/60 - D Loss: 5.204709374595495e-08, G Loss: 0.27206239104270935 - 17.71 s\n",
      "Epoch 94/3000, 60/60 - D Loss: 4.913064088896135e-08, G Loss: 0.27541229128837585 - 17.62 s\n",
      "Epoch 95/3000, 60/60 - D Loss: 4.583724155793334e-08, G Loss: 0.27548959851264954 - 17.69 s\n",
      "Epoch 96/3000, 60/60 - D Loss: 4.336497561041597e-08, G Loss: 0.26707354187965393 - 17.68 s\n",
      "Epoch 97/3000, 60/60 - D Loss: 3.860364206076383e-08, G Loss: 0.2684498131275177 - 17.99 s\n",
      "Epoch 98/3000, 60/60 - D Loss: 3.685545291176595e-08, G Loss: 0.2679707109928131 - 17.77 s\n",
      "Epoch 99/3000, 60/60 - D Loss: 3.4918018698704145e-08, G Loss: 0.27004191279411316 - 17.67 s\n",
      "Epoch 100/3000, 60/60 - D Loss: 3.1511440568010585e-08, G Loss: 0.26764482259750366 - 17.80 s\n",
      "Epoch 101/3000, 60/60 - D Loss: 3.045132856982491e-08, G Loss: 0.2656829059123993 - 17.63 s\n",
      "Epoch 102/3000, 60/60 - D Loss: 2.7860748375019284e-08, G Loss: 0.265414297580719 - 17.55 s\n",
      "Epoch 103/3000, 60/60 - D Loss: 2.6449176182552492e-08, G Loss: 0.262218713760376 - 17.53 s\n",
      "Epoch 104/3000, 60/60 - D Loss: 2.4461454195545684e-08, G Loss: 0.26392269134521484 - 17.33 s\n",
      "Epoch 105/3000, 60/60 - D Loss: 2.319155090191316e-08, G Loss: 0.260795533657074 - 17.64 s\n",
      "Epoch 106/3000, 60/60 - D Loss: 2.3557708672683475e-08, G Loss: 0.26050689816474915 - 17.72 s\n",
      "Epoch 107/3000, 60/60 - D Loss: 1.9578047183443914e-08, G Loss: 0.26647964119911194 - 17.80 s\n",
      "Epoch 108/3000, 60/60 - D Loss: 1.8743036900303878e-08, G Loss: 0.26131194829940796 - 17.09 s\n",
      "Epoch 109/3000, 60/60 - D Loss: 1.8057549677763518e-08, G Loss: 0.2607899010181427 - 17.59 s\n",
      "Epoch 110/3000, 60/60 - D Loss: 1.614831734997324e-08, G Loss: 0.26117610931396484 - 17.68 s\n",
      "Epoch 111/3000, 60/60 - D Loss: 1.5256801599861092e-08, G Loss: 0.25639989972114563 - 17.65 s\n",
      "Epoch 112/3000, 60/60 - D Loss: 1.4211209098391464e-08, G Loss: 0.2605708837509155 - 18.00 s\n",
      "Epoch 113/3000, 60/60 - D Loss: 1.310847208557675e-08, G Loss: 0.2569957375526428 - 17.96 s\n",
      "Epoch 114/3000, 60/60 - D Loss: 1.2433077234419443e-08, G Loss: 0.2591889202594757 - 17.50 s\n",
      "Epoch 115/3000, 60/60 - D Loss: 1.2209194988344052e-08, G Loss: 0.25570690631866455 - 17.78 s\n",
      "Epoch 116/3000, 60/60 - D Loss: 1.049140063713594e-08, G Loss: 0.2584381401538849 - 17.55 s\n",
      "Epoch 117/3000, 60/60 - D Loss: 1.0038510245280463e-08, G Loss: 0.2578495740890503 - 17.78 s\n",
      "Epoch 118/3000, 60/60 - D Loss: 9.58060386579973e-09, G Loss: 0.25648149847984314 - 17.64 s\n",
      "Epoch 119/3000, 60/60 - D Loss: 8.856551270142177e-09, G Loss: 0.25293660163879395 - 17.80 s\n",
      "Epoch 120/3000, 60/60 - D Loss: 8.443887145048734e-09, G Loss: 0.25337621569633484 - 17.81 s\n",
      "Epoch 121/3000, 60/60 - D Loss: 8.015119457027708e-09, G Loss: 0.2533852756023407 - 17.77 s\n",
      "Epoch 122/3000, 60/60 - D Loss: 7.709419325152567e-09, G Loss: 0.25268620252609253 - 17.69 s\n",
      "Epoch 123/3000, 60/60 - D Loss: 7.7657273944709e-09, G Loss: 0.2519932985305786 - 17.85 s\n",
      "Epoch 124/3000, 60/60 - D Loss: 7.187729300639489e-09, G Loss: 0.25503528118133545 - 17.62 s\n",
      "Epoch 125/3000, 60/60 - D Loss: 6.254563533758528e-09, G Loss: 0.2487240433692932 - 17.53 s\n",
      "Epoch 126/3000, 60/60 - D Loss: 6.321527523667214e-09, G Loss: 0.25181570649147034 - 17.72 s\n",
      "Epoch 127/3000, 60/60 - D Loss: 5.740627972983248e-09, G Loss: 0.25100865960121155 - 17.52 s\n",
      "Epoch 128/3000, 60/60 - D Loss: 5.135395331201664e-09, G Loss: 0.25092318654060364 - 18.10 s\n",
      "Epoch 129/3000, 60/60 - D Loss: 4.973429890142711e-09, G Loss: 0.2515641152858734 - 17.72 s\n",
      "Epoch 130/3000, 60/60 - D Loss: 4.8470335523020935e-09, G Loss: 0.24709853529930115 - 17.60 s\n",
      "Epoch 131/3000, 60/60 - D Loss: 4.3922876447055614e-09, G Loss: 0.24934034049510956 - 17.77 s\n",
      "Epoch 132/3000, 60/60 - D Loss: 4.254977814532879e-09, G Loss: 0.2470097690820694 - 17.66 s\n",
      "Epoch 133/3000, 60/60 - D Loss: 4.431043754138386e-09, G Loss: 0.2494475245475769 - 17.66 s\n",
      "Epoch 134/3000, 60/60 - D Loss: 3.882654753262216e-09, G Loss: 0.2464456558227539 - 17.62 s\n",
      "Epoch 135/3000, 60/60 - D Loss: 4.0016677749221685e-09, G Loss: 0.2512158155441284 - 17.58 s\n",
      "Epoch 136/3000, 60/60 - D Loss: 3.602432019356172e-09, G Loss: 0.24326381087303162 - 17.75 s\n",
      "Epoch 137/3000, 60/60 - D Loss: 3.210195664848925e-09, G Loss: 0.245575413107872 - 17.71 s\n",
      "Epoch 138/3000, 60/60 - D Loss: 3.100894097052276e-09, G Loss: 0.24467474222183228 - 17.65 s\n",
      "Epoch 139/3000, 60/60 - D Loss: 2.958177314216215e-09, G Loss: 0.24260975420475006 - 17.74 s\n",
      "Epoch 140/3000, 60/60 - D Loss: 3.0118273985912936e-09, G Loss: 0.24521879851818085 - 17.71 s\n",
      "Epoch 141/3000, 60/60 - D Loss: 2.77807865600721e-09, G Loss: 0.24175752699375153 - 17.79 s\n",
      "Epoch 142/3000, 60/60 - D Loss: 2.7776888567032643e-09, G Loss: 0.24053223431110382 - 17.37 s\n",
      "Epoch 143/3000, 60/60 - D Loss: 2.623298855919387e-09, G Loss: 0.24250848591327667 - 17.95 s\n",
      "Epoch 144/3000, 60/60 - D Loss: 2.5673945192039582e-09, G Loss: 0.24109657108783722 - 17.72 s\n",
      "Epoch 145/3000, 60/60 - D Loss: 2.5336219233729196e-09, G Loss: 0.2451663613319397 - 17.80 s\n",
      "Epoch 146/3000, 60/60 - D Loss: 2.2941084032446213e-09, G Loss: 0.239105686545372 - 17.73 s\n",
      "Epoch 147/3000, 60/60 - D Loss: 2.4562548661677397e-09, G Loss: 0.23748622834682465 - 17.56 s\n",
      "Epoch 148/3000, 60/60 - D Loss: 2.2904550478486385e-09, G Loss: 0.23651902377605438 - 17.76 s\n",
      "Epoch 149/3000, 60/60 - D Loss: 2.167335477754051e-09, G Loss: 0.2444199025630951 - 17.68 s\n",
      "Epoch 150/3000, 60/60 - D Loss: 2.1888184598140015e-09, G Loss: 0.24363741278648376 - 17.69 s\n",
      "Epoch 151/3000, 60/60 - D Loss: 2.0903589970089342e-09, G Loss: 0.2420395463705063 - 17.46 s\n",
      "Epoch 152/3000, 60/60 - D Loss: 2.1943816208569444e-09, G Loss: 0.23702114820480347 - 18.11 s\n",
      "Epoch 153/3000, 60/60 - D Loss: 2.1201424504901922e-09, G Loss: 0.24173977971076965 - 17.58 s\n",
      "Epoch 154/3000, 60/60 - D Loss: 2.0328239924705116e-09, G Loss: 0.2386614978313446 - 17.62 s\n",
      "Epoch 155/3000, 60/60 - D Loss: 2.0737359329725535e-09, G Loss: 0.234934464097023 - 17.76 s\n",
      "Epoch 156/3000, 60/60 - D Loss: 2.0321350435725805e-09, G Loss: 0.23462830483913422 - 17.68 s\n",
      "Epoch 157/3000, 60/60 - D Loss: 2.0676791667728622e-09, G Loss: 0.2343832552433014 - 17.70 s\n",
      "Epoch 158/3000, 60/60 - D Loss: 2.0323953631162794e-09, G Loss: 0.2335347831249237 - 17.90 s\n",
      "Epoch 159/3000, 60/60 - D Loss: 1.9716803745239986e-09, G Loss: 0.23958449065685272 - 17.46 s\n",
      "Epoch 160/3000, 60/60 - D Loss: 1.9559562303150813e-09, G Loss: 0.23717649281024933 - 17.47 s\n",
      "Epoch 161/3000, 60/60 - D Loss: 1.923502884748629e-09, G Loss: 0.23502041399478912 - 17.60 s\n",
      "Epoch 162/3000, 60/60 - D Loss: 1.9126333850039146e-09, G Loss: 0.23206813633441925 - 17.67 s\n",
      "Epoch 163/3000, 60/60 - D Loss: 1.8474000662571655e-09, G Loss: 0.23388460278511047 - 17.61 s\n",
      "Epoch 164/3000, 60/60 - D Loss: 1.8257028666646136e-09, G Loss: 0.23692120611667633 - 17.94 s\n",
      "Epoch 165/3000, 60/60 - D Loss: 1.870761018318845e-09, G Loss: 0.23029954731464386 - 17.71 s\n",
      "Epoch 166/3000, 60/60 - D Loss: 1.7805143470717155e-09, G Loss: 0.23219306766986847 - 17.73 s\n",
      "Epoch 167/3000, 60/60 - D Loss: 1.8168335447210637e-09, G Loss: 0.23192453384399414 - 17.68 s\n",
      "Epoch 168/3000, 60/60 - D Loss: 1.753704487184038e-09, G Loss: 0.22868295013904572 - 17.52 s\n",
      "Epoch 169/3000, 60/60 - D Loss: 1.8024470249677904e-09, G Loss: 0.2377333790063858 - 17.78 s\n",
      "Epoch 170/3000, 60/60 - D Loss: 1.7608401126079087e-09, G Loss: 0.23339535295963287 - 17.67 s\n",
      "Epoch 171/3000, 60/60 - D Loss: 1.6820513037973939e-09, G Loss: 0.22976164519786835 - 17.87 s\n",
      "Epoch 172/3000, 60/60 - D Loss: 1.8057165346307968e-09, G Loss: 0.2306518852710724 - 17.65 s\n",
      "Epoch 173/3000, 60/60 - D Loss: 1.7694049975203185e-09, G Loss: 0.23591767251491547 - 17.77 s\n",
      "Epoch 174/3000, 60/60 - D Loss: 1.764073026544466e-09, G Loss: 0.23089633882045746 - 17.51 s\n",
      "Epoch 175/3000, 60/60 - D Loss: 1.7484078074225806e-09, G Loss: 0.22614392638206482 - 17.88 s\n",
      "Epoch 176/3000, 60/60 - D Loss: 1.696914997406651e-09, G Loss: 0.2309570014476776 - 17.92 s\n",
      "Epoch 177/3000, 60/60 - D Loss: 1.7605783914076412e-09, G Loss: 0.22792552411556244 - 17.66 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 178/3000, 60/60 - D Loss: 1.7044992084436217e-09, G Loss: 0.23039166629314423 - 17.74 s\n",
      "Epoch 179/3000, 60/60 - D Loss: 1.694609036428929e-09, G Loss: 0.23357197642326355 - 17.62 s\n",
      "Epoch 180/3000, 60/60 - D Loss: 1.7081950159925086e-09, G Loss: 0.227168008685112 - 17.62 s\n",
      "Epoch 181/3000, 60/60 - D Loss: 1.667303697883149e-09, G Loss: 0.22446492314338684 - 17.59 s\n",
      "Epoch 182/3000, 60/60 - D Loss: 1.6555378734572912e-09, G Loss: 0.22698785364627838 - 17.66 s\n",
      "Epoch 183/3000, 60/60 - D Loss: 1.7097945004262982e-09, G Loss: 0.22801168262958527 - 17.67 s\n",
      "Epoch 184/3000, 60/60 - D Loss: 1.7617305531070215e-09, G Loss: 0.2297397404909134 - 17.67 s\n",
      "Epoch 185/3000, 60/60 - D Loss: 1.6875293662454993e-09, G Loss: 0.22520847618579865 - 17.88 s\n",
      "Epoch 186/3000, 60/60 - D Loss: 1.6768699345748317e-09, G Loss: 0.2255282700061798 - 17.57 s\n",
      "Epoch 187/3000, 60/60 - D Loss: 1.5811471160143142e-09, G Loss: 0.2242555469274521 - 17.25 s\n",
      "Epoch 188/3000, 60/60 - D Loss: 1.701247337448919e-09, G Loss: 0.22417853772640228 - 17.87 s\n",
      "Epoch 189/3000, 60/60 - D Loss: 1.5694986837955227e-09, G Loss: 0.22461052238941193 - 17.76 s\n",
      "Epoch 190/3000, 60/60 - D Loss: 1.6263815572070683e-09, G Loss: 0.22872811555862427 - 17.43 s\n",
      "Epoch 191/3000, 60/60 - D Loss: 1.5650257201293982e-09, G Loss: 0.22461271286010742 - 17.73 s\n",
      "Epoch 192/3000, 60/60 - D Loss: 1.6603192015685053e-09, G Loss: 0.2244827151298523 - 17.60 s\n",
      "Epoch 193/3000, 60/60 - D Loss: 1.5662801472471344e-09, G Loss: 0.22366079688072205 - 17.68 s\n",
      "Epoch 194/3000, 60/60 - D Loss: 1.6356424548114035e-09, G Loss: 0.2242465317249298 - 17.87 s\n",
      "Epoch 195/3000, 60/60 - D Loss: 1.6573025868327207e-09, G Loss: 0.22176483273506165 - 17.64 s\n",
      "Epoch 196/3000, 60/60 - D Loss: 1.5977501960362517e-09, G Loss: 0.22249023616313934 - 17.68 s\n",
      "Epoch 197/3000, 60/60 - D Loss: 1.6579924516646471e-09, G Loss: 0.2240625023841858 - 17.52 s\n",
      "Epoch 198/3000, 60/60 - D Loss: 1.5517207796467147e-09, G Loss: 0.22484317421913147 - 17.76 s\n",
      "Epoch 199/3000, 60/60 - D Loss: 1.6618233386611614e-09, G Loss: 0.22274810075759888 - 17.48 s\n",
      "Epoch 200/3000, 60/60 - D Loss: 1.6973093902583614e-09, G Loss: 0.22102822363376617 - 17.47 s\n",
      "Epoch 201/3000, 60/60 - D Loss: 1.5758239782504013e-09, G Loss: 0.22050900757312775 - 17.44 s\n",
      "Epoch 202/3000, 60/60 - D Loss: 1.5585952806151937e-09, G Loss: 0.220274418592453 - 16.86 s\n",
      "Epoch 203/3000, 60/60 - D Loss: 1.499690954753774e-09, G Loss: 0.22019590437412262 - 16.74 s\n",
      "Epoch 204/3000, 60/60 - D Loss: 1.5049508098052833e-09, G Loss: 0.21895943582057953 - 16.89 s\n",
      "Epoch 205/3000, 60/60 - D Loss: 1.5955500184960947e-09, G Loss: 0.2211383879184723 - 17.21 s\n",
      "Epoch 206/3000, 60/60 - D Loss: 1.711872581189322e-09, G Loss: 0.2239847630262375 - 17.76 s\n",
      "Epoch 207/3000, 60/60 - D Loss: 1.6712080885827874e-09, G Loss: 0.21953660249710083 - 17.80 s\n",
      "Epoch 208/3000, 60/60 - D Loss: 1.6924369822257646e-09, G Loss: 0.21862325072288513 - 17.79 s\n",
      "Epoch 209/3000, 60/60 - D Loss: 1.5567549888073628e-09, G Loss: 0.21901127696037292 - 17.77 s\n",
      "Epoch 210/3000, 60/60 - D Loss: 1.6130958027660292e-09, G Loss: 0.21776069700717926 - 17.74 s\n",
      "Epoch 211/3000, 60/60 - D Loss: 1.542470901005899e-09, G Loss: 0.21879440546035767 - 17.71 s\n",
      "Epoch 212/3000, 60/60 - D Loss: 1.5723181576166034e-09, G Loss: 0.21787971258163452 - 17.67 s\n",
      "Epoch 213/3000, 60/60 - D Loss: 1.6145915993082127e-09, G Loss: 0.22058336436748505 - 17.87 s\n",
      "Epoch 214/3000, 60/60 - D Loss: 1.6629454757666196e-09, G Loss: 0.21910686790943146 - 17.81 s\n",
      "Epoch 215/3000, 60/60 - D Loss: 1.3654510022664823e-09, G Loss: 0.2175745666027069 - 17.89 s\n",
      "Epoch 216/3000, 60/60 - D Loss: 1.708191518789981e-09, G Loss: 0.22476394474506378 - 17.77 s\n",
      "Epoch 217/3000, 60/60 - D Loss: 1.780520168803701e-09, G Loss: 0.21801720559597015 - 17.73 s\n",
      "Epoch 218/3000, 60/60 - D Loss: 1.5988666571264964e-09, G Loss: 0.21881437301635742 - 18.00 s\n",
      "Epoch 219/3000, 60/60 - D Loss: 1.6748657599707784e-09, G Loss: 0.21669894456863403 - 17.88 s\n",
      "Epoch 220/3000, 60/60 - D Loss: 1.6224435336886778e-09, G Loss: 0.22315889596939087 - 17.66 s\n",
      "Epoch 221/3000, 60/60 - D Loss: 1.6873594466115804e-09, G Loss: 0.21602153778076172 - 17.74 s\n",
      "Epoch 222/3000, 60/60 - D Loss: 1.5018499568975052e-09, G Loss: 0.2148205190896988 - 17.74 s\n",
      "Epoch 223/3000, 60/60 - D Loss: 1.6441671080169584e-09, G Loss: 0.21720360219478607 - 17.74 s\n",
      "Epoch 224/3000, 60/60 - D Loss: 1.5678934539575806e-09, G Loss: 0.2179700881242752 - 17.78 s\n",
      "Epoch 225/3000, 60/60 - D Loss: 1.5621378496311067e-09, G Loss: 0.21854548156261444 - 17.80 s\n",
      "Epoch 226/3000, 60/60 - D Loss: 1.4432450373691452e-09, G Loss: 0.2143813669681549 - 17.90 s\n",
      "Epoch 227/3000, 60/60 - D Loss: 1.4891137142813982e-09, G Loss: 0.22657828032970428 - 17.67 s\n",
      "Epoch 228/3000, 60/60 - D Loss: 1.6965538904289978e-09, G Loss: 0.22012397646903992 - 17.54 s\n",
      "Epoch 229/3000, 60/60 - D Loss: 1.7888191344850313e-09, G Loss: 0.2213728278875351 - 17.70 s\n",
      "Epoch 230/3000, 60/60 - D Loss: 1.8941522170190517e-09, G Loss: 0.22412723302841187 - 17.68 s\n",
      "Epoch 231/3000, 60/60 - D Loss: 1.7543655486673693e-09, G Loss: 0.2137896567583084 - 17.83 s\n",
      "Epoch 232/3000, 60/60 - D Loss: 1.6383845738432434e-09, G Loss: 0.2163432389497757 - 17.91 s\n",
      "Epoch 233/3000, 60/60 - D Loss: 1.7687153894274665e-09, G Loss: 0.2130391150712967 - 17.77 s\n",
      "Epoch 234/3000, 60/60 - D Loss: 1.4483292135047954e-09, G Loss: 0.21200522780418396 - 17.99 s\n",
      "Epoch 235/3000, 60/60 - D Loss: 1.749164549313953e-09, G Loss: 0.21158210933208466 - 17.96 s\n",
      "Epoch 236/3000, 60/60 - D Loss: 1.7361970958140738e-09, G Loss: 0.21445688605308533 - 17.66 s\n",
      "Epoch 237/3000, 60/60 - D Loss: 1.7883579894739654e-09, G Loss: 0.21249882876873016 - 17.69 s\n",
      "Epoch 238/3000, 60/60 - D Loss: 1.4691623265172815e-09, G Loss: 0.21105143427848816 - 17.79 s\n",
      "Epoch 239/3000, 60/60 - D Loss: 1.575588458313515e-09, G Loss: 0.2145598977804184 - 17.68 s\n",
      "Epoch 240/3000, 60/60 - D Loss: 1.7195183818485837e-09, G Loss: 0.21176138520240784 - 17.68 s\n",
      "Epoch 241/3000, 60/60 - D Loss: 1.7097509172336878e-09, G Loss: 0.21797095239162445 - 17.84 s\n",
      "Epoch 242/3000, 60/60 - D Loss: 1.602447813331409e-09, G Loss: 0.2140536904335022 - 17.81 s\n",
      "Epoch 243/3000, 60/60 - D Loss: 1.7980462049815849e-09, G Loss: 0.21256126463413239 - 17.70 s\n",
      "Epoch 244/3000, 60/60 - D Loss: 1.7906247526400243e-09, G Loss: 0.21414966881275177 - 17.61 s\n",
      "Epoch 245/3000, 60/60 - D Loss: 1.485516994137459e-09, G Loss: 0.21255110204219818 - 17.67 s\n",
      "Epoch 246/3000, 60/60 - D Loss: 1.6611192352189441e-09, G Loss: 0.21052958071231842 - 17.69 s\n",
      "Epoch 247/3000, 60/60 - D Loss: 1.8039714999584788e-09, G Loss: 0.21227124333381653 - 17.61 s\n",
      "Epoch 248/3000, 60/60 - D Loss: 1.8266935741806378e-09, G Loss: 0.21917282044887543 - 17.60 s\n",
      "Epoch 249/3000, 60/60 - D Loss: 1.886765285674663e-09, G Loss: 0.21405942738056183 - 17.90 s\n",
      "Epoch 250/3000, 60/60 - D Loss: 1.8321129682807857e-09, G Loss: 0.21146005392074585 - 17.77 s\n",
      "Epoch 251/3000, 60/60 - D Loss: 1.8300635590273728e-09, G Loss: 0.20965534448623657 - 17.84 s\n",
      "Epoch 252/3000, 60/60 - D Loss: 1.756290043952724e-09, G Loss: 0.21176712214946747 - 17.50 s\n",
      "Epoch 253/3000, 60/60 - D Loss: 1.6383381179485568e-09, G Loss: 0.21101325750350952 - 17.87 s\n",
      "Epoch 254/3000, 60/60 - D Loss: 1.6430282551160857e-09, G Loss: 0.20925922691822052 - 17.71 s\n",
      "Epoch 255/3000, 60/60 - D Loss: 1.8544722216184972e-09, G Loss: 0.21128098666667938 - 17.75 s\n",
      "Epoch 256/3000, 60/60 - D Loss: 1.723558094857136e-09, G Loss: 0.20913006365299225 - 17.81 s\n",
      "Epoch 257/3000, 60/60 - D Loss: 1.915537860275318e-09, G Loss: 0.20899246633052826 - 17.83 s\n",
      "Epoch 258/3000, 60/60 - D Loss: 1.8950111202453712e-09, G Loss: 0.21520736813545227 - 17.76 s\n",
      "Epoch 259/3000, 60/60 - D Loss: 1.4040690970995584e-09, G Loss: 0.20968753099441528 - 17.70 s\n",
      "Epoch 260/3000, 60/60 - D Loss: 1.6388258597399563e-09, G Loss: 0.21292750537395477 - 17.81 s\n",
      "Epoch 261/3000, 60/60 - D Loss: 1.6496723570513794e-09, G Loss: 0.20758101344108582 - 17.65 s\n",
      "Epoch 262/3000, 60/60 - D Loss: 1.9525611058557324e-09, G Loss: 0.210190549492836 - 17.66 s\n",
      "Epoch 263/3000, 60/60 - D Loss: 1.8448674532489662e-09, G Loss: 0.2115677297115326 - 17.65 s\n",
      "Epoch 264/3000, 60/60 - D Loss: 1.8992464614897564e-09, G Loss: 0.21458038687705994 - 17.55 s\n",
      "Epoch 265/3000, 60/60 - D Loss: 1.9148448243688776e-09, G Loss: 0.20750311017036438 - 17.82 s\n",
      "Epoch 266/3000, 60/60 - D Loss: 1.9547219329285603e-09, G Loss: 0.20911471545696259 - 17.56 s\n",
      "Epoch 267/3000, 60/60 - D Loss: 1.987446249240854e-09, G Loss: 0.20716087520122528 - 17.64 s\n",
      "Epoch 268/3000, 60/60 - D Loss: 1.8060186610102669e-09, G Loss: 0.21104900538921356 - 17.52 s\n",
      "Epoch 269/3000, 60/60 - D Loss: 2.060913210921722e-09, G Loss: 0.20681442320346832 - 17.90 s\n",
      "Epoch 270/3000, 60/60 - D Loss: 1.8864995815492946e-09, G Loss: 0.2081771343946457 - 17.77 s\n",
      "Epoch 271/3000, 60/60 - D Loss: 1.9014329971644983e-09, G Loss: 0.20924176275730133 - 17.85 s\n",
      "Epoch 272/3000, 60/60 - D Loss: 1.9410591259316767e-09, G Loss: 0.21041570603847504 - 17.60 s\n",
      "Epoch 273/3000, 60/60 - D Loss: 1.8421602160367812e-09, G Loss: 0.21107997000217438 - 17.92 s\n",
      "Epoch 274/3000, 60/60 - D Loss: 2.0068755754443224e-09, G Loss: 0.20978769659996033 - 17.82 s\n",
      "Epoch 275/3000, 60/60 - D Loss: 1.7109356154065836e-09, G Loss: 0.20612777769565582 - 17.56 s\n",
      "Epoch 276/3000, 60/60 - D Loss: 1.6394666943475578e-09, G Loss: 0.20513683557510376 - 17.79 s\n",
      "Epoch 277/3000, 60/60 - D Loss: 1.615152074585513e-09, G Loss: 0.20681025087833405 - 17.67 s\n",
      "Epoch 278/3000, 60/60 - D Loss: 1.5733878228063602e-09, G Loss: 0.20581784844398499 - 17.84 s\n",
      "Epoch 279/3000, 60/60 - D Loss: 1.894840777338924e-09, G Loss: 0.2079503983259201 - 17.74 s\n",
      "Epoch 280/3000, 60/60 - D Loss: 1.8057569883822566e-09, G Loss: 0.21091300249099731 - 17.86 s\n",
      "Epoch 281/3000, 60/60 - D Loss: 2.003077634316064e-09, G Loss: 0.2061745673418045 - 17.59 s\n",
      "Epoch 282/3000, 60/60 - D Loss: 2.14105819879018e-09, G Loss: 0.20616112649440765 - 17.69 s\n",
      "Epoch 283/3000, 60/60 - D Loss: 2.0375940830430483e-09, G Loss: 0.206179678440094 - 17.70 s\n",
      "Epoch 284/3000, 60/60 - D Loss: 1.7490800752195668e-09, G Loss: 0.2119046002626419 - 17.69 s\n",
      "Epoch 285/3000, 60/60 - D Loss: 1.9237193296661736e-09, G Loss: 0.20521053671836853 - 17.79 s\n",
      "Epoch 286/3000, 60/60 - D Loss: 1.667350993383998e-09, G Loss: 0.20835217833518982 - 17.92 s\n",
      "Epoch 287/3000, 60/60 - D Loss: 1.7242973091025071e-09, G Loss: 0.21072860062122345 - 18.25 s\n",
      "Epoch 288/3000, 60/60 - D Loss: 2.154729283887491e-09, G Loss: 0.2052357941865921 - 17.83 s\n",
      "Epoch 289/3000, 60/60 - D Loss: 2.1640303160541663e-09, G Loss: 0.20866607129573822 - 17.82 s\n",
      "Epoch 290/3000, 60/60 - D Loss: 2.146104849387598e-09, G Loss: 0.2063419669866562 - 17.69 s\n",
      "Epoch 291/3000, 60/60 - D Loss: 2.140759909619039e-09, G Loss: 0.2062227576971054 - 17.86 s\n",
      "Epoch 292/3000, 60/60 - D Loss: 2.182844890952218e-09, G Loss: 0.20892959833145142 - 17.70 s\n",
      "Epoch 293/3000, 60/60 - D Loss: 2.1661785004623013e-09, G Loss: 0.20370586216449738 - 17.47 s\n",
      "Epoch 294/3000, 60/60 - D Loss: 2.1506245395652712e-09, G Loss: 0.20448856055736542 - 17.70 s\n",
      "Epoch 295/3000, 60/60 - D Loss: 2.28585138317694e-09, G Loss: 0.20630866289138794 - 18.15 s\n",
      "Epoch 296/3000, 60/60 - D Loss: 1.903000861058768e-09, G Loss: 0.20475482940673828 - 17.66 s\n",
      "Epoch 297/3000, 60/60 - D Loss: 2.0883585277742256e-09, G Loss: 0.2105868011713028 - 17.75 s\n",
      "Epoch 298/3000, 60/60 - D Loss: 2.2250099263043666e-09, G Loss: 0.20855501294136047 - 17.41 s\n",
      "Epoch 299/3000, 60/60 - D Loss: 1.8105209553809232e-09, G Loss: 0.20553214848041534 - 17.60 s\n",
      "Epoch 300/3000, 60/60 - D Loss: 1.903402643832486e-09, G Loss: 0.20718662440776825 - 17.71 s\n",
      "Epoch 301/3000, 60/60 - D Loss: 1.9765056882836696e-09, G Loss: 0.20577505230903625 - 17.72 s\n",
      "Epoch 302/3000, 60/60 - D Loss: 2.29771468518436e-09, G Loss: 0.20450112223625183 - 17.69 s\n",
      "Epoch 303/3000, 60/60 - D Loss: 1.8360045567211714e-09, G Loss: 0.20394912362098694 - 17.76 s\n",
      "Epoch 304/3000, 60/60 - D Loss: 2.0288660126932534e-09, G Loss: 0.2051076889038086 - 17.91 s\n",
      "Epoch 305/3000, 60/60 - D Loss: 2.2868335627923564e-09, G Loss: 0.21703873574733734 - 17.63 s\n",
      "Epoch 306/3000, 60/60 - D Loss: 2.085335980284153e-09, G Loss: 0.20646651089191437 - 17.81 s\n",
      "Epoch 307/3000, 60/60 - D Loss: 1.6495806248739697e-09, G Loss: 0.20384101569652557 - 17.84 s\n",
      "Epoch 308/3000, 60/60 - D Loss: 2.0825288299386457e-09, G Loss: 0.20312830805778503 - 17.67 s\n",
      "Epoch 309/3000, 60/60 - D Loss: 2.0403952763481392e-09, G Loss: 0.20629878342151642 - 17.77 s\n",
      "Epoch 310/3000, 60/60 - D Loss: 2.110191375803705e-09, G Loss: 0.20402054488658905 - 18.11 s\n",
      "Epoch 311/3000, 60/60 - D Loss: 1.9653937297081647e-09, G Loss: 0.20354913175106049 - 17.69 s\n",
      "Epoch 312/3000, 60/60 - D Loss: 2.1861995963545766e-09, G Loss: 0.20623311400413513 - 17.53 s\n",
      "Epoch 313/3000, 60/60 - D Loss: 1.89979457165812e-09, G Loss: 0.20290504395961761 - 17.70 s\n",
      "Epoch 314/3000, 60/60 - D Loss: 1.904869581514923e-09, G Loss: 0.20406490564346313 - 18.16 s\n",
      "Epoch 315/3000, 60/60 - D Loss: 2.354806585835867e-09, G Loss: 0.2072945386171341 - 17.80 s\n",
      "Epoch 316/3000, 60/60 - D Loss: 2.158592180001584e-09, G Loss: 0.2044045478105545 - 17.59 s\n",
      "Epoch 317/3000, 60/60 - D Loss: 2.3369479962287443e-09, G Loss: 0.2011658251285553 - 17.78 s\n",
      "Epoch 318/3000, 60/60 - D Loss: 2.3421484404706483e-09, G Loss: 0.20676955580711365 - 17.57 s\n",
      "Epoch 319/3000, 60/60 - D Loss: 2.256319089899428e-09, G Loss: 0.20095856487751007 - 17.77 s\n",
      "Epoch 320/3000, 60/60 - D Loss: 2.3400483842306308e-09, G Loss: 0.20484855771064758 - 17.62 s\n",
      "Epoch 321/3000, 60/60 - D Loss: 2.4991319430567316e-09, G Loss: 0.20223672688007355 - 17.56 s\n",
      "Epoch 322/3000, 60/60 - D Loss: 2.00862048521655e-09, G Loss: 0.20286725461483002 - 17.74 s\n",
      "Epoch 323/3000, 60/60 - D Loss: 2.2621271938971788e-09, G Loss: 0.20272725820541382 - 17.44 s\n",
      "Epoch 324/3000, 60/60 - D Loss: 1.971621768626086e-09, G Loss: 0.20188717544078827 - 17.70 s\n",
      "Epoch 325/3000, 60/60 - D Loss: 1.5225877642022212e-09, G Loss: 0.20074763894081116 - 17.83 s\n",
      "Epoch 326/3000, 60/60 - D Loss: 2.0502630357355223e-09, G Loss: 0.21387062966823578 - 17.73 s\n",
      "Epoch 327/3000, 60/60 - D Loss: 2.322425779466286e-09, G Loss: 0.20085881650447845 - 17.72 s\n",
      "Epoch 328/3000, 60/60 - D Loss: 2.500112901426821e-09, G Loss: 0.20199355483055115 - 17.79 s\n",
      "Epoch 329/3000, 60/60 - D Loss: 2.4748109950234465e-09, G Loss: 0.20312470197677612 - 17.58 s\n",
      "Epoch 330/3000, 60/60 - D Loss: 2.0570443126644022e-09, G Loss: 0.20399467647075653 - 17.64 s\n",
      "Epoch 331/3000, 60/60 - D Loss: 2.4362776865349822e-09, G Loss: 0.20377470552921295 - 17.66 s\n",
      "Epoch 332/3000, 60/60 - D Loss: 2.726287071097566e-09, G Loss: 0.2031821310520172 - 17.86 s\n",
      "Epoch 333/3000, 60/60 - D Loss: 2.2253307807584832e-09, G Loss: 0.20453661680221558 - 17.78 s\n",
      "Epoch 334/3000, 60/60 - D Loss: 2.485689959419446e-09, G Loss: 0.19941945374011993 - 17.72 s\n",
      "Epoch 335/3000, 60/60 - D Loss: 2.308678519191698e-09, G Loss: 0.20152711868286133 - 18.10 s\n",
      "Epoch 336/3000, 60/60 - D Loss: 2.4802404297030733e-09, G Loss: 0.20041701197624207 - 17.74 s\n",
      "Epoch 337/3000, 60/60 - D Loss: 2.4130572678915385e-09, G Loss: 0.20221306383609772 - 17.66 s\n",
      "Epoch 338/3000, 60/60 - D Loss: 2.3764781115209743e-09, G Loss: 0.19960881769657135 - 17.84 s\n",
      "Epoch 339/3000, 60/60 - D Loss: 2.4782694438285624e-09, G Loss: 0.20366129279136658 - 17.98 s\n",
      "Epoch 340/3000, 60/60 - D Loss: 2.7929765583634136e-09, G Loss: 0.20026861131191254 - 17.86 s\n",
      "Epoch 341/3000, 60/60 - D Loss: 2.476037798404551e-09, G Loss: 0.20499493181705475 - 18.09 s\n",
      "Epoch 342/3000, 60/60 - D Loss: 2.60380748212663e-09, G Loss: 0.20115114748477936 - 17.72 s\n",
      "Epoch 343/3000, 60/60 - D Loss: 2.74929880250685e-09, G Loss: 0.19906456768512726 - 17.77 s\n",
      "Epoch 344/3000, 60/60 - D Loss: 2.1763431057308935e-09, G Loss: 0.2025739699602127 - 17.62 s\n",
      "Epoch 345/3000, 60/60 - D Loss: 1.866952641094155e-09, G Loss: 0.2003047913312912 - 17.43 s\n",
      "Epoch 346/3000, 60/60 - D Loss: 2.211675662056045e-09, G Loss: 0.20358464121818542 - 17.76 s\n",
      "Epoch 347/3000, 60/60 - D Loss: 2.4175085197586377e-09, G Loss: 0.19956053793430328 - 17.80 s\n",
      "Epoch 348/3000, 60/60 - D Loss: 2.3165376630829293e-09, G Loss: 0.20116446912288666 - 17.70 s\n",
      "Epoch 349/3000, 60/60 - D Loss: 2.29814389740568e-09, G Loss: 0.19996030628681183 - 17.81 s\n",
      "Epoch 350/3000, 60/60 - D Loss: 2.54598749027668e-09, G Loss: 0.2018541693687439 - 17.53 s\n",
      "Epoch 351/3000, 60/60 - D Loss: 2.7037653574368825e-09, G Loss: 0.2031858116388321 - 17.85 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 352/3000, 60/60 - D Loss: 2.510291946533627e-09, G Loss: 0.20002005994319916 - 17.84 s\n",
      "Epoch 353/3000, 60/60 - D Loss: 2.2796830048688044e-09, G Loss: 0.19919632375240326 - 17.66 s\n",
      "Epoch 354/3000, 60/60 - D Loss: 2.622737631241545e-09, G Loss: 0.20671378076076508 - 17.87 s\n",
      "Epoch 355/3000, 60/60 - D Loss: 2.645781919941026e-09, G Loss: 0.20028747618198395 - 17.77 s\n",
      "Epoch 356/3000, 60/60 - D Loss: 2.3897441595033264e-09, G Loss: 0.20320835709571838 - 18.15 s\n",
      "Epoch 357/3000, 60/60 - D Loss: 1.941245400538527e-09, G Loss: 0.1994580626487732 - 17.78 s\n",
      "Epoch 358/3000, 60/60 - D Loss: 2.2981224701013048e-09, G Loss: 0.199344664812088 - 17.49 s\n",
      "Epoch 359/3000, 60/60 - D Loss: 2.51871478534893e-09, G Loss: 0.19847173988819122 - 17.48 s\n",
      "Epoch 360/3000, 60/60 - D Loss: 2.809868275555072e-09, G Loss: 0.2004987597465515 - 17.65 s\n",
      "Epoch 361/3000, 60/60 - D Loss: 2.5637388531563055e-09, G Loss: 0.20475246012210846 - 17.64 s\n",
      "Epoch 362/3000, 60/60 - D Loss: 2.6632915109403577e-09, G Loss: 0.2092413306236267 - 17.72 s\n",
      "Epoch 363/3000, 60/60 - D Loss: 2.924324872533024e-09, G Loss: 0.19857174158096313 - 17.75 s\n",
      "Epoch 364/3000, 60/60 - D Loss: 2.566392459657507e-09, G Loss: 0.20219312608242035 - 17.48 s\n",
      "Epoch 365/3000, 60/60 - D Loss: 2.5128931366302787e-09, G Loss: 0.20069321990013123 - 17.78 s\n",
      "Epoch 366/3000, 60/60 - D Loss: 2.9256750355699523e-09, G Loss: 0.19928573071956635 - 17.89 s\n",
      "Epoch 367/3000, 60/60 - D Loss: 2.641728592822634e-09, G Loss: 0.1989724040031433 - 17.55 s\n",
      "Epoch 368/3000, 60/60 - D Loss: 3.0954587712517245e-09, G Loss: 0.20139802992343903 - 17.78 s\n",
      "Epoch 369/3000, 60/60 - D Loss: 2.6576940648226355e-09, G Loss: 0.1981736570596695 - 17.65 s\n",
      "Epoch 370/3000, 60/60 - D Loss: 2.521714476122483e-09, G Loss: 0.1993885189294815 - 17.86 s\n",
      "Epoch 371/3000, 60/60 - D Loss: 2.612665223056254e-09, G Loss: 0.19771647453308105 - 17.88 s\n",
      "Epoch 372/3000, 60/60 - D Loss: 2.332429874241093e-09, G Loss: 0.20318756997585297 - 17.61 s\n",
      "Epoch 373/3000, 60/60 - D Loss: 2.7980424158191575e-09, G Loss: 0.20211930572986603 - 17.74 s\n",
      "Epoch 374/3000, 60/60 - D Loss: 2.708054475109023e-09, G Loss: 0.2021687626838684 - 17.72 s\n",
      "Epoch 375/3000, 60/60 - D Loss: 2.7105789626724608e-09, G Loss: 0.2024943083524704 - 17.73 s\n",
      "Epoch 376/3000, 60/60 - D Loss: 2.026766462992491e-09, G Loss: 0.19996415078639984 - 17.61 s\n",
      "Epoch 377/3000, 60/60 - D Loss: 2.551496548763854e-09, G Loss: 0.1991875022649765 - 17.79 s\n",
      "Epoch 378/3000, 60/60 - D Loss: 2.836286941421129e-09, G Loss: 0.20021851360797882 - 17.72 s\n",
      "Epoch 379/3000, 60/60 - D Loss: 3.013535733453754e-09, G Loss: 0.20314554870128632 - 17.85 s\n",
      "Epoch 380/3000, 60/60 - D Loss: 2.7826935131147756e-09, G Loss: 0.1976320594549179 - 17.57 s\n",
      "Epoch 381/3000, 60/60 - D Loss: 2.7077187297885885e-09, G Loss: 0.1986784189939499 - 17.49 s\n",
      "Epoch 382/3000, 60/60 - D Loss: 2.3610720115141426e-09, G Loss: 0.19765231013298035 - 17.72 s\n",
      "Epoch 383/3000, 60/60 - D Loss: 3.1702392447319205e-09, G Loss: 0.19739362597465515 - 17.96 s\n",
      "Epoch 384/3000, 60/60 - D Loss: 2.649194856541026e-09, G Loss: 0.19760815799236298 - 17.46 s\n",
      "Epoch 385/3000, 60/60 - D Loss: 3.227685667350766e-09, G Loss: 0.20206080377101898 - 17.70 s\n",
      "Epoch 386/3000, 60/60 - D Loss: 2.9391554884350413e-09, G Loss: 0.19721780717372894 - 17.97 s\n",
      "Epoch 387/3000, 60/60 - D Loss: 2.3958144357294486e-09, G Loss: 0.20158956944942474 - 17.59 s\n",
      "Epoch 388/3000, 60/60 - D Loss: 3.1910347350616597e-09, G Loss: 0.19963574409484863 - 17.41 s\n",
      "Epoch 389/3000, 60/60 - D Loss: 3.2359616444765926e-09, G Loss: 0.19825080037117004 - 17.85 s\n",
      "Epoch 390/3000, 60/60 - D Loss: 3.0612195070500725e-09, G Loss: 0.19603100419044495 - 17.70 s\n",
      "Epoch 391/3000, 60/60 - D Loss: 2.4366852841017916e-09, G Loss: 0.19844388961791992 - 17.53 s\n",
      "Epoch 392/3000, 60/60 - D Loss: 3.1620627435891002e-09, G Loss: 0.19939763844013214 - 17.65 s\n",
      "Epoch 393/3000, 60/60 - D Loss: 2.632901452415126e-09, G Loss: 0.19833382964134216 - 17.59 s\n",
      "Epoch 394/3000, 60/60 - D Loss: 2.9202785328807934e-09, G Loss: 0.19772163033485413 - 17.72 s\n",
      "Epoch 395/3000, 60/60 - D Loss: 2.90461925345209e-09, G Loss: 0.1980140060186386 - 17.80 s\n",
      "Epoch 396/3000, 60/60 - D Loss: 2.733753487471624e-09, G Loss: 0.19741885364055634 - 17.44 s\n",
      "Epoch 397/3000, 60/60 - D Loss: 2.614464512940007e-09, G Loss: 0.19823171198368073 - 17.80 s\n",
      "Epoch 398/3000, 60/60 - D Loss: 2.728104041282986e-09, G Loss: 0.19731250405311584 - 17.69 s\n",
      "Epoch 399/3000, 60/60 - D Loss: 3.0337231823884814e-09, G Loss: 0.2024819552898407 - 17.52 s\n",
      "Epoch 400/3000, 60/60 - D Loss: 3.125681893867416e-09, G Loss: 0.1967613697052002 - 17.66 s\n",
      "Epoch 401/3000, 60/60 - D Loss: 2.9735600787228478e-09, G Loss: 0.1995883285999298 - 18.12 s\n",
      "Epoch 402/3000, 60/60 - D Loss: 3.2143057868139202e-09, G Loss: 0.19685064256191254 - 17.61 s\n",
      "Epoch 403/3000, 60/60 - D Loss: 3.0856109403676335e-09, G Loss: 0.19840478897094727 - 17.57 s\n",
      "Epoch 404/3000, 60/60 - D Loss: 2.894829306820945e-09, G Loss: 0.20400628447532654 - 17.72 s\n",
      "Epoch 405/3000, 60/60 - D Loss: 2.89204597769821e-09, G Loss: 0.19861823320388794 - 17.54 s\n",
      "Epoch 406/3000, 60/60 - D Loss: 2.948640907829425e-09, G Loss: 0.19611302018165588 - 17.57 s\n",
      "Epoch 407/3000, 60/60 - D Loss: 2.6158913507545734e-09, G Loss: 0.1977912336587906 - 17.55 s\n",
      "Epoch 408/3000, 60/60 - D Loss: 2.975631928359146e-09, G Loss: 0.20039670169353485 - 17.56 s\n",
      "Epoch 409/3000, 60/60 - D Loss: 2.745987957852858e-09, G Loss: 0.19634857773780823 - 17.84 s\n",
      "Epoch 410/3000, 60/60 - D Loss: 3.2081775708237004e-09, G Loss: 0.19653359055519104 - 17.74 s\n",
      "Epoch 411/3000, 60/60 - D Loss: 2.9189752698277616e-09, G Loss: 0.19628296792507172 - 17.81 s\n",
      "Epoch 412/3000, 60/60 - D Loss: 2.9906331022733212e-09, G Loss: 0.196845144033432 - 17.62 s\n",
      "Epoch 413/3000, 60/60 - D Loss: 2.40298772546943e-09, G Loss: 0.19702363014221191 - 17.59 s\n",
      "Epoch 414/3000, 60/60 - D Loss: 3.1876787598417167e-09, G Loss: 0.19771136343479156 - 17.83 s\n",
      "Epoch 415/3000, 60/60 - D Loss: 3.3020419523688282e-09, G Loss: 0.19884616136550903 - 17.44 s\n",
      "Epoch 416/3000, 60/60 - D Loss: 3.428277106642952e-09, G Loss: 0.1980973333120346 - 17.94 s\n",
      "Epoch 417/3000, 60/60 - D Loss: 2.6887848891821164e-09, G Loss: 0.2004401534795761 - 17.57 s\n",
      "Epoch 418/3000, 60/60 - D Loss: 2.5843549175674774e-09, G Loss: 0.19710521399974823 - 17.65 s\n",
      "Epoch 419/3000, 60/60 - D Loss: 3.3917553071693973e-09, G Loss: 0.19551357626914978 - 17.50 s\n",
      "Epoch 420/3000, 60/60 - D Loss: 2.7833520835351955e-09, G Loss: 0.19566984474658966 - 17.62 s\n",
      "Epoch 421/3000, 60/60 - D Loss: 2.8686353917284357e-09, G Loss: 0.19658158719539642 - 17.49 s\n",
      "Epoch 422/3000, 60/60 - D Loss: 2.48616089520981e-09, G Loss: 0.19688956439495087 - 17.50 s\n",
      "Epoch 423/3000, 60/60 - D Loss: 2.5781864074203575e-09, G Loss: 0.19603601098060608 - 17.78 s\n",
      "Epoch 424/3000, 60/60 - D Loss: 3.1728411981069016e-09, G Loss: 0.1961154341697693 - 17.53 s\n",
      "Epoch 425/3000, 60/60 - D Loss: 2.9636004900135404e-09, G Loss: 0.1976834237575531 - 17.50 s\n",
      "Epoch 426/3000, 60/60 - D Loss: 2.9288331343524376e-09, G Loss: 0.19723695516586304 - 17.54 s\n",
      "Epoch 427/3000, 60/60 - D Loss: 3.19004225812769e-09, G Loss: 0.19558309018611908 - 17.67 s\n",
      "Epoch 428/3000, 60/60 - D Loss: 3.0406370477020772e-09, G Loss: 0.19908078014850616 - 17.69 s\n",
      "Epoch 429/3000, 60/60 - D Loss: 3.149248813116845e-09, G Loss: 0.19850829243659973 - 17.77 s\n",
      "Epoch 430/3000, 60/60 - D Loss: 3.1511081105550787e-09, G Loss: 0.19917912781238556 - 17.57 s\n",
      "Epoch 431/3000, 60/60 - D Loss: 3.5861083408361694e-09, G Loss: 0.1980780065059662 - 17.90 s\n",
      "Epoch 432/3000, 60/60 - D Loss: 2.955221296840893e-09, G Loss: 0.1986161172389984 - 17.52 s\n",
      "Epoch 433/3000, 60/60 - D Loss: 3.043509312627979e-09, G Loss: 0.19648724794387817 - 17.71 s\n",
      "Epoch 434/3000, 60/60 - D Loss: 3.286163431648337e-09, G Loss: 0.1958441287279129 - 17.51 s\n",
      "Epoch 435/3000, 60/60 - D Loss: 3.0433995185097373e-09, G Loss: 0.19413143396377563 - 17.51 s\n",
      "Epoch 436/3000, 60/60 - D Loss: 2.470017995692686e-09, G Loss: 0.1973334401845932 - 17.48 s\n",
      "Epoch 437/3000, 60/60 - D Loss: 3.363987852167405e-09, G Loss: 0.19656042754650116 - 17.54 s\n",
      "Epoch 438/3000, 60/60 - D Loss: 4.00278427417633e-09, G Loss: 0.1962781846523285 - 17.49 s\n",
      "Epoch 439/3000, 60/60 - D Loss: 3.368999822173091e-09, G Loss: 0.1967371106147766 - 17.58 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 440/3000, 60/60 - D Loss: 3.2038100783449153e-09, G Loss: 0.19521383941173553 - 17.72 s\n",
      "Epoch 441/3000, 60/60 - D Loss: 3.5985900231905177e-09, G Loss: 0.1948288530111313 - 17.63 s\n",
      "Epoch 442/3000, 60/60 - D Loss: 3.0467089059960095e-09, G Loss: 0.19790984690189362 - 17.53 s\n",
      "Epoch 443/3000, 60/60 - D Loss: 3.0757678001758215e-09, G Loss: 0.19592711329460144 - 17.72 s\n",
      "Epoch 444/3000, 60/60 - D Loss: 3.554776709102647e-09, G Loss: 0.19810204207897186 - 17.65 s\n",
      "Epoch 445/3000, 60/60 - D Loss: 2.484684000214621e-09, G Loss: 0.19690735638141632 - 17.55 s\n",
      "Epoch 446/3000, 60/60 - D Loss: 3.942171793863691e-09, G Loss: 0.19866636395454407 - 17.58 s\n",
      "Epoch 447/3000, 60/60 - D Loss: 3.842301359024969e-09, G Loss: 0.19660472869873047 - 18.00 s\n",
      "Epoch 448/3000, 60/60 - D Loss: 3.977376386576914e-09, G Loss: 0.19795511662960052 - 17.40 s\n",
      "Epoch 449/3000, 60/60 - D Loss: 2.8586683228915e-09, G Loss: 0.19557200372219086 - 17.55 s\n",
      "Epoch 450/3000, 60/60 - D Loss: 2.7000494964846133e-09, G Loss: 0.1971846967935562 - 17.59 s\n",
      "Epoch 451/3000, 60/60 - D Loss: 3.755470892596868e-09, G Loss: 0.19598893821239471 - 17.96 s\n",
      "Epoch 452/3000, 60/60 - D Loss: 3.722808797346211e-09, G Loss: 0.197509303689003 - 17.67 s\n",
      "Epoch 453/3000, 60/60 - D Loss: 2.3585602776377e-09, G Loss: 0.19560031592845917 - 17.75 s\n",
      "Epoch 454/3000, 60/60 - D Loss: 3.5146975468913766e-09, G Loss: 0.19480301439762115 - 17.54 s\n",
      "Epoch 455/3000, 60/60 - D Loss: 3.513545974997978e-09, G Loss: 0.19547708332538605 - 17.58 s\n",
      "Epoch 456/3000, 60/60 - D Loss: 3.83679378546109e-09, G Loss: 0.1944812834262848 - 17.55 s\n",
      "Epoch 457/3000, 60/60 - D Loss: 3.664620815135855e-09, G Loss: 0.1972101479768753 - 17.85 s\n",
      "Epoch 458/3000, 60/60 - D Loss: 3.637292078639831e-09, G Loss: 0.1986575871706009 - 17.94 s\n",
      "Epoch 459/3000, 60/60 - D Loss: 3.484972491141214e-09, G Loss: 0.19532059133052826 - 18.10 s\n",
      "Epoch 460/3000, 60/60 - D Loss: 4.372994098778005e-09, G Loss: 0.19467519223690033 - 17.63 s\n",
      "Epoch 461/3000, 60/60 - D Loss: 2.596231882256994e-09, G Loss: 0.19687698781490326 - 17.78 s\n",
      "Epoch 462/3000, 60/60 - D Loss: 4.014111997657777e-09, G Loss: 0.19548538327217102 - 18.08 s\n",
      "Epoch 463/3000, 60/60 - D Loss: 3.846971966675605e-09, G Loss: 0.1963796317577362 - 17.67 s\n",
      "Epoch 464/3000, 60/60 - D Loss: 3.736349455524257e-09, G Loss: 0.19700953364372253 - 17.56 s\n",
      "Epoch 465/3000, 60/60 - D Loss: 3.727670692954543e-09, G Loss: 0.19830487668514252 - 17.42 s\n",
      "Epoch 466/3000, 60/60 - D Loss: 3.3872446930649502e-09, G Loss: 0.19553600251674652 - 17.61 s\n",
      "Epoch 467/3000, 60/60 - D Loss: 3.1311308199000187e-09, G Loss: 0.19629186391830444 - 17.60 s\n",
      "Epoch 468/3000, 60/60 - D Loss: 3.6719830023845823e-09, G Loss: 0.1963648945093155 - 17.53 s\n",
      "Epoch 469/3000, 60/60 - D Loss: 3.2941623526183683e-09, G Loss: 0.1951574981212616 - 17.60 s\n",
      "Epoch 470/3000, 60/60 - D Loss: 4.360391752528114e-09, G Loss: 0.19610445201396942 - 17.56 s\n",
      "Epoch 471/3000, 60/60 - D Loss: 3.942302158332911e-09, G Loss: 0.1972484588623047 - 17.71 s\n",
      "Epoch 472/3000, 60/60 - D Loss: 3.312606057515044e-09, G Loss: 0.20031224191188812 - 18.27 s\n",
      "Epoch 473/3000, 60/60 - D Loss: 3.811516505192181e-09, G Loss: 0.19488872587680817 - 17.62 s\n",
      "Epoch 474/3000, 60/60 - D Loss: 4.335260817001085e-09, G Loss: 0.1957247257232666 - 17.71 s\n",
      "Epoch 475/3000, 60/60 - D Loss: 2.898592789402077e-09, G Loss: 0.19465549290180206 - 17.78 s\n",
      "Epoch 476/3000, 60/60 - D Loss: 3.845274623021089e-09, G Loss: 0.19772958755493164 - 17.85 s\n",
      "Epoch 477/3000, 60/60 - D Loss: 3.826799811063042e-09, G Loss: 0.19482530653476715 - 18.24 s\n",
      "Epoch 478/3000, 60/60 - D Loss: 4.738475441884438e-09, G Loss: 0.1958579123020172 - 17.58 s\n",
      "Epoch 479/3000, 60/60 - D Loss: 4.2575122628785156e-09, G Loss: 0.1943107396364212 - 17.64 s\n",
      "Epoch 480/3000, 60/60 - D Loss: 3.2659375412413816e-09, G Loss: 0.19654758274555206 - 17.48 s\n",
      "Epoch 481/3000, 60/60 - D Loss: 3.23731203649702e-09, G Loss: 0.19561290740966797 - 17.63 s\n",
      "Epoch 482/3000, 60/60 - D Loss: 4.180917768242809e-09, G Loss: 0.19541141390800476 - 17.61 s\n",
      "Epoch 483/3000, 60/60 - D Loss: 3.9254010701650355e-09, G Loss: 0.19522717595100403 - 17.72 s\n",
      "Epoch 484/3000, 60/60 - D Loss: 4.159003745562995e-09, G Loss: 0.19628013670444489 - 17.69 s\n",
      "Epoch 485/3000, 60/60 - D Loss: 4.492512147014072e-09, G Loss: 0.19377665221691132 - 17.85 s\n",
      "Epoch 486/3000, 60/60 - D Loss: 4.108267934177512e-09, G Loss: 0.19524027407169342 - 17.66 s\n",
      "Epoch 487/3000, 60/60 - D Loss: 3.449239678599003e-09, G Loss: 0.19442062079906464 - 17.83 s\n",
      "Epoch 488/3000, 60/60 - D Loss: 3.6799897573658313e-09, G Loss: 0.19516658782958984 - 17.86 s\n",
      "Epoch 489/3000, 60/60 - D Loss: 4.353621609054503e-09, G Loss: 0.19352784752845764 - 17.76 s\n",
      "Epoch 490/3000, 60/60 - D Loss: 4.074247758611982e-09, G Loss: 0.19408024847507477 - 17.76 s\n",
      "Epoch 491/3000, 60/60 - D Loss: 4.465046631041414e-09, G Loss: 0.1949169784784317 - 17.79 s\n",
      "Epoch 492/3000, 60/60 - D Loss: 3.2639118352495444e-09, G Loss: 0.19440744817256927 - 17.76 s\n",
      "Epoch 493/3000, 60/60 - D Loss: 4.659070764606588e-09, G Loss: 0.1956697553396225 - 17.79 s\n",
      "Epoch 494/3000, 60/60 - D Loss: 4.0051732971835285e-09, G Loss: 0.19427992403507233 - 17.70 s\n",
      "Epoch 495/3000, 60/60 - D Loss: 3.433784583062316e-09, G Loss: 0.19457967579364777 - 17.82 s\n",
      "Epoch 496/3000, 60/60 - D Loss: 5.3446745952590025e-09, G Loss: 0.1951020359992981 - 17.74 s\n",
      "Epoch 497/3000, 60/60 - D Loss: 3.6908502434873647e-09, G Loss: 0.20016179978847504 - 17.84 s\n",
      "Epoch 498/3000, 60/60 - D Loss: 3.964813088952468e-09, G Loss: 0.19463685154914856 - 17.52 s\n",
      "Epoch 499/3000, 60/60 - D Loss: 2.5285582655465433e-09, G Loss: 0.19313527643680573 - 17.69 s\n",
      "Epoch 500/3000, 60/60 - D Loss: 3.5296828584940876e-09, G Loss: 0.19412685930728912 - 17.74 s\n",
      "Epoch 501/3000, 60/60 - D Loss: 4.744076635004868e-09, G Loss: 0.19737234711647034 - 17.67 s\n",
      "Epoch 502/3000, 60/60 - D Loss: 3.600524579872033e-09, G Loss: 0.19511006772518158 - 17.74 s\n",
      "Epoch 503/3000, 60/60 - D Loss: 3.2876740843623686e-09, G Loss: 0.19537407159805298 - 17.46 s\n",
      "Epoch 504/3000, 60/60 - D Loss: 4.755595122557521e-09, G Loss: 0.19627156853675842 - 17.74 s\n",
      "Epoch 505/3000, 60/60 - D Loss: 4.108968172655825e-09, G Loss: 0.1944783478975296 - 17.76 s\n",
      "Epoch 506/3000, 60/60 - D Loss: 3.93032424661488e-09, G Loss: 0.19576136767864227 - 17.77 s\n",
      "Epoch 507/3000, 60/60 - D Loss: 4.951161879790389e-09, G Loss: 0.1973589062690735 - 17.77 s\n",
      "Epoch 508/3000, 60/60 - D Loss: 5.032260882303907e-09, G Loss: 0.19338226318359375 - 17.84 s\n",
      "Epoch 509/3000, 60/60 - D Loss: 4.28719680589662e-09, G Loss: 0.19495519995689392 - 17.83 s\n",
      "Epoch 510/3000, 60/60 - D Loss: 2.64938984639862e-09, G Loss: 0.19533401727676392 - 17.44 s\n",
      "Epoch 511/3000, 60/60 - D Loss: 4.408382991982762e-09, G Loss: 0.1945285201072693 - 17.00 s\n",
      "Epoch 512/3000, 60/60 - D Loss: 5.24375493027307e-09, G Loss: 0.19326069951057434 - 17.58 s\n",
      "Epoch 513/3000, 60/60 - D Loss: 4.3533778075477425e-09, G Loss: 0.19415944814682007 - 17.13 s\n",
      "Epoch 514/3000, 60/60 - D Loss: 5.556449016175247e-09, G Loss: 0.19664627313613892 - 17.46 s\n",
      "Epoch 515/3000, 60/60 - D Loss: 4.7635331061612884e-09, G Loss: 0.19426758587360382 - 17.83 s\n",
      "Epoch 516/3000, 60/60 - D Loss: 4.945699010050486e-09, G Loss: 0.19385938346385956 - 17.89 s\n",
      "Epoch 517/3000, 60/60 - D Loss: 3.872236753466041e-09, G Loss: 0.19452829658985138 - 17.85 s\n",
      "Epoch 518/3000, 60/60 - D Loss: 5.453978504932344e-09, G Loss: 0.19552971422672272 - 17.41 s\n",
      "Epoch 519/3000, 60/60 - D Loss: 4.051533955551356e-09, G Loss: 0.19389663636684418 - 17.80 s\n",
      "Epoch 520/3000, 60/60 - D Loss: 4.182856512546795e-09, G Loss: 0.19445648789405823 - 17.73 s\n",
      "Epoch 521/3000, 60/60 - D Loss: 3.4221402031575643e-09, G Loss: 0.19689370691776276 - 17.72 s\n",
      "Epoch 522/3000, 60/60 - D Loss: 3.859401069561397e-09, G Loss: 0.19368669390678406 - 17.86 s\n",
      "Epoch 523/3000, 60/60 - D Loss: 5.8225237756226456e-09, G Loss: 0.1944403052330017 - 17.82 s\n",
      "Epoch 524/3000, 60/60 - D Loss: 4.597205977008301e-09, G Loss: 0.19429831206798553 - 17.61 s\n",
      "Epoch 525/3000, 60/60 - D Loss: 4.323905858361066e-09, G Loss: 0.20513853430747986 - 17.67 s\n",
      "Epoch 526/3000, 60/60 - D Loss: 4.121519125988016e-09, G Loss: 0.19486914575099945 - 17.84 s\n",
      "Epoch 527/3000, 60/60 - D Loss: 5.219235332115657e-09, G Loss: 0.19329184293746948 - 17.87 s\n",
      "Epoch 528/3000, 60/60 - D Loss: 3.5088943139971462e-09, G Loss: 0.19346009194850922 - 17.75 s\n",
      "Epoch 529/3000, 60/60 - D Loss: 5.307386655162283e-09, G Loss: 0.1945738047361374 - 17.45 s\n",
      "Epoch 530/3000, 60/60 - D Loss: 3.1007013692740948e-09, G Loss: 0.1934308260679245 - 17.52 s\n",
      "Epoch 531/3000, 60/60 - D Loss: 4.615270450536446e-09, G Loss: 0.19849273562431335 - 17.60 s\n",
      "Epoch 532/3000, 60/60 - D Loss: 3.769343150106241e-09, G Loss: 0.19445852935314178 - 17.51 s\n",
      "Epoch 533/3000, 60/60 - D Loss: 3.9159343198647e-09, G Loss: 0.1936044543981552 - 17.78 s\n",
      "Epoch 534/3000, 60/60 - D Loss: 3.3090272813507404e-09, G Loss: 0.1949986070394516 - 17.76 s\n",
      "Epoch 535/3000, 60/60 - D Loss: 1.9232352238551798e-09, G Loss: 0.1991949826478958 - 17.83 s\n",
      "Epoch 536/3000, 60/60 - D Loss: 5.3284252392316045e-09, G Loss: 0.1939261108636856 - 17.71 s\n",
      "Epoch 537/3000, 60/60 - D Loss: 4.2162008641322135e-09, G Loss: 0.19256025552749634 - 17.81 s\n",
      "Epoch 538/3000, 60/60 - D Loss: 4.474358741152207e-09, G Loss: 0.19485440850257874 - 18.01 s\n",
      "Epoch 539/3000, 60/60 - D Loss: 4.916849979247351e-09, G Loss: 0.1941317915916443 - 17.98 s\n",
      "Epoch 540/3000, 60/60 - D Loss: 4.9428930289086015e-09, G Loss: 0.19370774924755096 - 17.72 s\n",
      "Epoch 541/3000, 60/60 - D Loss: 5.29480032734897e-09, G Loss: 0.1955266147851944 - 17.84 s\n",
      "Epoch 542/3000, 60/60 - D Loss: 4.8915147800310255e-09, G Loss: 0.19304242730140686 - 17.87 s\n",
      "Epoch 543/3000, 60/60 - D Loss: 4.2287638113425174e-09, G Loss: 0.20182278752326965 - 17.81 s\n",
      "Epoch 544/3000, 60/60 - D Loss: 2.8946784205730047e-09, G Loss: 0.1952866017818451 - 17.57 s\n",
      "Epoch 545/3000, 60/60 - D Loss: 3.951092585052773e-09, G Loss: 0.19206862151622772 - 17.92 s\n",
      "Epoch 546/3000, 60/60 - D Loss: 3.605127855965673e-09, G Loss: 0.19912825524806976 - 17.80 s\n",
      "Epoch 547/3000, 60/60 - D Loss: 5.445484622251806e-09, G Loss: 0.19359956681728363 - 17.64 s\n",
      "Epoch 548/3000, 60/60 - D Loss: 3.6969128383579353e-09, G Loss: 0.19487638771533966 - 17.59 s\n",
      "Epoch 549/3000, 60/60 - D Loss: 4.829925827343207e-09, G Loss: 0.19423872232437134 - 17.83 s\n",
      "Epoch 550/3000, 60/60 - D Loss: 6.969266542339314e-09, G Loss: 0.19443626701831818 - 17.74 s\n",
      "Epoch 551/3000, 60/60 - D Loss: 4.371925058088699e-09, G Loss: 0.19494278728961945 - 17.70 s\n",
      "Epoch 552/3000, 60/60 - D Loss: 4.44915713992966e-09, G Loss: 0.19510652124881744 - 17.69 s\n",
      "Epoch 553/3000, 60/60 - D Loss: 5.714869067557915e-09, G Loss: 0.19377674162387848 - 18.15 s\n",
      "Epoch 554/3000, 60/60 - D Loss: 5.3093938308379496e-09, G Loss: 0.19242136180400848 - 17.70 s\n",
      "Epoch 555/3000, 60/60 - D Loss: 3.985581788212844e-09, G Loss: 0.19434119760990143 - 17.72 s\n",
      "Epoch 556/3000, 60/60 - D Loss: 4.623653748064838e-09, G Loss: 0.19459141790866852 - 17.44 s\n",
      "Epoch 557/3000, 60/60 - D Loss: 4.610136897231776e-09, G Loss: 0.1957666426897049 - 17.77 s\n",
      "Epoch 558/3000, 60/60 - D Loss: 2.070394286568522e-09, G Loss: 0.1988968551158905 - 17.90 s\n",
      "Epoch 559/3000, 60/60 - D Loss: 5.585588415402132e-09, G Loss: 0.19274264574050903 - 17.54 s\n",
      "Epoch 560/3000, 60/60 - D Loss: 5.071362361303011e-09, G Loss: 0.1994132250547409 - 17.75 s\n",
      "Epoch 561/3000, 60/60 - D Loss: 4.48003314307277e-09, G Loss: 0.19762521982192993 - 17.63 s\n",
      "Epoch 562/3000, 60/60 - D Loss: 4.6087878097234025e-09, G Loss: 0.19438068568706512 - 17.72 s\n",
      "Epoch 563/3000, 60/60 - D Loss: 3.2842345856765043e-09, G Loss: 0.1994551122188568 - 17.73 s\n",
      "Epoch 564/3000, 60/60 - D Loss: 5.034061455883032e-09, G Loss: 0.1966859996318817 - 17.92 s\n",
      "Epoch 565/3000, 60/60 - D Loss: 5.664276950256841e-09, G Loss: 0.19682955741882324 - 17.71 s\n",
      "Epoch 566/3000, 60/60 - D Loss: 4.5601521135585354e-09, G Loss: 0.1931200623512268 - 17.65 s\n",
      "Epoch 567/3000, 60/60 - D Loss: 3.718294720733706e-09, G Loss: 0.1940261572599411 - 17.47 s\n",
      "Epoch 568/3000, 60/60 - D Loss: 3.958416344707061e-09, G Loss: 0.19304825365543365 - 17.96 s\n",
      "Epoch 569/3000, 60/60 - D Loss: 5.3888492106135555e-09, G Loss: 0.19311946630477905 - 17.76 s\n",
      "Epoch 570/3000, 60/60 - D Loss: 5.325270176215202e-09, G Loss: 0.19373933970928192 - 17.97 s\n",
      "Epoch 571/3000, 60/60 - D Loss: 4.764442416982373e-09, G Loss: 0.19332680106163025 - 17.72 s\n",
      "Epoch 572/3000, 60/60 - D Loss: 4.230549473238643e-09, G Loss: 0.1926262378692627 - 17.71 s\n",
      "Epoch 573/3000, 60/60 - D Loss: 6.223645172137582e-09, G Loss: 0.19478942453861237 - 18.08 s\n",
      "Epoch 574/3000, 60/60 - D Loss: 3.776640604413739e-09, G Loss: 0.1921524554491043 - 17.76 s\n",
      "Epoch 575/3000, 60/60 - D Loss: 7.178977665805997e-09, G Loss: 0.20248590409755707 - 17.71 s\n",
      "Epoch 576/3000, 60/60 - D Loss: 2.471218549238152e-09, G Loss: 0.19217495620250702 - 17.58 s\n",
      "Epoch 577/3000, 60/60 - D Loss: 6.179107295278818e-09, G Loss: 0.193645179271698 - 17.60 s\n",
      "Epoch 578/3000, 60/60 - D Loss: 4.2478625825181915e-09, G Loss: 0.19404011964797974 - 17.65 s\n",
      "Epoch 579/3000, 60/60 - D Loss: 5.514261693095879e-09, G Loss: 0.1941167265176773 - 17.73 s\n",
      "Epoch 580/3000, 60/60 - D Loss: 5.6765376114931154e-09, G Loss: 0.19310669600963593 - 17.90 s\n",
      "Epoch 581/3000, 60/60 - D Loss: 4.184651888894386e-09, G Loss: 0.1936904788017273 - 17.65 s\n",
      "Epoch 582/3000, 60/60 - D Loss: 6.492465138424608e-09, G Loss: 0.19522666931152344 - 17.83 s\n",
      "Epoch 583/3000, 60/60 - D Loss: 2.9469188200792473e-09, G Loss: 0.20477011799812317 - 17.95 s\n",
      "Epoch 584/3000, 60/60 - D Loss: 4.905044950070936e-09, G Loss: 0.19983144104480743 - 17.91 s\n",
      "Epoch 585/3000, 60/60 - D Loss: 6.211198257111139e-09, G Loss: 0.1948477178812027 - 17.73 s\n",
      "Epoch 586/3000, 60/60 - D Loss: 3.2496407439963626e-09, G Loss: 0.1937650889158249 - 17.76 s\n",
      "Epoch 587/3000, 60/60 - D Loss: 5.125308257664107e-09, G Loss: 0.1964283436536789 - 17.72 s\n",
      "Epoch 588/3000, 60/60 - D Loss: 5.876574352137176e-09, G Loss: 0.1919652819633484 - 17.79 s\n",
      "Epoch 589/3000, 60/60 - D Loss: 5.008074999840462e-09, G Loss: 0.19337773323059082 - 17.68 s\n",
      "Epoch 590/3000, 60/60 - D Loss: 3.74764822647089e-09, G Loss: 0.19282600283622742 - 17.68 s\n",
      "Epoch 591/3000, 60/60 - D Loss: 5.9946472855121335e-09, G Loss: 0.1938311606645584 - 17.76 s\n",
      "Epoch 592/3000, 60/60 - D Loss: 5.220930444915783e-09, G Loss: 0.19352009892463684 - 17.83 s\n",
      "Epoch 593/3000, 60/60 - D Loss: 3.1249416873602165e-09, G Loss: 0.1938021034002304 - 17.65 s\n",
      "Epoch 594/3000, 60/60 - D Loss: 5.821022872054549e-09, G Loss: 0.1934777796268463 - 17.89 s\n",
      "Epoch 595/3000, 60/60 - D Loss: 6.7564925355922956e-09, G Loss: 0.19278475642204285 - 17.47 s\n",
      "Epoch 596/3000, 60/60 - D Loss: 6.37327547681199e-09, G Loss: 0.19352762401103973 - 17.75 s\n",
      "Epoch 597/3000, 60/60 - D Loss: 5.645532950848686e-09, G Loss: 0.19433511793613434 - 18.02 s\n",
      "Epoch 598/3000, 60/60 - D Loss: 6.303014558201037e-09, G Loss: 0.1956046223640442 - 17.68 s\n",
      "Epoch 599/3000, 60/60 - D Loss: 6.225032777446016e-09, G Loss: 0.1938309222459793 - 17.97 s\n",
      "Epoch 600/3000, 60/60 - D Loss: 6.722604101866425e-09, G Loss: 0.19430987536907196 - 17.61 s\n",
      "Epoch 601/3000, 60/60 - D Loss: 6.153991885526899e-09, G Loss: 0.19358287751674652 - 17.77 s\n",
      "Epoch 602/3000, 60/60 - D Loss: 6.62761268782619e-09, G Loss: 0.1927313506603241 - 17.77 s\n",
      "Epoch 603/3000, 60/60 - D Loss: 3.698988330913533e-09, G Loss: 0.19496966898441315 - 17.51 s\n",
      "Epoch 604/3000, 60/60 - D Loss: 6.002628356177597e-09, G Loss: 0.19366392493247986 - 17.74 s\n",
      "Epoch 605/3000, 60/60 - D Loss: 5.5629437063775544e-09, G Loss: 0.19421860575675964 - 17.72 s\n",
      "Epoch 606/3000, 60/60 - D Loss: 7.222459564443229e-09, G Loss: 0.19337551295757294 - 17.90 s\n",
      "Epoch 607/3000, 60/60 - D Loss: 5.129899130484894e-09, G Loss: 0.1920824944972992 - 17.90 s\n",
      "Epoch 608/3000, 60/60 - D Loss: 4.024422618070789e-09, G Loss: 0.19796067476272583 - 17.82 s\n",
      "Epoch 609/3000, 60/60 - D Loss: 4.98290620917774e-09, G Loss: 0.19386421144008636 - 17.69 s\n",
      "Epoch 610/3000, 60/60 - D Loss: 2.597168466400568e-09, G Loss: 0.19546815752983093 - 17.81 s\n",
      "Epoch 611/3000, 60/60 - D Loss: 6.31065292383548e-09, G Loss: 0.19356557726860046 - 17.83 s\n",
      "Epoch 612/3000, 60/60 - D Loss: 5.0810726112682225e-09, G Loss: 0.1998012810945511 - 17.52 s\n",
      "Epoch 613/3000, 60/60 - D Loss: 5.344486082858868e-09, G Loss: 0.1969590038061142 - 17.88 s\n",
      "Epoch 614/3000, 60/60 - D Loss: 5.62459383235403e-09, G Loss: 0.19449572265148163 - 17.75 s\n",
      "Epoch 615/3000, 60/60 - D Loss: 6.46161251241506e-09, G Loss: 0.19312071800231934 - 17.68 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 616/3000, 60/60 - D Loss: 6.850539874952988e-09, G Loss: 0.19301535189151764 - 17.67 s\n",
      "Epoch 617/3000, 60/60 - D Loss: 3.841800579051924e-09, G Loss: 0.192821204662323 - 17.90 s\n",
      "Epoch 618/3000, 60/60 - D Loss: 7.74368597369901e-09, G Loss: 0.19417144358158112 - 18.00 s\n",
      "Epoch 619/3000, 60/60 - D Loss: 4.920736571684126e-09, G Loss: 0.191740483045578 - 17.91 s\n",
      "Epoch 620/3000, 60/60 - D Loss: 5.396236561761025e-09, G Loss: 0.19280311465263367 - 17.65 s\n",
      "Epoch 621/3000, 60/60 - D Loss: 6.653916926127601e-09, G Loss: 0.1922667771577835 - 17.74 s\n",
      "Epoch 622/3000, 60/60 - D Loss: 7.694674845210026e-09, G Loss: 0.19363349676132202 - 17.88 s\n",
      "Epoch 623/3000, 60/60 - D Loss: 6.523123870921799e-09, G Loss: 0.19133888185024261 - 17.71 s\n",
      "Epoch 624/3000, 60/60 - D Loss: 4.84892174676399e-09, G Loss: 0.19237588346004486 - 17.66 s\n",
      "Epoch 625/3000, 60/60 - D Loss: 5.631858784882482e-09, G Loss: 0.19256886839866638 - 17.73 s\n",
      "Epoch 626/3000, 60/60 - D Loss: 5.417533439217026e-09, G Loss: 0.19335943460464478 - 17.79 s\n",
      "Epoch 627/3000, 60/60 - D Loss: 5.6117104609010315e-09, G Loss: 0.19335822761058807 - 17.81 s\n",
      "Epoch 628/3000, 60/60 - D Loss: 4.285102935680518e-09, G Loss: 0.19322173297405243 - 18.09 s\n",
      "Epoch 629/3000, 60/60 - D Loss: 2.890988892723101e-09, G Loss: 0.19149599969387054 - 18.10 s\n",
      "Epoch 630/3000, 60/60 - D Loss: 5.431753772461301e-09, G Loss: 0.19508031010627747 - 17.55 s\n",
      "Epoch 631/3000, 60/60 - D Loss: 8.168767632807383e-09, G Loss: 0.19473324716091156 - 17.72 s\n",
      "Epoch 632/3000, 60/60 - D Loss: 7.504778061148687e-09, G Loss: 0.19220225512981415 - 17.99 s\n",
      "Epoch 633/3000, 60/60 - D Loss: 3.859676026701786e-09, G Loss: 0.19342473149299622 - 17.84 s\n",
      "Epoch 634/3000, 60/60 - D Loss: 6.0222369209494975e-09, G Loss: 0.19160546362400055 - 17.62 s\n",
      "Epoch 635/3000, 60/60 - D Loss: 4.150830176102849e-09, G Loss: 0.193111389875412 - 17.50 s\n",
      "Epoch 636/3000, 60/60 - D Loss: 5.383110172896277e-09, G Loss: 0.19338664412498474 - 17.64 s\n",
      "Epoch 637/3000, 60/60 - D Loss: 7.2635392960734846e-09, G Loss: 0.19151510298252106 - 17.60 s\n",
      "Epoch 638/3000, 60/60 - D Loss: 7.526520318448804e-09, G Loss: 0.19770744442939758 - 17.70 s\n",
      "Epoch 639/3000, 60/60 - D Loss: 7.0853614225763195e-09, G Loss: 0.1930253654718399 - 17.67 s\n",
      "Epoch 640/3000, 60/60 - D Loss: 4.063714274804564e-09, G Loss: 0.19426371157169342 - 17.80 s\n",
      "Epoch 641/3000, 60/60 - D Loss: 5.426851312079206e-09, G Loss: 0.19429922103881836 - 18.15 s\n",
      "Epoch 642/3000, 60/60 - D Loss: 2.8901627618926895e-09, G Loss: 0.19256021082401276 - 18.00 s\n",
      "Epoch 643/3000, 60/60 - D Loss: 7.129442229289462e-09, G Loss: 0.19319646060466766 - 17.64 s\n",
      "Epoch 644/3000, 60/60 - D Loss: 4.0485066722661944e-09, G Loss: 0.1978209912776947 - 18.12 s\n",
      "Epoch 645/3000, 60/60 - D Loss: 6.8838482240873855e-09, G Loss: 0.19351324439048767 - 17.61 s\n",
      "Epoch 646/3000, 60/60 - D Loss: 5.1021337814372014e-09, G Loss: 0.1917354166507721 - 17.68 s\n",
      "Epoch 647/3000, 60/60 - D Loss: 7.746608292336088e-09, G Loss: 0.19903667271137238 - 17.67 s\n",
      "Epoch 648/3000, 60/60 - D Loss: 5.103637276682171e-09, G Loss: 0.19096390902996063 - 17.63 s\n",
      "Epoch 649/3000, 60/60 - D Loss: 6.227415329934649e-09, G Loss: 0.19338707625865936 - 17.66 s\n",
      "Epoch 650/3000, 60/60 - D Loss: 6.714160744741848e-09, G Loss: 0.1944756805896759 - 17.80 s\n",
      "Epoch 651/3000, 60/60 - D Loss: 4.817421350722384e-09, G Loss: 0.19373171031475067 - 17.43 s\n",
      "Epoch 652/3000, 60/60 - D Loss: 2.365887041833048e-09, G Loss: 0.19420532882213593 - 17.46 s\n",
      "Epoch 653/3000, 60/60 - D Loss: 6.4220149337268495e-09, G Loss: 0.1937521994113922 - 17.74 s\n",
      "Epoch 654/3000, 60/60 - D Loss: 4.559884948796e-09, G Loss: 0.19373276829719543 - 17.69 s\n",
      "Epoch 655/3000, 60/60 - D Loss: 6.397184261486277e-09, G Loss: 0.1933046281337738 - 17.81 s\n",
      "Epoch 656/3000, 60/60 - D Loss: 2.3161105325297804e-09, G Loss: 0.19212815165519714 - 17.64 s\n",
      "Epoch 657/3000, 60/60 - D Loss: 4.913398257699875e-09, G Loss: 0.19232264161109924 - 17.71 s\n",
      "Epoch 658/3000, 60/60 - D Loss: 5.832014659395979e-09, G Loss: 0.19193337857723236 - 17.58 s\n",
      "Epoch 659/3000, 60/60 - D Loss: 7.041558352030575e-09, G Loss: 0.19602341949939728 - 17.91 s\n",
      "Epoch 660/3000, 60/60 - D Loss: 8.441757480548429e-09, G Loss: 0.19502763450145721 - 17.55 s\n",
      "Epoch 661/3000, 60/60 - D Loss: 3.128563734466905e-09, G Loss: 0.19247056543827057 - 17.80 s\n",
      "Epoch 662/3000, 60/60 - D Loss: 5.805806476202879e-09, G Loss: 0.19318465888500214 - 17.74 s\n",
      "Epoch 663/3000, 60/60 - D Loss: 3.039957768152801e-09, G Loss: 0.1939029097557068 - 17.75 s\n",
      "Epoch 664/3000, 60/60 - D Loss: 2.9495436579263234e-09, G Loss: 0.19323822855949402 - 17.40 s\n",
      "Epoch 665/3000, 60/60 - D Loss: 2.7191630613820905e-09, G Loss: 0.19369728863239288 - 17.67 s\n",
      "Epoch 666/3000, 60/60 - D Loss: 4.17751802248012e-09, G Loss: 0.1932031661272049 - 17.68 s\n",
      "Epoch 667/3000, 60/60 - D Loss: 1.978886714215644e-09, G Loss: 0.1909334808588028 - 17.70 s\n",
      "Epoch 668/3000, 60/60 - D Loss: 3.485364295785498e-09, G Loss: 0.1939862072467804 - 17.50 s\n",
      "Epoch 669/3000, 60/60 - D Loss: 9.961675975084239e-07, G Loss: 0.1931348443031311 - 17.63 s\n",
      "Epoch 670/3000, 60/60 - D Loss: 0.0004568775239022216, G Loss: 0.19377250969409943 - 17.54 s\n",
      "Epoch 671/3000, 60/60 - D Loss: 5.478398543345975e-05, G Loss: 0.19394735991954803 - 17.67 s\n",
      "Epoch 672/3000, 60/60 - D Loss: 4.213825104670832e-05, G Loss: 0.19432587921619415 - 17.55 s\n",
      "Epoch 673/3000, 60/60 - D Loss: 3.302765526314033e-05, G Loss: 0.19458428025245667 - 17.70 s\n",
      "Epoch 674/3000, 60/60 - D Loss: 2.7138131372339558e-05, G Loss: 0.19357988238334656 - 17.85 s\n",
      "Epoch 675/3000, 60/60 - D Loss: 2.2911815449333517e-05, G Loss: 0.1938314437866211 - 17.92 s\n",
      "Epoch 676/3000, 60/60 - D Loss: 2.2145333787193522e-05, G Loss: 0.19225820899009705 - 17.74 s\n",
      "Epoch 677/3000, 60/60 - D Loss: 2.0126241906837095e-05, G Loss: 0.19358967244625092 - 17.62 s\n",
      "Epoch 678/3000, 60/60 - D Loss: 1.8805395484378096e-05, G Loss: 0.19200268387794495 - 17.46 s\n",
      "Epoch 679/3000, 60/60 - D Loss: 2.0982231944799423e-05, G Loss: 0.191915825009346 - 17.78 s\n",
      "Epoch 680/3000, 60/60 - D Loss: 1.827512460295111e-05, G Loss: 0.19137002527713776 - 17.72 s\n",
      "Epoch 681/3000, 60/60 - D Loss: 1.6557991330046207e-05, G Loss: 0.19428130984306335 - 17.73 s\n",
      "Epoch 682/3000, 60/60 - D Loss: 1.2979495295439847e-05, G Loss: 0.1925075650215149 - 17.77 s\n",
      "Epoch 683/3000, 60/60 - D Loss: 1.3752424820268061e-05, G Loss: 0.19264093041419983 - 17.61 s\n",
      "Epoch 684/3000, 60/60 - D Loss: 1.2193530437798472e-05, G Loss: 0.1957051008939743 - 17.56 s\n",
      "Epoch 685/3000, 60/60 - D Loss: 1.1861789516842691e-05, G Loss: 0.19202199578285217 - 17.59 s\n",
      "Epoch 686/3000, 60/60 - D Loss: 1.0455378287588246e-05, G Loss: 0.1943133920431137 - 17.78 s\n",
      "Epoch 687/3000, 60/60 - D Loss: 8.710341944606625e-06, G Loss: 0.19368433952331543 - 17.55 s\n",
      "Epoch 688/3000, 60/60 - D Loss: 9.07556022866629e-06, G Loss: 0.19523857533931732 - 17.61 s\n",
      "Epoch 689/3000, 60/60 - D Loss: 9.197540748573374e-06, G Loss: 0.19598129391670227 - 17.55 s\n",
      "Epoch 690/3000, 60/60 - D Loss: 9.37280674406793e-06, G Loss: 0.1988021284341812 - 18.05 s\n",
      "Epoch 691/3000, 60/60 - D Loss: 8.106314226097311e-06, G Loss: 0.19234134256839752 - 17.75 s\n",
      "Epoch 692/3000, 60/60 - D Loss: 9.06505874809227e-06, G Loss: 0.19140025973320007 - 17.85 s\n",
      "Epoch 693/3000, 60/60 - D Loss: 7.743164815110504e-06, G Loss: 0.20141008496284485 - 17.74 s\n",
      "Epoch 694/3000, 60/60 - D Loss: 7.0124010562722106e-06, G Loss: 0.19322755932807922 - 17.85 s\n",
      "Epoch 695/3000, 60/60 - D Loss: 6.394714318957995e-06, G Loss: 0.19356600940227509 - 17.68 s\n",
      "Epoch 696/3000, 60/60 - D Loss: 6.83068697071576e-06, G Loss: 0.19287000596523285 - 17.59 s\n",
      "Epoch 697/3000, 60/60 - D Loss: 5.456684220916941e-06, G Loss: 0.19133850932121277 - 17.51 s\n",
      "Epoch 698/3000, 60/60 - D Loss: 5.831768021380412e-06, G Loss: 0.1927652657032013 - 17.57 s\n",
      "Epoch 699/3000, 60/60 - D Loss: 5.902614702790743e-06, G Loss: 0.1932658553123474 - 17.53 s\n",
      "Epoch 700/3000, 60/60 - D Loss: 6.463967565650819e-06, G Loss: 0.1910993754863739 - 17.91 s\n",
      "Epoch 701/3000, 60/60 - D Loss: 5.724969241782674e-06, G Loss: 0.19420583546161652 - 17.60 s\n",
      "Epoch 702/3000, 60/60 - D Loss: 5.802791974929278e-06, G Loss: 0.19137172400951385 - 17.60 s\n",
      "Epoch 703/3000, 60/60 - D Loss: 6.564561772393063e-06, G Loss: 0.19380894303321838 - 17.80 s\n",
      "Epoch 704/3000, 60/60 - D Loss: 4.775999286721344e-06, G Loss: 0.1932182013988495 - 17.79 s\n",
      "Epoch 705/3000, 60/60 - D Loss: 4.601869022735627e-06, G Loss: 0.19012770056724548 - 17.73 s\n",
      "Epoch 706/3000, 60/60 - D Loss: 4.880979076915537e-06, G Loss: 0.1956925392150879 - 17.49 s\n",
      "Epoch 707/3000, 60/60 - D Loss: 4.073769900969637e-06, G Loss: 0.19046694040298462 - 17.85 s\n",
      "Epoch 708/3000, 60/60 - D Loss: 3.707998189383943e-06, G Loss: 0.19211827218532562 - 17.91 s\n",
      "Epoch 709/3000, 60/60 - D Loss: 4.395464202389121e-06, G Loss: 0.19109280407428741 - 17.72 s\n",
      "Epoch 710/3000, 60/60 - D Loss: 3.684811531456944e-06, G Loss: 0.19359347224235535 - 17.59 s\n",
      "Epoch 711/3000, 60/60 - D Loss: 3.7701173596360604e-06, G Loss: 0.19343852996826172 - 17.86 s\n",
      "Epoch 712/3000, 60/60 - D Loss: 3.33537843744125e-06, G Loss: 0.19309037923812866 - 17.69 s\n",
      "Epoch 713/3000, 60/60 - D Loss: 3.616247227000713e-06, G Loss: 0.19215932488441467 - 17.56 s\n",
      "Epoch 714/3000, 60/60 - D Loss: 3.0018273946552654e-06, G Loss: 0.1944921463727951 - 17.82 s\n",
      "Epoch 715/3000, 60/60 - D Loss: 2.9645054837601492e-06, G Loss: 0.19496510922908783 - 17.65 s\n",
      "Epoch 716/3000, 60/60 - D Loss: 2.7000595537174377e-06, G Loss: 0.1925189197063446 - 17.68 s\n",
      "Epoch 717/3000, 60/60 - D Loss: 2.9400482617347734e-06, G Loss: 0.19512447714805603 - 17.58 s\n",
      "Epoch 718/3000, 60/60 - D Loss: 2.243390099465614e-06, G Loss: 0.19382993876934052 - 17.73 s\n",
      "Epoch 719/3000, 60/60 - D Loss: 2.292698127348558e-06, G Loss: 0.19676844775676727 - 17.71 s\n",
      "Epoch 720/3000, 60/60 - D Loss: 2.0470417894102866e-06, G Loss: 0.19229862093925476 - 17.99 s\n",
      "Epoch 721/3000, 60/60 - D Loss: 1.9860071347466146e-06, G Loss: 0.2067624032497406 - 17.73 s\n",
      "Epoch 722/3000, 60/60 - D Loss: 2.0274488861105056e-06, G Loss: 0.1913660317659378 - 17.68 s\n",
      "Epoch 723/3000, 60/60 - D Loss: 1.8063893207909132e-06, G Loss: 0.19510479271411896 - 17.69 s\n",
      "Epoch 724/3000, 60/60 - D Loss: 1.7458694969718636e-06, G Loss: 0.1930036097764969 - 17.50 s\n",
      "Epoch 725/3000, 60/60 - D Loss: 1.7441270756535232e-06, G Loss: 0.1913890242576599 - 17.61 s\n",
      "Epoch 726/3000, 60/60 - D Loss: 1.6410260741395177e-06, G Loss: 0.1968792974948883 - 17.60 s\n",
      "Epoch 727/3000, 60/60 - D Loss: 1.676232898262242e-06, G Loss: 0.19212259352207184 - 17.62 s\n",
      "Epoch 728/3000, 60/60 - D Loss: 1.4516702151468053e-06, G Loss: 0.19439665973186493 - 17.88 s\n",
      "Epoch 729/3000, 60/60 - D Loss: 1.3708532833334175e-06, G Loss: 0.1920502930879593 - 17.89 s\n",
      "Epoch 730/3000, 60/60 - D Loss: 1.382126868065825e-06, G Loss: 0.19073106348514557 - 17.97 s\n",
      "Epoch 731/3000, 60/60 - D Loss: 1.3375325806919136e-06, G Loss: 0.19268803298473358 - 17.70 s\n",
      "Epoch 732/3000, 60/60 - D Loss: 1.3358728097045969e-06, G Loss: 0.19171643257141113 - 17.59 s\n",
      "Epoch 733/3000, 60/60 - D Loss: 1.197460903767933e-06, G Loss: 0.19275954365730286 - 17.85 s\n",
      "Epoch 734/3000, 60/60 - D Loss: 1.2298244200792396e-06, G Loss: 0.19242486357688904 - 17.53 s\n",
      "Epoch 735/3000, 60/60 - D Loss: 1.2353579847967922e-06, G Loss: 0.19182521104812622 - 17.95 s\n",
      "Epoch 736/3000, 60/60 - D Loss: 1.1091832590182094e-06, G Loss: 0.20148371160030365 - 18.01 s\n",
      "Epoch 737/3000, 60/60 - D Loss: 1.0929640836820909e-06, G Loss: 0.19377252459526062 - 17.60 s\n",
      "Epoch 738/3000, 60/60 - D Loss: 9.98266273199988e-07, G Loss: 0.19357426464557648 - 17.77 s\n",
      "Epoch 739/3000, 60/60 - D Loss: 9.734936270433536e-07, G Loss: 0.1941034495830536 - 17.64 s\n",
      "Epoch 740/3000, 60/60 - D Loss: 9.313720852333063e-07, G Loss: 0.19216029345989227 - 17.80 s\n",
      "Epoch 741/3000, 60/60 - D Loss: 8.723080497929914e-07, G Loss: 0.1936282366514206 - 17.78 s\n",
      "Epoch 742/3000, 60/60 - D Loss: 8.260892201406023e-07, G Loss: 0.19136780500411987 - 17.68 s\n",
      "Epoch 743/3000, 60/60 - D Loss: 7.84445845170012e-07, G Loss: 0.1913013458251953 - 17.62 s\n",
      "Epoch 744/3000, 60/60 - D Loss: 7.589535613305998e-07, G Loss: 0.1927862912416458 - 17.51 s\n",
      "Epoch 745/3000, 60/60 - D Loss: 7.084815791813526e-07, G Loss: 0.19278116524219513 - 17.69 s\n",
      "Epoch 746/3000, 60/60 - D Loss: 7.142766094148101e-07, G Loss: 0.1936425268650055 - 17.78 s\n",
      "Epoch 747/3000, 60/60 - D Loss: 7.03046865169199e-07, G Loss: 0.19342590868473053 - 17.55 s\n",
      "Epoch 748/3000, 60/60 - D Loss: 6.050319285577643e-07, G Loss: 0.19362333416938782 - 17.75 s\n",
      "Epoch 749/3000, 60/60 - D Loss: 6.121005924342171e-07, G Loss: 0.19427920877933502 - 17.98 s\n",
      "Epoch 750/3000, 60/60 - D Loss: 5.583065458836245e-07, G Loss: 0.19811449944972992 - 17.92 s\n",
      "Epoch 751/3000, 60/60 - D Loss: 5.284215376377688e-07, G Loss: 0.19290421903133392 - 18.08 s\n",
      "Epoch 752/3000, 60/60 - D Loss: 4.982506069950432e-07, G Loss: 0.19064660370349884 - 17.73 s\n",
      "Epoch 753/3000, 60/60 - D Loss: 4.470604082484897e-07, G Loss: 0.19293026626110077 - 17.63 s\n",
      "Epoch 754/3000, 60/60 - D Loss: 4.1231286473930595e-07, G Loss: 0.1899823248386383 - 17.72 s\n",
      "Epoch 755/3000, 60/60 - D Loss: 4.4429064161022325e-07, G Loss: 0.19595403969287872 - 17.68 s\n",
      "Epoch 756/3000, 60/60 - D Loss: 3.84725922231155e-07, G Loss: 0.19137103855609894 - 17.61 s\n",
      "Epoch 757/3000, 60/60 - D Loss: 3.459075479383955e-07, G Loss: 0.19580571353435516 - 17.80 s\n",
      "Epoch 758/3000, 60/60 - D Loss: 3.3891285156073536e-07, G Loss: 0.19241218268871307 - 17.59 s\n",
      "Epoch 759/3000, 60/60 - D Loss: 3.317972101513078e-07, G Loss: 0.19218112528324127 - 17.61 s\n",
      "Epoch 760/3000, 60/60 - D Loss: 2.883021394950447e-07, G Loss: 0.19276653230190277 - 17.66 s\n",
      "Epoch 761/3000, 60/60 - D Loss: 2.9176498372862625e-07, G Loss: 0.1915363222360611 - 17.61 s\n",
      "Epoch 762/3000, 60/60 - D Loss: 2.890767305530062e-07, G Loss: 0.1940745711326599 - 17.54 s\n",
      "Epoch 763/3000, 60/60 - D Loss: 2.499661952981569e-07, G Loss: 0.19140566885471344 - 17.47 s\n",
      "Epoch 764/3000, 60/60 - D Loss: 2.2785664199886924e-07, G Loss: 0.19301196932792664 - 17.59 s\n",
      "Epoch 765/3000, 60/60 - D Loss: 2.34777346008741e-07, G Loss: 0.19233010709285736 - 17.93 s\n",
      "Epoch 766/3000, 60/60 - D Loss: 2.0695841840279172e-07, G Loss: 0.1915869116783142 - 18.14 s\n",
      "Epoch 767/3000, 60/60 - D Loss: 2.0245174070510075e-07, G Loss: 0.19147655367851257 - 17.60 s\n",
      "Epoch 768/3000, 60/60 - D Loss: 1.8511330424075823e-07, G Loss: 0.19207221269607544 - 17.88 s\n",
      "Epoch 769/3000, 60/60 - D Loss: 1.7685469089201433e-07, G Loss: 0.19187474250793457 - 17.65 s\n",
      "Epoch 770/3000, 60/60 - D Loss: 1.68011045076355e-07, G Loss: 0.1908026784658432 - 17.61 s\n",
      "Epoch 771/3000, 60/60 - D Loss: 1.4039332718596143e-07, G Loss: 0.19043904542922974 - 17.57 s\n",
      "Epoch 772/3000, 60/60 - D Loss: 1.39933369780465e-07, G Loss: 0.19242507219314575 - 17.72 s\n",
      "Epoch 773/3000, 60/60 - D Loss: 1.2530578885616706e-07, G Loss: 0.19208551943302155 - 17.53 s\n",
      "Epoch 774/3000, 60/60 - D Loss: 1.2265322268945056e-07, G Loss: 0.19386282563209534 - 17.54 s\n",
      "Epoch 775/3000, 60/60 - D Loss: 1.1518072362548537e-07, G Loss: 0.19267842173576355 - 17.63 s\n",
      "Epoch 776/3000, 60/60 - D Loss: 1.1511264474961536e-07, G Loss: 0.19157567620277405 - 17.66 s\n",
      "Epoch 777/3000, 60/60 - D Loss: 1.138570500103242e-07, G Loss: 0.1911584883928299 - 17.66 s\n",
      "Epoch 778/3000, 60/60 - D Loss: 1.0853539222921427e-07, G Loss: 0.19338731467723846 - 18.04 s\n",
      "Epoch 779/3000, 60/60 - D Loss: 1.0150671858610849e-07, G Loss: 0.19115827977657318 - 17.76 s\n",
      "Epoch 780/3000, 60/60 - D Loss: 9.719744742398007e-08, G Loss: 0.19098904728889465 - 17.66 s\n",
      "Epoch 781/3000, 60/60 - D Loss: 9.26635435050116e-08, G Loss: 0.19017423689365387 - 17.91 s\n",
      "Epoch 782/3000, 60/60 - D Loss: 9.705170178619937e-08, G Loss: 0.19188706576824188 - 17.71 s\n",
      "Epoch 783/3000, 60/60 - D Loss: 8.005961493751101e-08, G Loss: 0.19122888147830963 - 17.45 s\n",
      "Epoch 784/3000, 60/60 - D Loss: 7.922964861961646e-08, G Loss: 0.19451890885829926 - 17.90 s\n",
      "Epoch 785/3000, 60/60 - D Loss: 7.461092010885295e-08, G Loss: 0.1926899403333664 - 17.81 s\n",
      "Epoch 786/3000, 60/60 - D Loss: 6.443255484711585e-08, G Loss: 0.1909809708595276 - 17.71 s\n",
      "Epoch 787/3000, 60/60 - D Loss: 6.555930065488269e-08, G Loss: 0.19123820960521698 - 17.67 s\n",
      "Epoch 788/3000, 60/60 - D Loss: 6.081978520811049e-08, G Loss: 0.19150987267494202 - 17.67 s\n",
      "Epoch 789/3000, 60/60 - D Loss: 5.878841768947041e-08, G Loss: 0.19099970161914825 - 17.57 s\n",
      "Epoch 790/3000, 60/60 - D Loss: 5.663772562058966e-08, G Loss: 0.18992586433887482 - 17.70 s\n",
      "Epoch 791/3000, 60/60 - D Loss: 4.832861888459661e-08, G Loss: 0.19058217108249664 - 17.63 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 792/3000, 60/60 - D Loss: 4.83070499157634e-08, G Loss: 0.1923869401216507 - 17.50 s\n",
      "Epoch 793/3000, 60/60 - D Loss: 4.706809164645165e-08, G Loss: 0.19265435636043549 - 17.76 s\n",
      "Epoch 794/3000, 60/60 - D Loss: 3.963565542441927e-08, G Loss: 0.19093632698059082 - 17.67 s\n",
      "Epoch 795/3000, 60/60 - D Loss: 3.7973356903364675e-08, G Loss: 0.193069726228714 - 17.74 s\n",
      "Epoch 796/3000, 60/60 - D Loss: 3.775547940954027e-08, G Loss: 0.19469361007213593 - 17.88 s\n",
      "Epoch 797/3000, 60/60 - D Loss: 3.592867958701618e-08, G Loss: 0.1931048035621643 - 17.41 s\n",
      "Epoch 798/3000, 60/60 - D Loss: 3.722976749109819e-08, G Loss: 0.19213418662548065 - 17.60 s\n",
      "Epoch 799/3000, 60/60 - D Loss: 3.1997152483143054e-08, G Loss: 0.19539614021778107 - 17.53 s\n",
      "Epoch 800/3000, 60/60 - D Loss: 3.19716724206387e-08, G Loss: 0.1925169825553894 - 17.60 s\n",
      "Epoch 801/3000, 60/60 - D Loss: 3.0646876592044237e-08, G Loss: 0.19251398742198944 - 17.56 s\n",
      "Epoch 802/3000, 60/60 - D Loss: 2.850003189003303e-08, G Loss: 0.1942598968744278 - 17.69 s\n",
      "Epoch 803/3000, 60/60 - D Loss: 2.4749533977797e-08, G Loss: 0.19162076711654663 - 17.57 s\n",
      "Epoch 804/3000, 60/60 - D Loss: 2.4553040045560692e-08, G Loss: 0.19201916456222534 - 17.51 s\n",
      "Epoch 805/3000, 60/60 - D Loss: 2.301531976112159e-08, G Loss: 0.19337430596351624 - 17.77 s\n",
      "Epoch 806/3000, 60/60 - D Loss: 2.3433467388755957e-08, G Loss: 0.19465790688991547 - 17.92 s\n",
      "Epoch 807/3000, 60/60 - D Loss: 2.0606871542483418e-08, G Loss: 0.19533906877040863 - 17.58 s\n",
      "Epoch 808/3000, 60/60 - D Loss: 1.8611959973213743e-08, G Loss: 0.1904851794242859 - 17.86 s\n",
      "Epoch 809/3000, 60/60 - D Loss: 1.6915586931176563e-08, G Loss: 0.19161109626293182 - 17.60 s\n",
      "Epoch 810/3000, 60/60 - D Loss: 1.7302352772752272e-08, G Loss: 0.1919119507074356 - 17.65 s\n",
      "Epoch 811/3000, 60/60 - D Loss: 1.6391693224981907e-08, G Loss: 0.1901347041130066 - 17.74 s\n",
      "Epoch 812/3000, 60/60 - D Loss: 1.713869263175738e-08, G Loss: 0.19241681694984436 - 18.05 s\n",
      "Epoch 813/3000, 60/60 - D Loss: 1.5415922982597863e-08, G Loss: 0.19245432317256927 - 17.81 s\n",
      "Epoch 814/3000, 60/60 - D Loss: 1.494878010266376e-08, G Loss: 0.19258002936840057 - 17.73 s\n",
      "Epoch 815/3000, 60/60 - D Loss: 1.3723699343604068e-08, G Loss: 0.19293993711471558 - 17.83 s\n",
      "Epoch 816/3000, 60/60 - D Loss: 1.3531586240400628e-08, G Loss: 0.19261698424816132 - 17.71 s\n",
      "Epoch 817/3000, 60/60 - D Loss: 1.2064349852458633e-08, G Loss: 0.19169212877750397 - 17.74 s\n",
      "Epoch 818/3000, 60/60 - D Loss: 1.2229719292822239e-08, G Loss: 0.19213205575942993 - 17.57 s\n",
      "Epoch 819/3000, 60/60 - D Loss: 1.1682508932464586e-08, G Loss: 0.19000592827796936 - 17.83 s\n",
      "Epoch 820/3000, 60/60 - D Loss: 1.0264794236825026e-08, G Loss: 0.1916854977607727 - 17.85 s\n",
      "Epoch 821/3000, 60/60 - D Loss: 1.031117374816759e-08, G Loss: 0.19076688587665558 - 17.68 s\n",
      "Epoch 822/3000, 60/60 - D Loss: 8.693932102010749e-09, G Loss: 0.19332082569599152 - 17.75 s\n",
      "Epoch 823/3000, 60/60 - D Loss: 8.878800028533362e-09, G Loss: 0.193474680185318 - 17.90 s\n",
      "Epoch 824/3000, 60/60 - D Loss: 8.07222316767664e-09, G Loss: 0.19102324545383453 - 17.82 s\n",
      "Epoch 825/3000, 60/60 - D Loss: 7.686994540900827e-09, G Loss: 0.19109992682933807 - 17.82 s\n",
      "Epoch 826/3000, 60/60 - D Loss: 7.2880118606732935e-09, G Loss: 0.18994084000587463 - 17.86 s\n",
      "Epoch 827/3000, 60/60 - D Loss: 7.105576654842238e-09, G Loss: 0.1920149028301239 - 17.81 s\n",
      "Epoch 828/3000, 60/60 - D Loss: 7.1540111612478086e-09, G Loss: 0.18992501497268677 - 17.74 s\n",
      "Epoch 829/3000, 60/60 - D Loss: 6.885195608097305e-09, G Loss: 0.1924087256193161 - 17.76 s\n",
      "Epoch 830/3000, 60/60 - D Loss: 7.011783237365066e-09, G Loss: 0.1907072514295578 - 17.80 s\n",
      "Epoch 831/3000, 60/60 - D Loss: 6.355621945397516e-09, G Loss: 0.19176751375198364 - 17.72 s\n",
      "Epoch 832/3000, 60/60 - D Loss: 5.855824034006751e-09, G Loss: 0.19220057129859924 - 17.73 s\n",
      "Epoch 833/3000, 60/60 - D Loss: 6.638475213982531e-09, G Loss: 0.193802148103714 - 17.78 s\n",
      "Epoch 834/3000, 60/60 - D Loss: 5.8895274906989314e-09, G Loss: 0.19245637953281403 - 17.80 s\n",
      "Epoch 835/3000, 60/60 - D Loss: 5.427803762003691e-09, G Loss: 0.1932220757007599 - 17.72 s\n",
      "Epoch 836/3000, 60/60 - D Loss: 5.336023914392918e-09, G Loss: 0.19022779166698456 - 17.88 s\n",
      "Epoch 837/3000, 60/60 - D Loss: 4.954026674997003e-09, G Loss: 0.19089849293231964 - 17.67 s\n",
      "Epoch 838/3000, 60/60 - D Loss: 4.756188429211328e-09, G Loss: 0.19199532270431519 - 17.80 s\n",
      "Epoch 839/3000, 60/60 - D Loss: 4.692914862269859e-09, G Loss: 0.19138182699680328 - 17.70 s\n",
      "Epoch 840/3000, 60/60 - D Loss: 4.769862504971911e-09, G Loss: 0.19005028903484344 - 17.94 s\n",
      "Epoch 841/3000, 60/60 - D Loss: 4.748522922093379e-09, G Loss: 0.1898230016231537 - 17.66 s\n",
      "Epoch 842/3000, 60/60 - D Loss: 4.470049691507327e-09, G Loss: 0.19278620183467865 - 17.96 s\n",
      "Epoch 843/3000, 60/60 - D Loss: 4.102671147254711e-09, G Loss: 0.19284012913703918 - 17.63 s\n",
      "Epoch 844/3000, 60/60 - D Loss: 4.421934665654881e-09, G Loss: 0.19114266335964203 - 17.73 s\n",
      "Epoch 845/3000, 60/60 - D Loss: 3.844354362625424e-09, G Loss: 0.19228404760360718 - 17.66 s\n",
      "Epoch 846/3000, 60/60 - D Loss: 3.900431852499331e-09, G Loss: 0.19008812308311462 - 17.48 s\n",
      "Epoch 847/3000, 60/60 - D Loss: 3.577409903032791e-09, G Loss: 0.19159579277038574 - 17.63 s\n",
      "Epoch 848/3000, 60/60 - D Loss: 3.4840506035749286e-09, G Loss: 0.19282269477844238 - 17.47 s\n",
      "Epoch 849/3000, 60/60 - D Loss: 3.5392046723869974e-09, G Loss: 0.19684165716171265 - 17.91 s\n",
      "Epoch 850/3000, 60/60 - D Loss: 3.656902752835478e-09, G Loss: 0.19179296493530273 - 17.72 s\n",
      "Epoch 851/3000, 60/60 - D Loss: 3.6206626446988466e-09, G Loss: 0.1913662999868393 - 17.67 s\n",
      "Epoch 852/3000, 60/60 - D Loss: 3.5391134745044184e-09, G Loss: 0.19474679231643677 - 17.71 s\n",
      "Epoch 853/3000, 60/60 - D Loss: 3.3640545453461623e-09, G Loss: 0.19156210124492645 - 17.86 s\n",
      "Epoch 854/3000, 60/60 - D Loss: 3.1733101389652685e-09, G Loss: 0.19017331302165985 - 17.80 s\n",
      "Epoch 855/3000, 60/60 - D Loss: 3.4280635136108017e-09, G Loss: 0.19252638518810272 - 17.46 s\n",
      "Epoch 856/3000, 60/60 - D Loss: 3.438532221108903e-09, G Loss: 0.1918449103832245 - 17.56 s\n",
      "Epoch 857/3000, 60/60 - D Loss: 3.1243953986515027e-09, G Loss: 0.19025883078575134 - 17.84 s\n",
      "Epoch 858/3000, 60/60 - D Loss: 3.11397203509034e-09, G Loss: 0.19180764257907867 - 17.78 s\n",
      "Epoch 859/3000, 60/60 - D Loss: 3.4867286696771416e-09, G Loss: 0.1924009472131729 - 17.74 s\n",
      "Epoch 860/3000, 60/60 - D Loss: 3.217942235594906e-09, G Loss: 0.19116511940956116 - 17.59 s\n",
      "Epoch 861/3000, 60/60 - D Loss: 3.1721650792237988e-09, G Loss: 0.19068530201911926 - 17.68 s\n",
      "Epoch 862/3000, 60/60 - D Loss: 3.0846577028786903e-09, G Loss: 0.19050027430057526 - 17.78 s\n",
      "Epoch 863/3000, 60/60 - D Loss: 3.1502021061169394e-09, G Loss: 0.1942977011203766 - 17.69 s\n",
      "Epoch 864/3000, 60/60 - D Loss: 3.166147082359072e-09, G Loss: 0.19090476632118225 - 17.76 s\n",
      "Epoch 865/3000, 60/60 - D Loss: 2.8424509170010825e-09, G Loss: 0.19489137828350067 - 17.73 s\n",
      "Epoch 866/3000, 60/60 - D Loss: 3.070715286213055e-09, G Loss: 0.19263304769992828 - 17.68 s\n",
      "Epoch 867/3000, 60/60 - D Loss: 2.797219083097713e-09, G Loss: 0.19244953989982605 - 17.71 s\n",
      "Epoch 868/3000, 60/60 - D Loss: 2.9256964273124964e-09, G Loss: 0.19215363264083862 - 17.79 s\n",
      "Epoch 869/3000, 60/60 - D Loss: 2.9930265262739786e-09, G Loss: 0.19140462577342987 - 17.80 s\n",
      "Epoch 870/3000, 60/60 - D Loss: 2.9930898037822118e-09, G Loss: 0.19215530157089233 - 17.68 s\n",
      "Epoch 871/3000, 60/60 - D Loss: 2.9206542028620275e-09, G Loss: 0.19157631695270538 - 17.84 s\n",
      "Epoch 872/3000, 60/60 - D Loss: 2.5036675640674466e-09, G Loss: 0.19057337939739227 - 17.96 s\n",
      "Epoch 873/3000, 60/60 - D Loss: 2.6622523317809677e-09, G Loss: 0.19097179174423218 - 17.61 s\n",
      "Epoch 874/3000, 60/60 - D Loss: 2.8954382658846756e-09, G Loss: 0.19251368939876556 - 17.48 s\n",
      "Epoch 875/3000, 60/60 - D Loss: 2.800808993584647e-09, G Loss: 0.19426769018173218 - 17.75 s\n",
      "Epoch 876/3000, 60/60 - D Loss: 2.6016750863042115e-09, G Loss: 0.1908644735813141 - 17.68 s\n",
      "Epoch 877/3000, 60/60 - D Loss: 2.5349123442680588e-09, G Loss: 0.19191351532936096 - 17.75 s\n",
      "Epoch 878/3000, 60/60 - D Loss: 2.399491693880207e-09, G Loss: 0.1928502470254898 - 17.88 s\n",
      "Epoch 879/3000, 60/60 - D Loss: 2.584903252382531e-09, G Loss: 0.1917976289987564 - 17.71 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 880/3000, 60/60 - D Loss: 2.5800899741343364e-09, G Loss: 0.1922644078731537 - 17.64 s\n",
      "Epoch 881/3000, 60/60 - D Loss: 2.5557394204428485e-09, G Loss: 0.1943197250366211 - 17.81 s\n",
      "Epoch 882/3000, 60/60 - D Loss: 2.48390512064961e-09, G Loss: 0.19408917427062988 - 17.98 s\n",
      "Epoch 883/3000, 60/60 - D Loss: 2.4371644997273068e-09, G Loss: 0.19436605274677277 - 17.50 s\n",
      "Epoch 884/3000, 60/60 - D Loss: 2.5722148733892114e-09, G Loss: 0.1912345141172409 - 17.56 s\n",
      "Epoch 885/3000, 60/60 - D Loss: 2.3368897615616557e-09, G Loss: 0.19122663140296936 - 17.57 s\n",
      "Epoch 886/3000, 60/60 - D Loss: 2.436121652495127e-09, G Loss: 0.19099844992160797 - 17.75 s\n",
      "Epoch 887/3000, 60/60 - D Loss: 2.2540936318003746e-09, G Loss: 0.19083355367183685 - 18.06 s\n",
      "Epoch 888/3000, 60/60 - D Loss: 2.276125643432131e-09, G Loss: 0.19314314424991608 - 17.66 s\n",
      "Epoch 889/3000, 60/60 - D Loss: 2.2141655184673703e-09, G Loss: 0.19042283296585083 - 17.73 s\n",
      "Epoch 890/3000, 60/60 - D Loss: 2.274434799786479e-09, G Loss: 0.19063465297222137 - 17.91 s\n",
      "Epoch 891/3000, 60/60 - D Loss: 2.1126635137494465e-09, G Loss: 0.1913912147283554 - 17.69 s\n",
      "Epoch 892/3000, 60/60 - D Loss: 2.4708701187522997e-09, G Loss: 0.19008593261241913 - 17.58 s\n",
      "Epoch 893/3000, 60/60 - D Loss: 2.254096291998825e-09, G Loss: 0.2003776580095291 - 17.53 s\n",
      "Epoch 894/3000, 60/60 - D Loss: 2.368188265714638e-09, G Loss: 0.19219884276390076 - 17.56 s\n",
      "Epoch 895/3000, 60/60 - D Loss: 2.34874883865005e-09, G Loss: 0.1901097148656845 - 17.51 s\n",
      "Epoch 896/3000, 60/60 - D Loss: 2.347628964491366e-09, G Loss: 0.19036294519901276 - 17.57 s\n",
      "Epoch 897/3000, 60/60 - D Loss: 2.314972108539287e-09, G Loss: 0.19250154495239258 - 17.85 s\n",
      "Epoch 898/3000, 60/60 - D Loss: 2.3781418058768655e-09, G Loss: 0.19027771055698395 - 17.78 s\n",
      "Epoch 899/3000, 60/60 - D Loss: 2.0186553357712245e-09, G Loss: 0.19664891064167023 - 17.66 s\n",
      "Epoch 900/3000, 60/60 - D Loss: 2.384039334436122e-09, G Loss: 0.19249407947063446 - 17.67 s\n",
      "Epoch 901/3000, 60/60 - D Loss: 2.37966210668919e-09, G Loss: 0.18994387984275818 - 18.37 s\n",
      "Epoch 902/3000, 60/60 - D Loss: 2.274187734831179e-09, G Loss: 0.19067205488681793 - 17.56 s\n",
      "Epoch 903/3000, 60/60 - D Loss: 2.3723401368358488e-09, G Loss: 0.1916893571615219 - 17.91 s\n",
      "Epoch 904/3000, 60/60 - D Loss: 1.805139409911255e-09, G Loss: 0.19189493358135223 - 17.93 s\n",
      "Epoch 905/3000, 60/60 - D Loss: 2.4578031020995594e-09, G Loss: 0.19080282747745514 - 17.58 s\n",
      "Epoch 906/3000, 60/60 - D Loss: 2.4164954958924567e-09, G Loss: 0.189522847533226 - 17.64 s\n",
      "Epoch 907/3000, 60/60 - D Loss: 1.960118910498271e-09, G Loss: 0.19088824093341827 - 17.77 s\n",
      "Epoch 908/3000, 60/60 - D Loss: 2.422170971606852e-09, G Loss: 0.190952330827713 - 17.73 s\n",
      "Epoch 909/3000, 60/60 - D Loss: 2.1741323639416082e-09, G Loss: 0.19060878455638885 - 17.55 s\n",
      "Epoch 910/3000, 60/60 - D Loss: 2.152068741744992e-09, G Loss: 0.19144806265830994 - 17.78 s\n",
      "Epoch 911/3000, 60/60 - D Loss: 2.0937311168665873e-09, G Loss: 0.19165441393852234 - 17.57 s\n",
      "Epoch 912/3000, 60/60 - D Loss: 2.3036100878399968e-09, G Loss: 0.18998296558856964 - 17.60 s\n",
      "Epoch 913/3000, 60/60 - D Loss: 1.776791952283474e-09, G Loss: 0.1905544102191925 - 17.60 s\n",
      "Epoch 914/3000, 60/60 - D Loss: 3.0750260161544613e-09, G Loss: 0.1904861181974411 - 17.82 s\n",
      "Epoch 915/3000, 60/60 - D Loss: 2.044372097390751e-09, G Loss: 0.1908787339925766 - 17.76 s\n",
      "Epoch 916/3000, 60/60 - D Loss: 1.6144512094383445e-09, G Loss: 0.19094230234622955 - 17.69 s\n",
      "Epoch 917/3000, 60/60 - D Loss: 1.7951106287203422e-09, G Loss: 0.19282551109790802 - 17.94 s\n",
      "Epoch 918/3000, 60/60 - D Loss: 1.8024035428228224e-09, G Loss: 0.19067518413066864 - 18.03 s\n",
      "Epoch 919/3000, 60/60 - D Loss: 1.7576687813547487e-09, G Loss: 0.190325528383255 - 17.76 s\n",
      "Epoch 920/3000, 60/60 - D Loss: 1.896683821936071e-09, G Loss: 0.19087937474250793 - 17.92 s\n",
      "Epoch 921/3000, 60/60 - D Loss: 1.7226516801568947e-09, G Loss: 0.19129395484924316 - 17.72 s\n",
      "Epoch 922/3000, 60/60 - D Loss: 0.0837226550502237, G Loss: 0.19071032106876373 - 17.78 s\n",
      "Epoch 923/3000, 60/60 - D Loss: 0.00044931747106602415, G Loss: 0.19094248116016388 - 17.75 s\n",
      "Epoch 924/3000, 60/60 - D Loss: 0.00035049946745857596, G Loss: 0.19644035398960114 - 17.66 s\n",
      "Epoch 925/3000, 60/60 - D Loss: 0.0002014638957916759, G Loss: 0.1908256709575653 - 17.95 s\n",
      "Epoch 926/3000, 60/60 - D Loss: 0.00015249198622768745, G Loss: 0.19201810657978058 - 17.77 s\n",
      "Epoch 927/3000, 60/60 - D Loss: 0.00011424056356190704, G Loss: 0.1901397556066513 - 17.74 s\n",
      "Epoch 928/3000, 60/60 - D Loss: 8.876818901626393e-05, G Loss: 0.19293442368507385 - 17.80 s\n",
      "Epoch 929/3000, 60/60 - D Loss: 8.101303137664218e-05, G Loss: 0.19184929132461548 - 17.71 s\n",
      "Epoch 930/3000, 60/60 - D Loss: 6.552637933054939e-05, G Loss: 0.18986889719963074 - 17.62 s\n",
      "Epoch 931/3000, 60/60 - D Loss: 5.044070530857425e-05, G Loss: 0.19256220757961273 - 17.56 s\n",
      "Epoch 932/3000, 60/60 - D Loss: 4.777227877639234e-05, G Loss: 0.19230344891548157 - 17.88 s\n",
      "Epoch 933/3000, 60/60 - D Loss: 4.065362736582756e-05, G Loss: 0.19170087575912476 - 18.06 s\n",
      "Epoch 934/3000, 60/60 - D Loss: 3.1753216717334e-05, G Loss: 0.19052892923355103 - 17.88 s\n",
      "Epoch 935/3000, 60/60 - D Loss: 2.9493668080249336e-05, G Loss: 0.1903984099626541 - 17.89 s\n",
      "Epoch 936/3000, 60/60 - D Loss: 2.4549166482756846e-05, G Loss: 0.1917087882757187 - 17.71 s\n",
      "Epoch 937/3000, 60/60 - D Loss: 2.2461392291006632e-05, G Loss: 0.19182810187339783 - 17.83 s\n",
      "Epoch 938/3000, 60/60 - D Loss: 1.963720569619909e-05, G Loss: 0.19304105639457703 - 17.81 s\n",
      "Epoch 939/3000, 60/60 - D Loss: 1.7264629605051596e-05, G Loss: 0.1896526962518692 - 17.72 s\n",
      "Epoch 940/3000, 60/60 - D Loss: 1.4478273442364298e-05, G Loss: 0.19148539006710052 - 17.62 s\n",
      "Epoch 941/3000, 60/60 - D Loss: 1.3405943263933295e-05, G Loss: 0.19041278958320618 - 17.78 s\n",
      "Epoch 942/3000, 60/60 - D Loss: 1.2276001143618487e-05, G Loss: 0.191569522023201 - 17.65 s\n",
      "Epoch 943/3000, 60/60 - D Loss: 1.1061769782827469e-05, G Loss: 0.19158942997455597 - 17.69 s\n",
      "Epoch 944/3000, 60/60 - D Loss: 1.0175797569900169e-05, G Loss: 0.1912221759557724 - 17.65 s\n",
      "Epoch 945/3000, 60/60 - D Loss: 1.0921799002971966e-05, G Loss: 0.19115029275417328 - 17.77 s\n",
      "Epoch 946/3000, 60/60 - D Loss: 8.663230119054788e-06, G Loss: 0.19375406205654144 - 17.87 s\n",
      "Epoch 947/3000, 60/60 - D Loss: 8.259024980361573e-06, G Loss: 0.19169950485229492 - 17.56 s\n",
      "Epoch 948/3000, 60/60 - D Loss: 7.872382639106945e-06, G Loss: 0.19061067700386047 - 17.80 s\n",
      "Epoch 949/3000, 60/60 - D Loss: 6.702423320348316e-06, G Loss: 0.19224680960178375 - 17.88 s\n",
      "Epoch 950/3000, 60/60 - D Loss: 6.98460291914671e-06, G Loss: 0.19245819747447968 - 17.67 s\n",
      "Epoch 951/3000, 60/60 - D Loss: 6.296926812865422e-06, G Loss: 0.1891600638628006 - 17.75 s\n",
      "Epoch 952/3000, 60/60 - D Loss: 5.851862738381897e-06, G Loss: 0.1892978996038437 - 17.84 s\n",
      "Epoch 953/3000, 60/60 - D Loss: 5.7153919215124915e-06, G Loss: 0.19136196374893188 - 17.72 s\n",
      "Epoch 954/3000, 60/60 - D Loss: 5.05411105677922e-06, G Loss: 0.18973997235298157 - 17.77 s\n",
      "Epoch 955/3000, 60/60 - D Loss: 4.664651328312175e-06, G Loss: 0.18952727317810059 - 17.66 s\n",
      "Epoch 956/3000, 60/60 - D Loss: 4.473788408176915e-06, G Loss: 0.19091914594173431 - 17.79 s\n",
      "Epoch 957/3000, 60/60 - D Loss: 4.246179514666437e-06, G Loss: 0.19095411896705627 - 17.69 s\n",
      "Epoch 958/3000, 60/60 - D Loss: 4.275872015568893e-06, G Loss: 0.1906743049621582 - 17.54 s\n",
      "Epoch 959/3000, 60/60 - D Loss: 4.1316586134598765e-06, G Loss: 0.19151446223258972 - 17.72 s\n",
      "Epoch 960/3000, 60/60 - D Loss: 4.0209577036876e-06, G Loss: 0.1923421025276184 - 17.77 s\n",
      "Epoch 961/3000, 60/60 - D Loss: 3.614224254988585e-06, G Loss: 0.19075506925582886 - 17.82 s\n",
      "Epoch 962/3000, 60/60 - D Loss: 3.302595331433622e-06, G Loss: 0.1913975179195404 - 17.94 s\n",
      "Epoch 963/3000, 60/60 - D Loss: 3.118267784429918e-06, G Loss: 0.19340525567531586 - 17.70 s\n",
      "Epoch 964/3000, 60/60 - D Loss: 3.1146134631399036e-06, G Loss: 0.1906689703464508 - 17.85 s\n",
      "Epoch 965/3000, 60/60 - D Loss: 3.1892716592096804e-06, G Loss: 0.19141454994678497 - 17.61 s\n",
      "Epoch 966/3000, 60/60 - D Loss: 2.727625862064542e-06, G Loss: 0.18987657129764557 - 17.40 s\n",
      "Epoch 967/3000, 60/60 - D Loss: 2.740103973053465e-06, G Loss: 0.19034555554389954 - 17.64 s\n",
      "Epoch 968/3000, 60/60 - D Loss: 2.4379946168551214e-06, G Loss: 0.1915634572505951 - 17.49 s\n",
      "Epoch 969/3000, 60/60 - D Loss: 2.1567329895333387e-06, G Loss: 0.1913529932498932 - 17.51 s\n",
      "Epoch 970/3000, 60/60 - D Loss: 2.072893128257647e-06, G Loss: 0.18975208699703217 - 17.82 s\n",
      "Epoch 971/3000, 60/60 - D Loss: 1.9230772068112856e-06, G Loss: 0.18977108597755432 - 17.56 s\n",
      "Epoch 972/3000, 60/60 - D Loss: 1.7868894097716748e-06, G Loss: 0.19118157029151917 - 17.76 s\n",
      "Epoch 973/3000, 60/60 - D Loss: 1.593021490720048e-06, G Loss: 0.19064730405807495 - 17.68 s\n",
      "Epoch 974/3000, 60/60 - D Loss: 1.4578307379053967e-06, G Loss: 0.1908237636089325 - 17.43 s\n",
      "Epoch 975/3000, 60/60 - D Loss: 1.3909029235037451e-06, G Loss: 0.18935266137123108 - 17.64 s\n",
      "Epoch 976/3000, 60/60 - D Loss: 1.3241988536805138e-06, G Loss: 0.19003313779830933 - 17.66 s\n",
      "Epoch 977/3000, 60/60 - D Loss: 1.3382719536991772e-06, G Loss: 0.19165045022964478 - 17.68 s\n",
      "Epoch 978/3000, 60/60 - D Loss: 1.3452534268765248e-06, G Loss: 0.19132700562477112 - 17.57 s\n",
      "Epoch 979/3000, 60/60 - D Loss: 1.292461190516292e-06, G Loss: 0.1915266364812851 - 17.63 s\n",
      "Epoch 980/3000, 60/60 - D Loss: 1.1384718261453486e-06, G Loss: 0.20029260218143463 - 17.83 s\n",
      "Epoch 981/3000, 60/60 - D Loss: 1.082727334456024e-06, G Loss: 0.1899288296699524 - 17.74 s\n",
      "Epoch 982/3000, 60/60 - D Loss: 1.0837417381281966e-06, G Loss: 0.1915939301252365 - 17.73 s\n",
      "Epoch 983/3000, 60/60 - D Loss: 1.154911122291935e-06, G Loss: 0.19018922746181488 - 17.68 s\n",
      "Epoch 984/3000, 60/60 - D Loss: 1.0822198910354075e-06, G Loss: 0.19111567735671997 - 17.60 s\n",
      "Epoch 985/3000, 60/60 - D Loss: 9.449061595390162e-07, G Loss: 0.1928098499774933 - 17.46 s\n",
      "Epoch 986/3000, 60/60 - D Loss: 9.979146371463798e-07, G Loss: 0.19238875806331635 - 17.45 s\n",
      "Epoch 987/3000, 60/60 - D Loss: 8.064105823102352e-07, G Loss: 0.190242737531662 - 17.38 s\n",
      "Epoch 988/3000, 60/60 - D Loss: 6.833531420724626e-07, G Loss: 0.19205032289028168 - 17.45 s\n",
      "Epoch 989/3000, 60/60 - D Loss: 6.368588705285561e-07, G Loss: 0.18926620483398438 - 17.72 s\n",
      "Epoch 990/3000, 60/60 - D Loss: 6.231185849525644e-07, G Loss: 0.19038085639476776 - 17.88 s\n",
      "Epoch 991/3000, 60/60 - D Loss: 5.964764624444285e-07, G Loss: 0.19059844315052032 - 17.63 s\n",
      "Epoch 992/3000, 60/60 - D Loss: 6.309765254730593e-07, G Loss: 0.18835397064685822 - 17.86 s\n",
      "Epoch 993/3000, 60/60 - D Loss: 5.985603765967795e-07, G Loss: 0.19040407240390778 - 17.72 s\n",
      "Epoch 994/3000, 60/60 - D Loss: 5.750221998557237e-07, G Loss: 0.1893303245306015 - 18.09 s\n",
      "Epoch 995/3000, 60/60 - D Loss: 5.044189679281885e-07, G Loss: 0.19279339909553528 - 17.47 s\n",
      "Epoch 996/3000, 60/60 - D Loss: 4.5872912579625336e-07, G Loss: 0.19091571867465973 - 17.60 s\n",
      "Epoch 997/3000, 60/60 - D Loss: 4.132673787626828e-07, G Loss: 0.1912733018398285 - 17.62 s\n",
      "Epoch 998/3000, 60/60 - D Loss: 3.967841344376666e-07, G Loss: 0.1916799694299698 - 17.54 s\n",
      "Epoch 999/3000, 60/60 - D Loss: 3.5079999305409615e-07, G Loss: 0.1899479180574417 - 17.89 s\n",
      "Epoch 1000/3000, 60/60 - D Loss: 3.452027121930712e-07, G Loss: 0.19140976667404175 - 17.65 s\n",
      "Epoch 1001/3000, 60/60 - D Loss: 3.84923204865828e-07, G Loss: 0.19282764196395874 - 17.39 s\n",
      "Epoch 1002/3000, 60/60 - D Loss: 4.371882683651407e-07, G Loss: 0.19109229743480682 - 17.56 s\n",
      "Epoch 1003/3000, 60/60 - D Loss: 3.5360373862314276e-07, G Loss: 0.19162920117378235 - 17.53 s\n",
      "Epoch 1004/3000, 60/60 - D Loss: 2.9321841243934443e-07, G Loss: 0.19243678450584412 - 17.58 s\n",
      "Epoch 1005/3000, 60/60 - D Loss: 2.794866971189691e-07, G Loss: 0.1910495012998581 - 17.66 s\n",
      "Epoch 1006/3000, 60/60 - D Loss: 2.629276534271696e-07, G Loss: 0.19260072708129883 - 17.68 s\n",
      "Epoch 1007/3000, 60/60 - D Loss: 2.5237642398145965e-07, G Loss: 0.18963202834129333 - 17.73 s\n",
      "Epoch 1008/3000, 60/60 - D Loss: 2.3640972068861288e-07, G Loss: 0.19031691551208496 - 17.84 s\n",
      "Epoch 1009/3000, 60/60 - D Loss: 2.284475643099526e-07, G Loss: 0.19040128588676453 - 17.71 s\n",
      "Epoch 1010/3000, 60/60 - D Loss: 2.318755231156544e-07, G Loss: 0.19075468182563782 - 17.63 s\n",
      "Epoch 1011/3000, 60/60 - D Loss: 2.0726405947080195e-07, G Loss: 0.18895572423934937 - 17.55 s\n",
      "Epoch 1012/3000, 60/60 - D Loss: 2.21088254703794e-07, G Loss: 0.1903141289949417 - 17.54 s\n",
      "Epoch 1013/3000, 60/60 - D Loss: 1.92900581397204e-07, G Loss: 0.19023706018924713 - 17.66 s\n",
      "Epoch 1014/3000, 60/60 - D Loss: 1.775367988732235e-07, G Loss: 0.18991635739803314 - 17.70 s\n",
      "Epoch 1015/3000, 60/60 - D Loss: 1.613314984938441e-07, G Loss: 0.190373033285141 - 17.53 s\n",
      "Epoch 1016/3000, 60/60 - D Loss: 1.6360265614467906e-07, G Loss: 0.1900862604379654 - 17.57 s\n",
      "Epoch 1017/3000, 60/60 - D Loss: 1.638967530581681e-07, G Loss: 0.1916135847568512 - 17.67 s\n",
      "Epoch 1018/3000, 60/60 - D Loss: 1.3390557818127036e-07, G Loss: 0.19178573787212372 - 17.50 s\n",
      "Epoch 1019/3000, 60/60 - D Loss: 1.1851847447896802e-07, G Loss: 0.19211693108081818 - 17.70 s\n",
      "Epoch 1020/3000, 60/60 - D Loss: 1.3242979818306466e-07, G Loss: 0.19004221260547638 - 17.43 s\n",
      "Epoch 1021/3000, 60/60 - D Loss: 1.3266315157522968e-07, G Loss: 0.1908133625984192 - 17.33 s\n",
      "Epoch 1022/3000, 60/60 - D Loss: 1.2148499650699307e-07, G Loss: 0.18991945683956146 - 17.94 s\n",
      "Epoch 1023/3000, 60/60 - D Loss: 9.957299967888744e-08, G Loss: 0.19042706489562988 - 17.85 s\n",
      "Epoch 1024/3000, 60/60 - D Loss: 8.675719928574743e-08, G Loss: 0.19179241359233856 - 18.04 s\n",
      "Epoch 1025/3000, 60/60 - D Loss: 7.617163080841927e-08, G Loss: 0.19108626246452332 - 17.57 s\n",
      "Epoch 1026/3000, 60/60 - D Loss: 6.631440635507246e-08, G Loss: 0.1905135065317154 - 17.63 s\n",
      "Epoch 1027/3000, 60/60 - D Loss: 6.185919296242126e-08, G Loss: 0.19100446999073029 - 17.52 s\n",
      "Epoch 1028/3000, 60/60 - D Loss: 5.6266872550114755e-08, G Loss: 0.19050441682338715 - 17.85 s\n",
      "Epoch 1029/3000, 60/60 - D Loss: 5.2405507849906385e-08, G Loss: 0.18918436765670776 - 17.70 s\n",
      "Epoch 1030/3000, 60/60 - D Loss: 5.3936052457626005e-08, G Loss: 0.19242677092552185 - 17.65 s\n",
      "Epoch 1031/3000, 60/60 - D Loss: 5.6884705085247234e-08, G Loss: 0.19062703847885132 - 17.73 s\n",
      "Epoch 1032/3000, 60/60 - D Loss: 4.68313272883325e-08, G Loss: 0.1957690417766571 - 17.87 s\n",
      "Epoch 1033/3000, 60/60 - D Loss: 4.3918667314013504e-08, G Loss: 0.19013084471225739 - 17.69 s\n",
      "Epoch 1034/3000, 60/60 - D Loss: 4.5968626266601476e-08, G Loss: 0.19094689190387726 - 17.67 s\n",
      "Epoch 1035/3000, 60/60 - D Loss: 4.1479029239122056e-08, G Loss: 0.1900758147239685 - 17.70 s\n",
      "Epoch 1036/3000, 60/60 - D Loss: 3.9098641338441986e-08, G Loss: 0.19007346034049988 - 17.53 s\n",
      "Epoch 1037/3000, 60/60 - D Loss: 3.600173073547985e-08, G Loss: 0.18967574834823608 - 17.58 s\n",
      "Epoch 1038/3000, 60/60 - D Loss: 3.540782619881888e-08, G Loss: 0.193368062376976 - 17.57 s\n",
      "Epoch 1039/3000, 60/60 - D Loss: 3.133025471002693e-08, G Loss: 0.1895298808813095 - 17.55 s\n",
      "Epoch 1040/3000, 60/60 - D Loss: 3.262553124883105e-08, G Loss: 0.19034765660762787 - 17.92 s\n",
      "Epoch 1041/3000, 60/60 - D Loss: 3.939496306254453e-08, G Loss: 0.18918830156326294 - 17.76 s\n",
      "Epoch 1042/3000, 60/60 - D Loss: 4.7436004076140126e-08, G Loss: 0.19112657010555267 - 17.83 s\n",
      "Epoch 1043/3000, 60/60 - D Loss: 3.2160745185028095e-08, G Loss: 0.19085176289081573 - 17.77 s\n",
      "Epoch 1044/3000, 60/60 - D Loss: 2.8077224281664037e-08, G Loss: 0.19100205600261688 - 17.43 s\n",
      "Epoch 1045/3000, 60/60 - D Loss: 2.8003079327598712e-08, G Loss: 0.1909155249595642 - 17.62 s\n",
      "Epoch 1046/3000, 60/60 - D Loss: 2.4428242512142262e-08, G Loss: 0.1913001537322998 - 17.65 s\n",
      "Epoch 1047/3000, 60/60 - D Loss: 2.2638568394905967e-08, G Loss: 0.18970130383968353 - 17.68 s\n",
      "Epoch 1048/3000, 60/60 - D Loss: 2.2011495390272806e-08, G Loss: 0.19073647260665894 - 17.58 s\n",
      "Epoch 1049/3000, 60/60 - D Loss: 1.854505386061911e-08, G Loss: 0.19037002325057983 - 17.71 s\n",
      "Epoch 1050/3000, 60/60 - D Loss: 1.6886879443400726e-08, G Loss: 0.19053907692432404 - 17.74 s\n",
      "Epoch 1051/3000, 60/60 - D Loss: 1.6840002024787193e-08, G Loss: 0.1904444545507431 - 17.78 s\n",
      "Epoch 1052/3000, 60/60 - D Loss: 1.6931971338896012e-08, G Loss: 0.1951570063829422 - 17.75 s\n",
      "Epoch 1053/3000, 60/60 - D Loss: 1.6392218724764485e-08, G Loss: 0.19244210422039032 - 17.91 s\n",
      "Epoch 1054/3000, 60/60 - D Loss: 1.5313537448519154e-08, G Loss: 0.1895303726196289 - 17.67 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1055/3000, 60/60 - D Loss: 1.343823620089668e-08, G Loss: 0.18884605169296265 - 18.10 s\n",
      "Epoch 1056/3000, 60/60 - D Loss: 1.2203150233369708e-08, G Loss: 0.19209766387939453 - 17.67 s\n",
      "Epoch 1057/3000, 60/60 - D Loss: 1.2399526801154526e-08, G Loss: 0.19120335578918457 - 17.81 s\n",
      "Epoch 1058/3000, 60/60 - D Loss: 1.2199037661975165e-08, G Loss: 0.1914479285478592 - 17.97 s\n",
      "Epoch 1059/3000, 60/60 - D Loss: 1.0223503089240582e-08, G Loss: 0.19004686176776886 - 17.73 s\n",
      "Epoch 1060/3000, 60/60 - D Loss: 9.381742594599674e-09, G Loss: 0.18976102769374847 - 17.62 s\n",
      "Epoch 1061/3000, 60/60 - D Loss: 9.910250942796184e-09, G Loss: 0.19003675878047943 - 17.67 s\n",
      "Epoch 1062/3000, 60/60 - D Loss: 8.749778648148299e-09, G Loss: 0.19113412499427795 - 17.71 s\n",
      "Epoch 1063/3000, 60/60 - D Loss: 7.994061043334333e-09, G Loss: 0.19019672274589539 - 17.46 s\n",
      "Epoch 1064/3000, 60/60 - D Loss: 7.7706617772344e-09, G Loss: 0.18986819684505463 - 17.62 s\n",
      "Epoch 1065/3000, 60/60 - D Loss: 7.493609262623768e-09, G Loss: 0.19020918011665344 - 17.71 s\n",
      "Epoch 1066/3000, 60/60 - D Loss: 7.437138436344748e-09, G Loss: 0.19054098427295685 - 17.70 s\n",
      "Epoch 1067/3000, 60/60 - D Loss: 6.559058979109522e-09, G Loss: 0.18953916430473328 - 17.69 s\n",
      "Epoch 1068/3000, 60/60 - D Loss: 6.3887826383390856e-09, G Loss: 0.19184325635433197 - 17.94 s\n",
      "Epoch 1069/3000, 60/60 - D Loss: 5.9892428516672425e-09, G Loss: 0.1944931447505951 - 17.67 s\n",
      "Epoch 1070/3000, 60/60 - D Loss: 5.9408496082616336e-09, G Loss: 0.1909029483795166 - 18.01 s\n",
      "Epoch 1071/3000, 60/60 - D Loss: 6.51344562183423e-09, G Loss: 0.18982014060020447 - 17.86 s\n",
      "Epoch 1072/3000, 60/60 - D Loss: 6.44276943167349e-09, G Loss: 0.18989962339401245 - 17.52 s\n",
      "Epoch 1073/3000, 60/60 - D Loss: 6.8376643448837815e-09, G Loss: 0.19166074693202972 - 17.66 s\n",
      "Epoch 1074/3000, 60/60 - D Loss: 6.526680665971257e-09, G Loss: 0.19492268562316895 - 17.82 s\n",
      "Epoch 1075/3000, 60/60 - D Loss: 7.798907656394322e-09, G Loss: 0.19019685685634613 - 17.63 s\n",
      "Epoch 1076/3000, 60/60 - D Loss: 5.889817183881568e-09, G Loss: 0.1892637461423874 - 17.93 s\n",
      "Epoch 1077/3000, 60/60 - D Loss: 5.42469122443226e-09, G Loss: 0.19178050756454468 - 17.70 s\n",
      "Epoch 1078/3000, 60/60 - D Loss: 5.328646167758813e-09, G Loss: 0.1931234747171402 - 17.80 s\n",
      "Epoch 1079/3000, 60/60 - D Loss: 4.321493716260422e-09, G Loss: 0.19194626808166504 - 17.68 s\n",
      "Epoch 1080/3000, 60/60 - D Loss: 4.039750997044886e-09, G Loss: 0.19086109101772308 - 17.56 s\n",
      "Epoch 1081/3000, 60/60 - D Loss: 3.6897931060368705e-09, G Loss: 0.19013722240924835 - 17.67 s\n",
      "Epoch 1082/3000, 60/60 - D Loss: 3.2768099830771114e-09, G Loss: 0.1912521868944168 - 17.73 s\n",
      "Epoch 1083/3000, 60/60 - D Loss: 3.302551264145609e-09, G Loss: 0.19165772199630737 - 17.61 s\n",
      "Epoch 1084/3000, 60/60 - D Loss: 3.460804380486099e-09, G Loss: 0.19088193774223328 - 17.76 s\n",
      "Epoch 1085/3000, 60/60 - D Loss: 3.3159996891364216e-09, G Loss: 0.19034399092197418 - 18.03 s\n",
      "Epoch 1086/3000, 60/60 - D Loss: 3.5913446641468866e-09, G Loss: 0.19106104969978333 - 17.56 s\n",
      "Epoch 1087/3000, 60/60 - D Loss: 3.3313580780677116e-09, G Loss: 0.1940322369337082 - 17.86 s\n",
      "Epoch 1088/3000, 60/60 - D Loss: 3.1892781764044503e-09, G Loss: 0.1930869221687317 - 17.57 s\n",
      "Epoch 1089/3000, 60/60 - D Loss: 2.672400196534336e-09, G Loss: 0.1915615200996399 - 17.98 s\n",
      "Epoch 1090/3000, 60/60 - D Loss: 3.5857041102592066e-09, G Loss: 0.18989764153957367 - 17.70 s\n",
      "Epoch 1091/3000, 60/60 - D Loss: 2.992899783582116e-09, G Loss: 0.18991884589195251 - 17.71 s\n",
      "Epoch 1092/3000, 60/60 - D Loss: 5.298055619868888e-09, G Loss: 0.19174404442310333 - 18.29 s\n",
      "Epoch 1093/3000, 60/60 - D Loss: 3.091888827052528e-09, G Loss: 0.1906062215566635 - 17.78 s\n",
      "Epoch 1094/3000, 60/60 - D Loss: 2.4331588161928715e-09, G Loss: 0.19008071720600128 - 17.72 s\n",
      "Epoch 1095/3000, 60/60 - D Loss: 2.4423307515937173e-09, G Loss: 0.18961770832538605 - 17.78 s\n",
      "Epoch 1096/3000, 60/60 - D Loss: 2.2214113680040642e-09, G Loss: 0.1898524910211563 - 17.61 s\n",
      "Epoch 1097/3000, 60/60 - D Loss: 2.3040823188724864e-09, G Loss: 0.19090119004249573 - 17.65 s\n",
      "Epoch 1098/3000, 60/60 - D Loss: 2.0952376210436365e-09, G Loss: 0.1907372623682022 - 17.92 s\n",
      "Epoch 1099/3000, 60/60 - D Loss: 1.9767360964783633e-09, G Loss: 0.19000358879566193 - 18.17 s\n",
      "Epoch 1100/3000, 60/60 - D Loss: 1.9829735793935183e-09, G Loss: 0.19339656829833984 - 18.01 s\n",
      "Epoch 1101/3000, 60/60 - D Loss: 2.2191662386286844e-09, G Loss: 0.1892041712999344 - 17.77 s\n",
      "Epoch 1102/3000, 60/60 - D Loss: 1.917485969077414e-09, G Loss: 0.1906532198190689 - 17.61 s\n",
      "Epoch 1103/3000, 60/60 - D Loss: 1.8239147309443912e-09, G Loss: 0.1900918334722519 - 17.79 s\n",
      "Epoch 1104/3000, 60/60 - D Loss: 1.827566001311176e-09, G Loss: 0.19124749302864075 - 17.89 s\n",
      "Epoch 1105/3000, 60/60 - D Loss: 1.8057939853177198e-09, G Loss: 0.19013476371765137 - 17.85 s\n",
      "Epoch 1106/3000, 60/60 - D Loss: 1.9190076799217685e-09, G Loss: 0.19131074845790863 - 17.76 s\n",
      "Epoch 1107/3000, 60/60 - D Loss: 1.8461260800238175e-09, G Loss: 0.19089581072330475 - 17.86 s\n",
      "Epoch 1108/3000, 60/60 - D Loss: 1.7762660653051956e-09, G Loss: 0.19373401999473572 - 17.73 s\n",
      "Epoch 1109/3000, 60/60 - D Loss: 1.7558439471528162e-09, G Loss: 0.18963457643985748 - 17.58 s\n",
      "Epoch 1110/3000, 60/60 - D Loss: 1.7261866515447689e-09, G Loss: 0.19042205810546875 - 17.90 s\n",
      "Epoch 1111/3000, 60/60 - D Loss: 1.8036494595227107e-09, G Loss: 0.19060491025447845 - 17.75 s\n",
      "Epoch 1112/3000, 60/60 - D Loss: 1.8619185291051266e-09, G Loss: 0.19069857895374298 - 17.80 s\n",
      "Epoch 1113/3000, 60/60 - D Loss: 1.8525188480932823e-09, G Loss: 0.19024105370044708 - 17.67 s\n",
      "Epoch 1114/3000, 60/60 - D Loss: 1.860077813699507e-09, G Loss: 0.19039157032966614 - 17.60 s\n",
      "Epoch 1115/3000, 60/60 - D Loss: 1.878077912262875e-09, G Loss: 0.1895243376493454 - 18.34 s\n",
      "Epoch 1116/3000, 60/60 - D Loss: 1.8610853210366141e-09, G Loss: 0.18984976410865784 - 17.81 s\n",
      "Epoch 1117/3000, 60/60 - D Loss: 1.6966154370084627e-09, G Loss: 0.19043679535388947 - 17.63 s\n",
      "Epoch 1118/3000, 60/60 - D Loss: 1.7398316136038854e-09, G Loss: 0.18944305181503296 - 17.63 s\n",
      "Epoch 1119/3000, 60/60 - D Loss: 1.7804088851000946e-09, G Loss: 0.19161953032016754 - 17.66 s\n",
      "Epoch 1120/3000, 60/60 - D Loss: 1.53137147672908e-09, G Loss: 0.18912434577941895 - 17.80 s\n",
      "Epoch 1121/3000, 60/60 - D Loss: 1.7486811660694969e-09, G Loss: 0.19220885634422302 - 17.54 s\n",
      "Epoch 1122/3000, 60/60 - D Loss: 1.594111295928364e-09, G Loss: 0.19011828303337097 - 17.68 s\n",
      "Epoch 1123/3000, 60/60 - D Loss: 1.6142203025919667e-09, G Loss: 0.19574324786663055 - 17.85 s\n",
      "Epoch 1124/3000, 60/60 - D Loss: 1.3345569270994603e-09, G Loss: 0.18949030339717865 - 17.61 s\n",
      "Epoch 1125/3000, 60/60 - D Loss: 1.5816395130987919e-09, G Loss: 0.19002914428710938 - 17.78 s\n",
      "Epoch 1126/3000, 60/60 - D Loss: 1.5457355918781397e-09, G Loss: 0.19081328809261322 - 17.64 s\n",
      "Epoch 1127/3000, 60/60 - D Loss: 1.4929141555663139e-09, G Loss: 0.18980816006660461 - 17.64 s\n",
      "Epoch 1128/3000, 60/60 - D Loss: 1.5090978558066981e-09, G Loss: 0.19077759981155396 - 17.70 s\n",
      "Epoch 1129/3000, 60/60 - D Loss: 1.42263021768983e-09, G Loss: 0.19078809022903442 - 17.75 s\n",
      "Epoch 1130/3000, 60/60 - D Loss: 1.5012093837046e-09, G Loss: 0.20438307523727417 - 17.84 s\n",
      "Epoch 1131/3000, 60/60 - D Loss: 1.4392416819284824e-09, G Loss: 0.1899689882993698 - 17.90 s\n",
      "Epoch 1132/3000, 60/60 - D Loss: 1.498584584062098e-09, G Loss: 0.19030259549617767 - 17.67 s\n",
      "Epoch 1133/3000, 60/60 - D Loss: 1.458500693794231e-09, G Loss: 0.1906355917453766 - 17.67 s\n",
      "Epoch 1134/3000, 60/60 - D Loss: 1.4987574284397974e-09, G Loss: 0.189214289188385 - 17.53 s\n",
      "Epoch 1135/3000, 60/60 - D Loss: 1.4289197638035723e-09, G Loss: 0.19002850353717804 - 17.59 s\n",
      "Epoch 1136/3000, 60/60 - D Loss: 1.4096266896123808e-09, G Loss: 0.18893693387508392 - 17.60 s\n",
      "Epoch 1137/3000, 60/60 - D Loss: 1.3405337376173193e-09, G Loss: 0.1912364959716797 - 17.70 s\n",
      "Epoch 1138/3000, 60/60 - D Loss: 1.2902767418680652e-09, G Loss: 0.18976326286792755 - 17.87 s\n",
      "Epoch 1139/3000, 60/60 - D Loss: 1.2012919767685456e-09, G Loss: 0.1914292722940445 - 17.70 s\n",
      "Epoch 1140/3000, 60/60 - D Loss: 1.3121601244523839e-09, G Loss: 0.1987334042787552 - 17.69 s\n",
      "Epoch 1141/3000, 60/60 - D Loss: 1.3387780726273031e-09, G Loss: 0.19131174683570862 - 17.93 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1142/3000, 60/60 - D Loss: 1.3458778501802783e-09, G Loss: 0.19233866035938263 - 17.65 s\n",
      "Epoch 1143/3000, 60/60 - D Loss: 1.3211393675555648e-09, G Loss: 0.19054415822029114 - 17.72 s\n",
      "Epoch 1144/3000, 60/60 - D Loss: 1.2821612737859032e-09, G Loss: 0.19012640416622162 - 17.83 s\n",
      "Epoch 1145/3000, 60/60 - D Loss: 1.475718987466049e-09, G Loss: 0.1915949136018753 - 17.82 s\n",
      "Epoch 1146/3000, 60/60 - D Loss: 1.494929896174869e-09, G Loss: 0.19285136461257935 - 18.05 s\n",
      "Epoch 1147/3000, 60/60 - D Loss: 1.2850593545277612e-09, G Loss: 0.18904727697372437 - 17.74 s\n",
      "Epoch 1148/3000, 60/60 - D Loss: 1.4094363732556855e-09, G Loss: 0.1907677799463272 - 17.72 s\n",
      "Epoch 1149/3000, 60/60 - D Loss: 1.2972098030556638e-09, G Loss: 0.1895090639591217 - 17.86 s\n",
      "Epoch 1150/3000, 60/60 - D Loss: 1.4576109987044406e-09, G Loss: 0.18955615162849426 - 17.60 s\n",
      "Epoch 1151/3000, 60/60 - D Loss: 1.4016802439394062e-09, G Loss: 0.19020476937294006 - 17.67 s\n",
      "Epoch 1152/3000, 60/60 - D Loss: 1.1456226979911287e-09, G Loss: 0.1907983273267746 - 17.70 s\n",
      "Epoch 1153/3000, 60/60 - D Loss: 1.4521184698145103e-09, G Loss: 0.18878310918807983 - 17.60 s\n",
      "Epoch 1154/3000, 60/60 - D Loss: 1.376790358803559e-09, G Loss: 0.19097116589546204 - 17.65 s\n",
      "Epoch 1155/3000, 60/60 - D Loss: 1.4047082597793185e-09, G Loss: 0.18963217735290527 - 17.64 s\n",
      "Epoch 1156/3000, 60/60 - D Loss: 1.462284929956508e-09, G Loss: 0.18919025361537933 - 17.55 s\n",
      "Epoch 1157/3000, 60/60 - D Loss: 1.5532029420348712e-09, G Loss: 0.1906920075416565 - 17.82 s\n",
      "Epoch 1158/3000, 60/60 - D Loss: 1.5374051606846474e-09, G Loss: 0.19114457070827484 - 17.59 s\n",
      "Epoch 1159/3000, 60/60 - D Loss: 1.5906777467544872e-09, G Loss: 0.1911669671535492 - 17.99 s\n",
      "Epoch 1160/3000, 60/60 - D Loss: 1.6727596720938467e-09, G Loss: 0.18995574116706848 - 17.76 s\n",
      "Epoch 1161/3000, 60/60 - D Loss: 1.6594361071625274e-09, G Loss: 0.18943485617637634 - 17.92 s\n",
      "Epoch 1162/3000, 60/60 - D Loss: 1.6534316532907539e-09, G Loss: 0.1894025355577469 - 17.93 s\n",
      "Epoch 1163/3000, 60/60 - D Loss: 1.414845251590871e-09, G Loss: 0.1892431229352951 - 17.71 s\n",
      "Epoch 1164/3000, 60/60 - D Loss: 1.5140387582370028e-09, G Loss: 0.19240327179431915 - 18.12 s\n",
      "Epoch 1165/3000, 60/60 - D Loss: 1.5556212293934288e-09, G Loss: 0.19125208258628845 - 17.75 s\n",
      "Epoch 1166/3000, 60/60 - D Loss: 1.5424878168128974e-09, G Loss: 0.1890992671251297 - 17.80 s\n",
      "Epoch 1167/3000, 60/60 - D Loss: 1.4788396479004855e-09, G Loss: 0.19308076798915863 - 17.84 s\n",
      "Epoch 1168/3000, 60/60 - D Loss: 1.4877406473549914e-09, G Loss: 0.18992291390895844 - 17.83 s\n",
      "Epoch 1169/3000, 60/60 - D Loss: 1.6988916997032351e-09, G Loss: 0.19067713618278503 - 17.71 s\n",
      "Epoch 1170/3000, 60/60 - D Loss: 1.6503356825216416e-09, G Loss: 0.1912134289741516 - 17.85 s\n",
      "Epoch 1171/3000, 60/60 - D Loss: 1.6403054164379266e-09, G Loss: 0.1895473301410675 - 17.81 s\n",
      "Epoch 1172/3000, 60/60 - D Loss: 1.6675863629454313e-09, G Loss: 0.1901228278875351 - 17.61 s\n",
      "Epoch 1173/3000, 60/60 - D Loss: 1.6946061900899062e-09, G Loss: 0.1907738596200943 - 17.71 s\n",
      "Epoch 1174/3000, 60/60 - D Loss: 1.7038464877919778e-09, G Loss: 0.19091172516345978 - 17.85 s\n",
      "Epoch 1175/3000, 60/60 - D Loss: 1.4345686310926248e-09, G Loss: 0.18880191445350647 - 17.69 s\n",
      "Epoch 1176/3000, 60/60 - D Loss: 1.9167921902891972e-09, G Loss: 0.18968860805034637 - 17.96 s\n",
      "Epoch 1177/3000, 60/60 - D Loss: 1.7679239114162706e-09, G Loss: 0.1896401047706604 - 17.66 s\n",
      "Epoch 1178/3000, 60/60 - D Loss: 1.5121249331781505e-09, G Loss: 0.19141259789466858 - 17.79 s\n",
      "Epoch 1179/3000, 60/60 - D Loss: 1.2513287626675536e-09, G Loss: 0.18959149718284607 - 18.26 s\n",
      "Epoch 1180/3000, 60/60 - D Loss: 1.69943542157347e-09, G Loss: 0.19100823998451233 - 17.75 s\n",
      "Epoch 1181/3000, 60/60 - D Loss: 2.0140672197699365e-09, G Loss: 0.19118735194206238 - 17.63 s\n",
      "Epoch 1182/3000, 60/60 - D Loss: 2.2103803358643594e-09, G Loss: 0.19060395658016205 - 17.52 s\n",
      "Epoch 1183/3000, 60/60 - D Loss: 1.1139633162232933e-08, G Loss: 0.19161289930343628 - 17.73 s\n",
      "Epoch 1184/3000, 60/60 - D Loss: 5.1979761911836314e-12, G Loss: 0.19007590413093567 - 17.93 s\n",
      "Epoch 1185/3000, 60/60 - D Loss: 1.0355797003345352e-11, G Loss: 0.1908944994211197 - 17.76 s\n",
      "Epoch 1186/3000, 60/60 - D Loss: 3.201619085948723e-11, G Loss: 0.19013963639736176 - 17.77 s\n",
      "Epoch 1187/3000, 60/60 - D Loss: 2.503628042187754e-12, G Loss: 0.1904435157775879 - 17.67 s\n",
      "Epoch 1188/3000, 60/60 - D Loss: 5.020844891646774e-12, G Loss: 0.19077415764331818 - 17.74 s\n",
      "Epoch 1189/3000, 60/60 - D Loss: 4.949847985918215e-12, G Loss: 0.1913195550441742 - 17.83 s\n",
      "Epoch 1190/3000, 60/60 - D Loss: 1.710102895337469e-11, G Loss: 0.19084115326404572 - 17.72 s\n",
      "Epoch 1191/3000, 60/60 - D Loss: 2.109943160943581e-12, G Loss: 0.1896696537733078 - 17.84 s\n",
      "Epoch 1192/3000, 60/60 - D Loss: 3.754927074025964e-12, G Loss: 0.18937362730503082 - 17.94 s\n",
      "Epoch 1193/3000, 60/60 - D Loss: 1.3095888441597495e-12, G Loss: 0.18910160660743713 - 17.70 s\n",
      "Epoch 1194/3000, 60/60 - D Loss: 1.8249030623229343e-12, G Loss: 0.18899354338645935 - 17.72 s\n",
      "Epoch 1195/3000, 60/60 - D Loss: 4.132620162961356e-12, G Loss: 0.18977965414524078 - 17.76 s\n",
      "Epoch 1196/3000, 60/60 - D Loss: 1.4068264375709585e-12, G Loss: 0.18934927880764008 - 17.85 s\n",
      "Epoch 1197/3000, 60/60 - D Loss: 7.71578138295155e-12, G Loss: 0.19038088619709015 - 17.78 s\n",
      "Epoch 1198/3000, 60/60 - D Loss: 5.65441167195771e-12, G Loss: 0.19015510380268097 - 17.86 s\n",
      "Epoch 1199/3000, 60/60 - D Loss: 2.0603405049765544e-12, G Loss: 0.19071364402770996 - 17.77 s\n",
      "Epoch 1200/3000, 60/60 - D Loss: 7.21337933503631e-12, G Loss: 0.19220341742038727 - 17.57 s\n",
      "Epoch 1201/3000, 60/60 - D Loss: 1.592552755251206e-11, G Loss: 0.19091777503490448 - 17.63 s\n",
      "Epoch 1202/3000, 60/60 - D Loss: 5.5892277297294596e-12, G Loss: 0.18922454118728638 - 17.69 s\n",
      "Epoch 1203/3000, 60/60 - D Loss: 1.8017438669498675e-12, G Loss: 0.18958301842212677 - 17.80 s\n",
      "Epoch 1204/3000, 60/60 - D Loss: 7.281572534604402e-13, G Loss: 0.1900191605091095 - 17.86 s\n",
      "Epoch 1205/3000, 60/60 - D Loss: 1.858307182179414e-12, G Loss: 0.1894010603427887 - 17.68 s\n",
      "Epoch 1206/3000, 60/60 - D Loss: 2.330209674305736e-12, G Loss: 0.189515620470047 - 17.53 s\n",
      "Epoch 1207/3000, 60/60 - D Loss: 8.947950616133155e-13, G Loss: 0.1908419132232666 - 17.76 s\n",
      "Epoch 1208/3000, 60/60 - D Loss: 6.462413747578347e-13, G Loss: 0.19047442078590393 - 17.60 s\n",
      "Epoch 1209/3000, 60/60 - D Loss: 7.466198612051528e-13, G Loss: 0.19059300422668457 - 17.66 s\n",
      "Epoch 1210/3000, 60/60 - D Loss: 5.849731438719967e-13, G Loss: 0.1908387541770935 - 17.58 s\n",
      "Epoch 1211/3000, 60/60 - D Loss: 6.025842463354934e-13, G Loss: 0.19127623736858368 - 17.56 s\n",
      "Epoch 1212/3000, 60/60 - D Loss: 1.0815318367447813e-12, G Loss: 0.1901508867740631 - 17.77 s\n",
      "Epoch 1213/3000, 60/60 - D Loss: 1.2533567933514789e-12, G Loss: 0.19233903288841248 - 17.65 s\n",
      "Epoch 1214/3000, 60/60 - D Loss: 7.873145494927125e-13, G Loss: 0.18961955606937408 - 17.78 s\n",
      "Epoch 1215/3000, 60/60 - D Loss: 8.433851761500272e-13, G Loss: 0.18935410678386688 - 17.75 s\n",
      "Epoch 1216/3000, 60/60 - D Loss: 1.6573784987657103e-12, G Loss: 0.18997474014759064 - 17.71 s\n",
      "Epoch 1217/3000, 60/60 - D Loss: 2.721042466867496e-12, G Loss: 0.18993757665157318 - 17.85 s\n",
      "Epoch 1218/3000, 60/60 - D Loss: 3.6268068626457706e-12, G Loss: 0.1911216825246811 - 17.61 s\n",
      "Epoch 1219/3000, 60/60 - D Loss: 1.2068327430057521e-12, G Loss: 0.18951983749866486 - 17.61 s\n",
      "Epoch 1220/3000, 60/60 - D Loss: 1.2068592381963422e-12, G Loss: 0.18945975601673126 - 17.72 s\n",
      "Epoch 1221/3000, 60/60 - D Loss: 2.372578696008265e-12, G Loss: 0.19180157780647278 - 17.72 s\n",
      "Epoch 1222/3000, 60/60 - D Loss: 1.3022840726802098e-12, G Loss: 0.20093072950839996 - 18.15 s\n",
      "Epoch 1223/3000, 60/60 - D Loss: 1.5098675348185209e-12, G Loss: 0.18948565423488617 - 17.44 s\n",
      "Epoch 1224/3000, 60/60 - D Loss: 1.2905820414633486e-12, G Loss: 0.19012726843357086 - 16.98 s\n",
      "Epoch 1225/3000, 60/60 - D Loss: 1.4726336063130077e-12, G Loss: 0.1896900087594986 - 17.62 s\n",
      "Epoch 1226/3000, 60/60 - D Loss: 1.4781511924839494e-12, G Loss: 0.18921402096748352 - 17.67 s\n",
      "Epoch 1227/3000, 60/60 - D Loss: 2.5295534980887546e-12, G Loss: 0.19076518714427948 - 17.60 s\n",
      "Epoch 1228/3000, 60/60 - D Loss: 2.5967661181069968e-12, G Loss: 0.1966961771249771 - 17.58 s\n",
      "Epoch 1229/3000, 60/60 - D Loss: 1.7872608639368445e-12, G Loss: 0.1887018382549286 - 17.72 s\n",
      "Epoch 1230/3000, 60/60 - D Loss: 3.716673344957011e-12, G Loss: 0.19056421518325806 - 17.58 s\n",
      "Epoch 1231/3000, 60/60 - D Loss: 2.3323698658192504e-12, G Loss: 0.19289246201515198 - 17.86 s\n",
      "Epoch 1232/3000, 60/60 - D Loss: 2.2710490579043105e-12, G Loss: 0.18912525475025177 - 17.62 s\n",
      "Epoch 1233/3000, 60/60 - D Loss: 3.4022481013029626e-12, G Loss: 0.18957801163196564 - 17.83 s\n",
      "Epoch 1234/3000, 60/60 - D Loss: 3.4128531164329123e-12, G Loss: 0.19098787009716034 - 17.67 s\n",
      "Epoch 1235/3000, 60/60 - D Loss: 2.681863493217071e-12, G Loss: 0.19335311651229858 - 17.51 s\n",
      "Epoch 1236/3000, 60/60 - D Loss: 2.7753455051704803e-12, G Loss: 0.19033700227737427 - 17.73 s\n",
      "Epoch 1237/3000, 60/60 - D Loss: 3.2018436025346014e-12, G Loss: 0.18914836645126343 - 17.90 s\n",
      "Epoch 1238/3000, 60/60 - D Loss: 3.3451802661449773e-12, G Loss: 0.1901022046804428 - 17.54 s\n",
      "Epoch 1239/3000, 60/60 - D Loss: 3.784119190415082e-12, G Loss: 0.18916374444961548 - 17.72 s\n",
      "Epoch 1240/3000, 60/60 - D Loss: 4.411344445114096e-12, G Loss: 0.18960796296596527 - 17.81 s\n",
      "Epoch 1241/3000, 60/60 - D Loss: 6.553632636574491e-12, G Loss: 0.1883566975593567 - 17.82 s\n",
      "Epoch 1242/3000, 60/60 - D Loss: 5.0810844789452025e-12, G Loss: 0.18920521438121796 - 17.70 s\n",
      "Epoch 1243/3000, 60/60 - D Loss: 6.993008602740325e-12, G Loss: 0.18935741484165192 - 17.57 s\n",
      "Epoch 1244/3000, 60/60 - D Loss: 5.830792337672974e-12, G Loss: 0.19056802988052368 - 17.70 s\n",
      "Epoch 1245/3000, 60/60 - D Loss: 6.399203486984889e-12, G Loss: 0.19655190408229828 - 17.92 s\n",
      "Epoch 1246/3000, 60/60 - D Loss: 6.388059623375214e-12, G Loss: 0.19384607672691345 - 17.57 s\n",
      "Epoch 1247/3000, 60/60 - D Loss: 8.014590401929367e-12, G Loss: 0.18987230956554413 - 17.69 s\n",
      "Epoch 1248/3000, 60/60 - D Loss: 7.591712740222245e-12, G Loss: 0.18903781473636627 - 17.53 s\n",
      "Epoch 1249/3000, 60/60 - D Loss: 8.248642112233806e-12, G Loss: 0.1975696086883545 - 17.67 s\n",
      "Epoch 1250/3000, 60/60 - D Loss: 8.966190501646584e-12, G Loss: 0.19132719933986664 - 17.60 s\n",
      "Epoch 1251/3000, 60/60 - D Loss: 9.957186876234647e-12, G Loss: 0.19006475806236267 - 17.65 s\n",
      "Epoch 1252/3000, 60/60 - D Loss: 1.0561559683460245e-11, G Loss: 0.19016577303409576 - 17.86 s\n",
      "Epoch 1253/3000, 60/60 - D Loss: 1.1795807901412644e-11, G Loss: 0.19116762280464172 - 17.58 s\n",
      "Epoch 1254/3000, 60/60 - D Loss: 1.2149983710102452e-11, G Loss: 0.1904812604188919 - 17.69 s\n",
      "Epoch 1255/3000, 60/60 - D Loss: 1.3510902737313285e-11, G Loss: 0.19062906503677368 - 17.88 s\n",
      "Epoch 1256/3000, 60/60 - D Loss: 1.7557088487045458e-11, G Loss: 0.1901998519897461 - 17.64 s\n",
      "Epoch 1257/3000, 60/60 - D Loss: 1.933801854541936e-11, G Loss: 0.18936043977737427 - 17.50 s\n",
      "Epoch 1258/3000, 60/60 - D Loss: 2.0273565297249455e-11, G Loss: 0.18923640251159668 - 17.71 s\n",
      "Epoch 1259/3000, 60/60 - D Loss: 2.1244265651114078e-11, G Loss: 0.19192318618297577 - 17.66 s\n",
      "Epoch 1260/3000, 60/60 - D Loss: 2.3822409690583382e-11, G Loss: 0.1893680989742279 - 17.64 s\n",
      "Epoch 1261/3000, 60/60 - D Loss: 2.4843106665603195e-11, G Loss: 0.1884441077709198 - 17.60 s\n",
      "Epoch 1262/3000, 60/60 - D Loss: 2.691657503437958e-11, G Loss: 0.19187763333320618 - 17.75 s\n",
      "Epoch 1263/3000, 60/60 - D Loss: 3.069763293930307e-11, G Loss: 0.19072026014328003 - 17.78 s\n",
      "Epoch 1264/3000, 60/60 - D Loss: 3.4943501861321984e-11, G Loss: 0.19201701879501343 - 17.59 s\n",
      "Epoch 1265/3000, 60/60 - D Loss: 3.881651190117087e-11, G Loss: 0.190779909491539 - 17.77 s\n",
      "Epoch 1266/3000, 60/60 - D Loss: 4.1380744046203705e-11, G Loss: 0.18951216340065002 - 17.74 s\n",
      "Epoch 1267/3000, 60/60 - D Loss: 4.742602469935881e-11, G Loss: 0.19064854085445404 - 17.62 s\n",
      "Epoch 1268/3000, 60/60 - D Loss: 4.925833813784102e-11, G Loss: 0.18973538279533386 - 17.90 s\n",
      "Epoch 1269/3000, 60/60 - D Loss: 5.266902437120013e-11, G Loss: 0.19282008707523346 - 17.82 s\n",
      "Epoch 1270/3000, 60/60 - D Loss: 5.425764442837139e-11, G Loss: 0.19018951058387756 - 17.96 s\n",
      "Epoch 1271/3000, 60/60 - D Loss: 5.6626140079984446e-11, G Loss: 0.19356606900691986 - 17.53 s\n",
      "Epoch 1272/3000, 60/60 - D Loss: 6.02362482742219e-11, G Loss: 0.19016346335411072 - 17.64 s\n",
      "Epoch 1273/3000, 60/60 - D Loss: 7.446379195083465e-11, G Loss: 0.18943415582180023 - 17.77 s\n",
      "Epoch 1274/3000, 60/60 - D Loss: 8.049945025888697e-11, G Loss: 0.1900697499513626 - 17.70 s\n",
      "Epoch 1275/3000, 60/60 - D Loss: 8.286841396735415e-11, G Loss: 0.18980273604393005 - 17.71 s\n",
      "Epoch 1276/3000, 60/60 - D Loss: 8.845210130200946e-11, G Loss: 0.18906202912330627 - 17.64 s\n",
      "Epoch 1277/3000, 60/60 - D Loss: 9.32704988004969e-11, G Loss: 0.19025693833827972 - 17.69 s\n",
      "Epoch 1278/3000, 60/60 - D Loss: 1.0411500558110578e-10, G Loss: 0.18902811408042908 - 17.66 s\n",
      "Epoch 1279/3000, 60/60 - D Loss: 1.0684987439377614e-10, G Loss: 0.18995694816112518 - 18.08 s\n",
      "Epoch 1280/3000, 60/60 - D Loss: 1.1226396312677585e-10, G Loss: 0.190677672624588 - 17.90 s\n",
      "Epoch 1281/3000, 60/60 - D Loss: 1.1530076201056037e-10, G Loss: 0.189641535282135 - 17.78 s\n",
      "Epoch 1282/3000, 60/60 - D Loss: 1.1757292951448516e-10, G Loss: 0.19040422141551971 - 17.81 s\n",
      "Epoch 1283/3000, 60/60 - D Loss: 1.2054110769439685e-10, G Loss: 0.1886499673128128 - 18.11 s\n",
      "Epoch 1284/3000, 60/60 - D Loss: 1.2872956748553573e-10, G Loss: 0.18958133459091187 - 17.56 s\n",
      "Epoch 1285/3000, 60/60 - D Loss: 1.3195585261001038e-10, G Loss: 0.18981967866420746 - 17.56 s\n",
      "Epoch 1286/3000, 60/60 - D Loss: 1.3816235338421322e-10, G Loss: 0.18826451897621155 - 17.53 s\n",
      "Epoch 1287/3000, 60/60 - D Loss: 1.4114549694562513e-10, G Loss: 0.18986865878105164 - 17.74 s\n",
      "Epoch 1288/3000, 60/60 - D Loss: 1.5084974578066096e-10, G Loss: 0.19057662785053253 - 17.78 s\n",
      "Epoch 1289/3000, 60/60 - D Loss: 1.6317500246777663e-10, G Loss: 0.18997198343276978 - 17.68 s\n",
      "Epoch 1290/3000, 60/60 - D Loss: 1.8328026689238457e-10, G Loss: 0.19065603613853455 - 17.72 s\n",
      "Epoch 1291/3000, 60/60 - D Loss: 1.891262323209771e-10, G Loss: 0.1888374239206314 - 17.65 s\n",
      "Epoch 1292/3000, 60/60 - D Loss: 1.948308473104579e-10, G Loss: 0.18921791017055511 - 17.88 s\n",
      "Epoch 1293/3000, 60/60 - D Loss: 2.0104306666158568e-10, G Loss: 0.1899355798959732 - 17.81 s\n",
      "Epoch 1294/3000, 60/60 - D Loss: 2.2171810570782886e-10, G Loss: 0.18908333778381348 - 17.70 s\n",
      "Epoch 1295/3000, 60/60 - D Loss: 2.835728301481266e-10, G Loss: 0.19016437232494354 - 17.51 s\n",
      "Epoch 1296/3000, 60/60 - D Loss: 2.828511194591398e-10, G Loss: 0.1908358633518219 - 17.81 s\n",
      "Epoch 1297/3000, 60/60 - D Loss: 2.784371091048792e-10, G Loss: 0.19114871323108673 - 17.71 s\n",
      "Epoch 1298/3000, 60/60 - D Loss: 2.7704220713445924e-10, G Loss: 0.19221751391887665 - 17.90 s\n",
      "Epoch 1299/3000, 60/60 - D Loss: 2.692853309858422e-10, G Loss: 0.19349537789821625 - 17.57 s\n",
      "Epoch 1300/3000, 60/60 - D Loss: 2.70751170276611e-10, G Loss: 0.18884553015232086 - 17.56 s\n",
      "Epoch 1301/3000, 60/60 - D Loss: 2.645864777798293e-10, G Loss: 0.18904154002666473 - 17.71 s\n",
      "Epoch 1302/3000, 60/60 - D Loss: 2.554087017361405e-10, G Loss: 0.19013093411922455 - 17.67 s\n",
      "Epoch 1303/3000, 60/60 - D Loss: 2.506126771357293e-10, G Loss: 0.19216085970401764 - 17.80 s\n",
      "Epoch 1304/3000, 60/60 - D Loss: 2.458946972295063e-10, G Loss: 0.1951657235622406 - 17.46 s\n",
      "Epoch 1305/3000, 60/60 - D Loss: 2.5094045635524033e-10, G Loss: 0.18954156339168549 - 17.43 s\n",
      "Epoch 1306/3000, 60/60 - D Loss: 2.6992568153105497e-10, G Loss: 0.19052737951278687 - 17.68 s\n",
      "Epoch 1307/3000, 60/60 - D Loss: 2.531732634637629e-10, G Loss: 0.18889550864696503 - 17.62 s\n",
      "Epoch 1308/3000, 60/60 - D Loss: 2.4913038775022903e-10, G Loss: 0.1925601214170456 - 17.76 s\n",
      "Epoch 1309/3000, 60/60 - D Loss: 3.232114875016006e-10, G Loss: 0.18869079649448395 - 17.63 s\n",
      "Epoch 1310/3000, 60/60 - D Loss: 3.0866590257057694e-10, G Loss: 0.18918819725513458 - 17.87 s\n",
      "Epoch 1311/3000, 60/60 - D Loss: 2.754446873156637e-10, G Loss: 0.1901061087846756 - 17.67 s\n",
      "Epoch 1312/3000, 60/60 - D Loss: 2.4940046606893355e-10, G Loss: 0.19271132349967957 - 17.82 s\n",
      "Epoch 1313/3000, 60/60 - D Loss: 2.2922969841472195e-10, G Loss: 0.1893896460533142 - 17.84 s\n",
      "Epoch 1314/3000, 60/60 - D Loss: 2.1758146414414207e-10, G Loss: 0.18914589285850525 - 17.51 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1315/3000, 60/60 - D Loss: 2.080024051015852e-10, G Loss: 0.1901683807373047 - 17.58 s\n",
      "Epoch 1316/3000, 60/60 - D Loss: 2.037288115714752e-10, G Loss: 0.1902671754360199 - 17.79 s\n",
      "Epoch 1317/3000, 60/60 - D Loss: 1.995978379456956e-10, G Loss: 0.18908819556236267 - 17.55 s\n",
      "Epoch 1318/3000, 60/60 - D Loss: 1.9840215604585102e-10, G Loss: 0.18927332758903503 - 17.64 s\n",
      "Epoch 1319/3000, 60/60 - D Loss: 1.9174567237097574e-10, G Loss: 0.1891893446445465 - 17.67 s\n",
      "Epoch 1320/3000, 60/60 - D Loss: 1.8155094008354329e-10, G Loss: 0.18849025666713715 - 17.43 s\n",
      "Epoch 1321/3000, 60/60 - D Loss: 1.7410947055772927e-10, G Loss: 0.1898411065340042 - 16.93 s\n",
      "Epoch 1322/3000, 60/60 - D Loss: 1.7559864566040503e-10, G Loss: 0.19013838469982147 - 14.60 s\n",
      "Epoch 1323/3000, 60/60 - D Loss: 1.9084084933203966e-10, G Loss: 0.18876342475414276 - 16.74 s\n",
      "Epoch 1324/3000, 60/60 - D Loss: 1.7771625152523474e-10, G Loss: 0.19002215564250946 - 17.01 s\n",
      "Epoch 1325/3000, 60/60 - D Loss: 1.6811424034281468e-10, G Loss: 0.18982145190238953 - 17.20 s\n",
      "Epoch 1326/3000, 60/60 - D Loss: 1.7022298365787156e-10, G Loss: 0.18925593793392181 - 16.93 s\n",
      "Epoch 1327/3000, 60/60 - D Loss: 1.6949028601728153e-10, G Loss: 0.1890556365251541 - 17.25 s\n",
      "Epoch 1328/3000, 60/60 - D Loss: 1.6301602376870205e-10, G Loss: 0.19298677146434784 - 17.31 s\n",
      "Epoch 1329/3000, 60/60 - D Loss: 1.6555759269511596e-10, G Loss: 0.1904657930135727 - 17.08 s\n",
      "Epoch 1330/3000, 60/60 - D Loss: 1.7190889696953009e-10, G Loss: 0.18991030752658844 - 16.63 s\n",
      "Epoch 1331/3000, 60/60 - D Loss: 1.5898647824041077e-10, G Loss: 0.19255439937114716 - 16.85 s\n",
      "Epoch 1332/3000, 60/60 - D Loss: 1.5677851214061245e-10, G Loss: 0.1896914839744568 - 17.02 s\n",
      "Epoch 1333/3000, 60/60 - D Loss: 1.46071331825739e-10, G Loss: 0.1920439749956131 - 16.87 s\n",
      "Epoch 1334/3000, 60/60 - D Loss: 1.3617368195219568e-10, G Loss: 0.18974076211452484 - 17.08 s\n",
      "Epoch 1335/3000, 60/60 - D Loss: 1.2482592659449715e-10, G Loss: 0.18957118690013885 - 16.76 s\n",
      "Epoch 1336/3000, 60/60 - D Loss: 1.2294610436168188e-10, G Loss: 0.19005495309829712 - 17.00 s\n",
      "Epoch 1337/3000, 60/60 - D Loss: 1.3364736189650395e-10, G Loss: 0.18980813026428223 - 16.93 s\n",
      "Epoch 1338/3000, 60/60 - D Loss: 1.3890355022497028e-10, G Loss: 0.19219732284545898 - 17.09 s\n",
      "Epoch 1339/3000, 60/60 - D Loss: 1.3128831313813008e-10, G Loss: 0.18903674185276031 - 17.22 s\n",
      "Epoch 1340/3000, 60/60 - D Loss: 1.1814214878997997e-10, G Loss: 0.19083954393863678 - 16.91 s\n",
      "Epoch 1341/3000, 60/60 - D Loss: 1.1201747894978061e-10, G Loss: 0.18950703740119934 - 17.02 s\n",
      "Epoch 1342/3000, 60/60 - D Loss: 1.1076211348640439e-10, G Loss: 0.19142715632915497 - 17.18 s\n",
      "Epoch 1343/3000, 60/60 - D Loss: 1.0787406763111695e-10, G Loss: 0.18991602957248688 - 17.31 s\n",
      "Epoch 1344/3000, 60/60 - D Loss: 1.010976832670313e-10, G Loss: 0.19048336148262024 - 17.04 s\n",
      "Epoch 1345/3000, 60/60 - D Loss: 1.0264889340893215e-10, G Loss: 0.18977342545986176 - 16.92 s\n",
      "Epoch 1346/3000, 60/60 - D Loss: 9.506677493594103e-11, G Loss: 0.18882328271865845 - 17.08 s\n",
      "Epoch 1347/3000, 60/60 - D Loss: 9.607864180720857e-11, G Loss: 0.19003233313560486 - 17.02 s\n",
      "Epoch 1348/3000, 60/60 - D Loss: 9.258694274767686e-11, G Loss: 0.19028547406196594 - 17.13 s\n",
      "Epoch 1349/3000, 60/60 - D Loss: 8.923959253389289e-11, G Loss: 0.18934696912765503 - 16.73 s\n",
      "Epoch 1350/3000, 60/60 - D Loss: 8.612904907770737e-11, G Loss: 0.1911495327949524 - 16.66 s\n",
      "Epoch 1351/3000, 60/60 - D Loss: 8.362565145071588e-11, G Loss: 0.18951794505119324 - 17.07 s\n",
      "Epoch 1352/3000, 60/60 - D Loss: 7.913358883191011e-11, G Loss: 0.189533531665802 - 17.03 s\n",
      "Epoch 1353/3000, 60/60 - D Loss: 8.125154304369806e-11, G Loss: 0.19010843336582184 - 16.91 s\n",
      "Epoch 1354/3000, 60/60 - D Loss: 7.569246233739773e-11, G Loss: 0.19485576450824738 - 16.91 s\n",
      "Epoch 1355/3000, 60/60 - D Loss: 7.211335386512829e-11, G Loss: 0.19305507838726044 - 16.75 s\n",
      "Epoch 1356/3000, 60/60 - D Loss: 7.279002137856494e-11, G Loss: 0.19085337221622467 - 17.00 s\n",
      "Epoch 1357/3000, 60/60 - D Loss: 7.153357900438172e-11, G Loss: 0.19017928838729858 - 16.63 s\n",
      "Epoch 1358/3000, 60/60 - D Loss: 6.828199481609375e-11, G Loss: 0.18959935009479523 - 16.76 s\n",
      "Epoch 1359/3000, 60/60 - D Loss: 6.800036592392191e-11, G Loss: 0.18840369582176208 - 17.34 s\n",
      "Epoch 1360/3000, 60/60 - D Loss: 6.748710816114719e-11, G Loss: 0.18821467459201813 - 16.86 s\n",
      "Epoch 1361/3000, 60/60 - D Loss: 6.326604012877224e-11, G Loss: 0.18793322145938873 - 16.89 s\n",
      "Epoch 1362/3000, 60/60 - D Loss: 6.176817326950602e-11, G Loss: 0.18955300748348236 - 16.91 s\n",
      "Epoch 1363/3000, 60/60 - D Loss: 6.711304121115339e-11, G Loss: 0.18898504972457886 - 17.01 s\n",
      "Epoch 1364/3000, 60/60 - D Loss: 6.152212492484347e-11, G Loss: 0.18896368145942688 - 16.92 s\n",
      "Epoch 1365/3000, 60/60 - D Loss: 5.7642997135322934e-11, G Loss: 0.18888913094997406 - 16.90 s\n",
      "Epoch 1366/3000, 60/60 - D Loss: 5.496510330182372e-11, G Loss: 0.18918511271476746 - 16.83 s\n",
      "Epoch 1367/3000, 60/60 - D Loss: 5.1845864928820764e-11, G Loss: 0.1901022493839264 - 17.00 s\n",
      "Epoch 1368/3000, 60/60 - D Loss: 4.605217413274469e-11, G Loss: 0.18966446816921234 - 16.74 s\n",
      "Epoch 1369/3000, 60/60 - D Loss: 4.578937282510013e-11, G Loss: 0.190070241689682 - 16.75 s\n",
      "Epoch 1370/3000, 60/60 - D Loss: 4.695998750148707e-11, G Loss: 0.19082260131835938 - 17.02 s\n",
      "Epoch 1371/3000, 60/60 - D Loss: 4.359016289792733e-11, G Loss: 0.18936023116111755 - 16.91 s\n",
      "Epoch 1372/3000, 60/60 - D Loss: 4.458378416324443e-11, G Loss: 0.1896510124206543 - 16.89 s\n",
      "Epoch 1373/3000, 60/60 - D Loss: 4.753070601280478e-11, G Loss: 0.18998171389102936 - 16.99 s\n",
      "Epoch 1374/3000, 60/60 - D Loss: 4.217633970690239e-11, G Loss: 0.1893392652273178 - 17.12 s\n",
      "Epoch 1375/3000, 60/60 - D Loss: 4.734208185027288e-11, G Loss: 0.19321662187576294 - 16.85 s\n",
      "Epoch 1376/3000, 60/60 - D Loss: 3.974133259834516e-11, G Loss: 0.1906818300485611 - 16.97 s\n",
      "Epoch 1377/3000, 60/60 - D Loss: 3.541390792008685e-11, G Loss: 0.1895962357521057 - 16.89 s\n",
      "Epoch 1378/3000, 60/60 - D Loss: 3.2395870806210776e-11, G Loss: 0.1899687945842743 - 17.23 s\n",
      "Epoch 1379/3000, 60/60 - D Loss: 2.6896252905559608e-11, G Loss: 0.1899922788143158 - 17.13 s\n",
      "Epoch 1380/3000, 60/60 - D Loss: 2.7407488221318696e-11, G Loss: 0.18884094059467316 - 17.12 s\n",
      "Epoch 1381/3000, 60/60 - D Loss: 2.8748558191157548e-11, G Loss: 0.19048161804676056 - 17.04 s\n",
      "Epoch 1382/3000, 60/60 - D Loss: 2.5534870841805988e-11, G Loss: 0.1890602558851242 - 17.10 s\n",
      "Epoch 1383/3000, 60/60 - D Loss: 2.8631943002135495e-11, G Loss: 0.19055309891700745 - 16.91 s\n",
      "Epoch 1384/3000, 60/60 - D Loss: 3.006674991613534e-11, G Loss: 0.18967756628990173 - 17.08 s\n",
      "Epoch 1385/3000, 60/60 - D Loss: 2.9410783496851823e-11, G Loss: 0.19237105548381805 - 17.12 s\n",
      "Epoch 1386/3000, 60/60 - D Loss: 2.479315627391808e-11, G Loss: 0.19019415974617004 - 17.30 s\n",
      "Epoch 1387/3000, 60/60 - D Loss: 2.2026047627617215e-11, G Loss: 0.18943698704242706 - 17.05 s\n",
      "Epoch 1388/3000, 60/60 - D Loss: 2.1167890186477684e-11, G Loss: 0.18858925998210907 - 17.22 s\n",
      "Epoch 1389/3000, 60/60 - D Loss: 2.1678762375863858e-11, G Loss: 0.1907476931810379 - 17.26 s\n",
      "Epoch 1390/3000, 60/60 - D Loss: 2.5358645831201192e-11, G Loss: 0.1902196854352951 - 17.02 s\n",
      "Epoch 1391/3000, 60/60 - D Loss: 1.539338199920336e-11, G Loss: 0.19432282447814941 - 16.99 s\n",
      "Epoch 1392/3000, 60/60 - D Loss: 1.56905694382015e-11, G Loss: 0.1899394690990448 - 17.10 s\n",
      "Epoch 1393/3000, 60/60 - D Loss: 1.5962947396581913e-11, G Loss: 0.18897727131843567 - 16.96 s\n",
      "Epoch 1394/3000, 60/60 - D Loss: 1.441586919535266e-11, G Loss: 0.19012996554374695 - 16.98 s\n",
      "Epoch 1395/3000, 60/60 - D Loss: 1.4263452394927522e-11, G Loss: 0.1889258772134781 - 17.29 s\n",
      "Epoch 1396/3000, 60/60 - D Loss: 1.6881473681709956e-11, G Loss: 0.1893378049135208 - 16.90 s\n",
      "Epoch 1397/3000, 60/60 - D Loss: 1.5983649569664773e-11, G Loss: 0.18953058123588562 - 17.03 s\n",
      "Epoch 1398/3000, 60/60 - D Loss: 1.5545937898014248e-11, G Loss: 0.19060204923152924 - 16.91 s\n",
      "Epoch 1399/3000, 60/60 - D Loss: 1.343518557269422e-11, G Loss: 0.18951644003391266 - 17.34 s\n",
      "Epoch 1400/3000, 60/60 - D Loss: 1.495376580301738e-11, G Loss: 0.18963688611984253 - 17.25 s\n",
      "Epoch 1401/3000, 60/60 - D Loss: 1.3884001224795157e-11, G Loss: 0.1912975013256073 - 17.18 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1402/3000, 60/60 - D Loss: 1.3372363598607993e-11, G Loss: 0.18918460607528687 - 16.79 s\n",
      "Epoch 1403/3000, 60/60 - D Loss: 1.5906184191345818e-11, G Loss: 0.1893414556980133 - 16.98 s\n",
      "Epoch 1404/3000, 60/60 - D Loss: 1.4087918701480851e-11, G Loss: 0.18970756232738495 - 17.40 s\n",
      "Epoch 1405/3000, 60/60 - D Loss: 1.4314054050472856e-11, G Loss: 0.1903088241815567 - 16.88 s\n",
      "Epoch 1406/3000, 60/60 - D Loss: 1.1966524926298248e-11, G Loss: 0.18871863186359406 - 16.73 s\n",
      "Epoch 1407/3000, 60/60 - D Loss: 1.3323365744538765e-11, G Loss: 0.1902160346508026 - 16.81 s\n",
      "Epoch 1408/3000, 60/60 - D Loss: 1.274171354656176e-11, G Loss: 0.18897055089473724 - 16.64 s\n",
      "Epoch 1409/3000, 60/60 - D Loss: 1.2054854749400248e-11, G Loss: 0.18901990354061127 - 16.83 s\n",
      "Epoch 1410/3000, 60/60 - D Loss: 1.1529228924175868e-11, G Loss: 0.19016194343566895 - 17.01 s\n",
      "Epoch 1411/3000, 60/60 - D Loss: 1.1467525783353895e-11, G Loss: 0.18971356749534607 - 16.80 s\n",
      "Epoch 1412/3000, 60/60 - D Loss: 1.1213244132419355e-11, G Loss: 0.19045943021774292 - 17.51 s\n",
      "Epoch 1413/3000, 60/60 - D Loss: 1.181672984313448e-11, G Loss: 0.18940602242946625 - 16.90 s\n",
      "Epoch 1414/3000, 60/60 - D Loss: 1.3475579480112432e-11, G Loss: 0.18938657641410828 - 17.22 s\n",
      "Epoch 1415/3000, 60/60 - D Loss: 1.1583826606822798e-11, G Loss: 0.18922096490859985 - 17.10 s\n",
      "Epoch 1416/3000, 60/60 - D Loss: 1.0576644955099007e-11, G Loss: 0.1892380714416504 - 17.22 s\n",
      "Epoch 1417/3000, 60/60 - D Loss: 1.0014070327928913e-11, G Loss: 0.18932770192623138 - 16.84 s\n",
      "Epoch 1418/3000, 60/60 - D Loss: 1.1262878244804453e-11, G Loss: 0.18915754556655884 - 17.15 s\n",
      "Epoch 1419/3000, 60/60 - D Loss: 9.117712170450376e-12, G Loss: 0.18960191309452057 - 17.26 s\n",
      "Epoch 1420/3000, 60/60 - D Loss: 9.016706149446602e-12, G Loss: 0.18985961377620697 - 16.79 s\n",
      "Epoch 1421/3000, 60/60 - D Loss: 9.552804508863391e-12, G Loss: 0.19120365381240845 - 16.97 s\n",
      "Epoch 1422/3000, 60/60 - D Loss: 9.63921230945888e-12, G Loss: 0.19352753460407257 - 16.86 s\n",
      "Epoch 1423/3000, 60/60 - D Loss: 1.0043931368630739e-11, G Loss: 0.19113963842391968 - 16.87 s\n",
      "Epoch 1424/3000, 60/60 - D Loss: 1.5350887581729195e-11, G Loss: 0.18945103883743286 - 16.90 s\n",
      "Epoch 1425/3000, 60/60 - D Loss: 1.3341708398377582e-11, G Loss: 0.19015353918075562 - 17.13 s\n",
      "Epoch 1426/3000, 60/60 - D Loss: 9.935728774588452e-12, G Loss: 0.18944069743156433 - 17.12 s\n",
      "Epoch 1427/3000, 60/60 - D Loss: 8.893033024950295e-12, G Loss: 0.18900226056575775 - 17.29 s\n",
      "Epoch 1428/3000, 60/60 - D Loss: 8.898350188418645e-12, G Loss: 0.19156093895435333 - 17.02 s\n",
      "Epoch 1429/3000, 60/60 - D Loss: 7.961613717192486e-12, G Loss: 0.1906433254480362 - 16.86 s\n",
      "Epoch 1430/3000, 60/60 - D Loss: 1.457939454894995e-11, G Loss: 0.19041283428668976 - 17.03 s\n",
      "Epoch 1431/3000, 60/60 - D Loss: 6.403910467681036e-12, G Loss: 0.19057993590831757 - 17.09 s\n",
      "Epoch 1432/3000, 60/60 - D Loss: 1.2316319637743654e-11, G Loss: 0.1887473165988922 - 16.91 s\n",
      "Epoch 1433/3000, 60/60 - D Loss: 7.594517143464357e-12, G Loss: 0.18928787112236023 - 16.54 s\n",
      "Epoch 1434/3000, 60/60 - D Loss: 1.2250147899083543e-11, G Loss: 0.18843916058540344 - 17.04 s\n",
      "Epoch 1435/3000, 60/60 - D Loss: 7.457502037474809e-12, G Loss: 0.18892642855644226 - 17.56 s\n",
      "Epoch 1436/3000, 60/60 - D Loss: 1.3490349745117353e-11, G Loss: 0.19178636372089386 - 17.91 s\n",
      "Epoch 1437/3000, 60/60 - D Loss: 7.359400032392474e-12, G Loss: 0.1894509494304657 - 17.91 s\n",
      "Epoch 1438/3000, 60/60 - D Loss: 1.3669510962066995e-11, G Loss: 0.19038960337638855 - 17.71 s\n",
      "Epoch 1439/3000, 60/60 - D Loss: 7.580519459853468e-12, G Loss: 0.18935281038284302 - 17.67 s\n",
      "Epoch 1440/3000, 60/60 - D Loss: 1.1679054531294374e-11, G Loss: 0.18981948494911194 - 17.57 s\n",
      "Epoch 1441/3000, 60/60 - D Loss: 1.5125685372054505e-11, G Loss: 0.18928052484989166 - 17.55 s\n",
      "Epoch 1442/3000, 60/60 - D Loss: 7.2024449816462785e-12, G Loss: 0.1890944093465805 - 17.66 s\n",
      "Epoch 1443/3000, 60/60 - D Loss: 1.4024596708251852e-11, G Loss: 0.19184426963329315 - 17.71 s\n",
      "Epoch 1444/3000, 60/60 - D Loss: 5.9860401369269665e-12, G Loss: 0.1907353699207306 - 17.89 s\n",
      "Epoch 1445/3000, 60/60 - D Loss: 1.0429661392957334e-11, G Loss: 0.19031022489070892 - 17.89 s\n",
      "Epoch 1446/3000, 60/60 - D Loss: 1.4029557568752715e-11, G Loss: 0.18999242782592773 - 17.79 s\n",
      "Epoch 1447/3000, 60/60 - D Loss: 6.367912671477236e-12, G Loss: 0.18855805695056915 - 17.59 s\n",
      "Epoch 1448/3000, 60/60 - D Loss: 5.328063520470826e-12, G Loss: 0.18924185633659363 - 17.55 s\n",
      "Epoch 1449/3000, 60/60 - D Loss: 1.205983242429197e-11, G Loss: 0.18955619633197784 - 17.75 s\n",
      "Epoch 1450/3000, 60/60 - D Loss: 4.5866440318145896e-12, G Loss: 0.18936865031719208 - 17.73 s\n",
      "Epoch 1451/3000, 60/60 - D Loss: 1.2491861274092482e-11, G Loss: 0.19257931411266327 - 17.70 s\n",
      "Epoch 1452/3000, 60/60 - D Loss: 5.026531521665307e-12, G Loss: 0.1887003779411316 - 17.33 s\n",
      "Epoch 1453/3000, 60/60 - D Loss: 4.121132249150046e-12, G Loss: 0.18883129954338074 - 17.56 s\n",
      "Epoch 1454/3000, 60/60 - D Loss: 8.201925263002516e-12, G Loss: 0.19119976460933685 - 17.65 s\n",
      "Epoch 1455/3000, 60/60 - D Loss: 1.2022584321440958e-11, G Loss: 0.18958956003189087 - 17.70 s\n",
      "Epoch 1456/3000, 60/60 - D Loss: 4.3507850938270684e-12, G Loss: 0.18901203572750092 - 18.07 s\n",
      "Epoch 1457/3000, 60/60 - D Loss: 3.270777760548172e-12, G Loss: 0.1888190060853958 - 17.50 s\n",
      "Epoch 1458/3000, 60/60 - D Loss: 8.445621073953649e-12, G Loss: 0.19002917408943176 - 17.46 s\n",
      "Epoch 1459/3000, 60/60 - D Loss: 1.252770035709392e-11, G Loss: 0.19668418169021606 - 17.52 s\n",
      "Epoch 1460/3000, 60/60 - D Loss: 5.035349474421414e-12, G Loss: 0.18918223679065704 - 17.66 s\n",
      "Epoch 1461/3000, 60/60 - D Loss: 1.3778667676871933e-11, G Loss: 0.18918070197105408 - 17.69 s\n",
      "Epoch 1462/3000, 60/60 - D Loss: 3.975029957907476e-12, G Loss: 0.1903192102909088 - 17.69 s\n",
      "Epoch 1463/3000, 60/60 - D Loss: 4.282842717481609e-12, G Loss: 0.18873737752437592 - 17.82 s\n",
      "Epoch 1464/3000, 60/60 - D Loss: 9.429587622464617e-12, G Loss: 0.18939904868602753 - 17.42 s\n",
      "Epoch 1465/3000, 60/60 - D Loss: 1.2695417473573311e-11, G Loss: 0.19052283465862274 - 17.97 s\n",
      "Epoch 1466/3000, 60/60 - D Loss: 6.09540630330072e-12, G Loss: 0.18859897553920746 - 17.46 s\n",
      "Epoch 1467/3000, 60/60 - D Loss: 2.1248607681433335e-11, G Loss: 0.19020581245422363 - 17.39 s\n",
      "Epoch 1468/3000, 60/60 - D Loss: 1.7704719099177551e-12, G Loss: 0.189678356051445 - 17.71 s\n",
      "Epoch 1469/3000, 60/60 - D Loss: 1.1359720766809088e-11, G Loss: 0.18939830362796783 - 17.52 s\n",
      "Epoch 1470/3000, 60/60 - D Loss: 4.458008263116166e-12, G Loss: 0.18915587663650513 - 17.87 s\n",
      "Epoch 1471/3000, 60/60 - D Loss: 4.3717271364828126e-12, G Loss: 0.19097678363323212 - 17.64 s\n",
      "Epoch 1472/3000, 60/60 - D Loss: 8.233638852315607e-12, G Loss: 0.1909443885087967 - 17.94 s\n",
      "Epoch 1473/3000, 60/60 - D Loss: 3.0804027757440567e-12, G Loss: 0.19003139436244965 - 17.58 s\n",
      "Epoch 1474/3000, 60/60 - D Loss: 7.346037598629251e-12, G Loss: 0.19606007635593414 - 17.61 s\n",
      "Epoch 1475/3000, 60/60 - D Loss: 2.70750900152492e-12, G Loss: 0.1887776404619217 - 17.90 s\n",
      "Epoch 1476/3000, 60/60 - D Loss: 4.163554587505636e-12, G Loss: 0.18826933205127716 - 17.68 s\n",
      "Epoch 1477/3000, 60/60 - D Loss: 5.817137728064866e-12, G Loss: 0.18999844789505005 - 17.60 s\n",
      "Epoch 1478/3000, 60/60 - D Loss: 6.390328832801045e-12, G Loss: 0.18858279287815094 - 17.51 s\n",
      "Epoch 1479/3000, 60/60 - D Loss: 6.625387926697071e-12, G Loss: 0.19252514839172363 - 17.61 s\n",
      "Epoch 1480/3000, 60/60 - D Loss: 6.8618172998177375e-12, G Loss: 0.18885467946529388 - 18.06 s\n",
      "Epoch 1481/3000, 60/60 - D Loss: 2.861262510103926e-12, G Loss: 0.18994559347629547 - 17.70 s\n",
      "Epoch 1482/3000, 60/60 - D Loss: 5.900907031794794e-12, G Loss: 0.18940575420856476 - 17.50 s\n",
      "Epoch 1483/3000, 60/60 - D Loss: 6.515002782100966e-12, G Loss: 0.18848438560962677 - 17.74 s\n",
      "Epoch 1484/3000, 60/60 - D Loss: 7.318625657081577e-12, G Loss: 0.188588485121727 - 17.61 s\n",
      "Epoch 1485/3000, 60/60 - D Loss: 3.4911853984799506e-12, G Loss: 0.1910838782787323 - 17.79 s\n",
      "Epoch 1486/3000, 60/60 - D Loss: 5.406309063544886e-12, G Loss: 0.18936094641685486 - 17.47 s\n",
      "Epoch 1487/3000, 60/60 - D Loss: 6.057488881741627e-12, G Loss: 0.18890027701854706 - 17.52 s\n",
      "Epoch 1488/3000, 60/60 - D Loss: 2.673217958255851e-12, G Loss: 0.18930751085281372 - 17.52 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1489/3000, 60/60 - D Loss: 1.94762537218709e-11, G Loss: 0.19069334864616394 - 17.80 s\n",
      "Epoch 1490/3000, 60/60 - D Loss: 1.6874237625927155e-11, G Loss: 0.1901773363351822 - 17.83 s\n",
      "Epoch 1491/3000, 60/60 - D Loss: 1.5337208284255615e-11, G Loss: 0.1895289570093155 - 17.56 s\n",
      "Epoch 1492/3000, 60/60 - D Loss: 1.1698499971897581e-11, G Loss: 0.18870945274829865 - 17.62 s\n",
      "Epoch 1493/3000, 60/60 - D Loss: 1.328914638802011e-11, G Loss: 0.18923160433769226 - 17.75 s\n",
      "Epoch 1494/3000, 60/60 - D Loss: 2.2111963288451648e-11, G Loss: 0.18889078497886658 - 17.49 s\n",
      "Epoch 1495/3000, 60/60 - D Loss: 2.5800311572727173e-11, G Loss: 0.19039081037044525 - 17.56 s\n",
      "Epoch 1496/3000, 60/60 - D Loss: 2.97226099363584e-11, G Loss: 0.18957802653312683 - 17.90 s\n",
      "Epoch 1497/3000, 60/60 - D Loss: 3.261131572329512e-11, G Loss: 0.18938593566417694 - 17.46 s\n",
      "Epoch 1498/3000, 60/60 - D Loss: 3.696681694767172e-11, G Loss: 0.1901414692401886 - 17.53 s\n",
      "Epoch 1499/3000, 60/60 - D Loss: 2.2734851645933783e-11, G Loss: 0.1902729719877243 - 17.78 s\n",
      "Epoch 1500/3000, 60/60 - D Loss: 0.0025678498223931424, G Loss: 0.1911826729774475 - 17.58 s\n",
      "Epoch 1501/3000, 60/60 - D Loss: 0.0062784704205114394, G Loss: 0.19057829678058624 - 17.64 s\n",
      "Epoch 1502/3000, 60/60 - D Loss: 0.01353077305975603, G Loss: 0.18928483128547668 - 17.65 s\n",
      "Epoch 1503/3000, 60/60 - D Loss: 0.0012961704269400798, G Loss: 0.18927587568759918 - 17.53 s\n",
      "Epoch 1504/3000, 60/60 - D Loss: 0.0006645294670306612, G Loss: 0.18909050524234772 - 17.77 s\n",
      "Epoch 1505/3000, 60/60 - D Loss: 0.00044881688518216833, G Loss: 0.18877775967121124 - 17.61 s\n",
      "Epoch 1506/3000, 60/60 - D Loss: 0.0003311589389340952, G Loss: 0.1892211139202118 - 17.46 s\n",
      "Epoch 1507/3000, 60/60 - D Loss: 0.00024599270727776457, G Loss: 0.1889391839504242 - 17.74 s\n",
      "Epoch 1508/3000, 60/60 - D Loss: 0.00020435822625586297, G Loss: 0.18841888010501862 - 17.73 s\n",
      "Epoch 1509/3000, 60/60 - D Loss: 0.00015910997990431497, G Loss: 0.18927370011806488 - 17.76 s\n",
      "Epoch 1510/3000, 60/60 - D Loss: 0.00012993165546504315, G Loss: 0.18950563669204712 - 17.72 s\n",
      "Epoch 1511/3000, 60/60 - D Loss: 0.00010673740962374723, G Loss: 0.18928930163383484 - 18.04 s\n",
      "Epoch 1512/3000, 60/60 - D Loss: 9.328360192739638e-05, G Loss: 0.18908563256263733 - 17.72 s\n",
      "Epoch 1513/3000, 60/60 - D Loss: 7.499185949200182e-05, G Loss: 0.18870680034160614 - 17.62 s\n",
      "Epoch 1514/3000, 60/60 - D Loss: 6.572605843757628e-05, G Loss: 0.19152706861495972 - 17.82 s\n",
      "Epoch 1515/3000, 60/60 - D Loss: 5.83505002396123e-05, G Loss: 0.18967868387699127 - 17.76 s\n",
      "Epoch 1516/3000, 60/60 - D Loss: 5.084695521873073e-05, G Loss: 0.1890355795621872 - 17.72 s\n",
      "Epoch 1517/3000, 60/60 - D Loss: 4.616927708411822e-05, G Loss: 0.18921688199043274 - 17.70 s\n",
      "Epoch 1518/3000, 60/60 - D Loss: 4.057301248394651e-05, G Loss: 0.18901759386062622 - 17.84 s\n",
      "Epoch 1519/3000, 60/60 - D Loss: 3.575269624889188e-05, G Loss: 0.19013288617134094 - 17.77 s\n",
      "Epoch 1520/3000, 60/60 - D Loss: 3.2323229333997006e-05, G Loss: 0.18944603204727173 - 17.80 s\n",
      "Epoch 1521/3000, 60/60 - D Loss: 3.0454735679086298e-05, G Loss: 0.19078610837459564 - 17.70 s\n",
      "Epoch 1522/3000, 60/60 - D Loss: 2.7007948574464535e-05, G Loss: 0.18970778584480286 - 17.88 s\n",
      "Epoch 1523/3000, 60/60 - D Loss: 2.480662362813746e-05, G Loss: 0.18883433938026428 - 17.67 s\n",
      "Epoch 1524/3000, 60/60 - D Loss: 2.324683350707346e-05, G Loss: 0.18990157544612885 - 17.45 s\n",
      "Epoch 1525/3000, 60/60 - D Loss: 2.047947964456398e-05, G Loss: 0.1903746873140335 - 17.66 s\n",
      "Epoch 1526/3000, 60/60 - D Loss: 1.929238447928583e-05, G Loss: 0.1894248127937317 - 18.00 s\n",
      "Epoch 1527/3000, 60/60 - D Loss: 1.6214594211305666e-05, G Loss: 0.19000378251075745 - 17.54 s\n",
      "Epoch 1528/3000, 60/60 - D Loss: 1.4978814363075799e-05, G Loss: 0.1888173371553421 - 17.84 s\n",
      "Epoch 1529/3000, 60/60 - D Loss: 1.4119171908077988e-05, G Loss: 0.1904086023569107 - 17.90 s\n",
      "Epoch 1530/3000, 60/60 - D Loss: 1.3075050731004012e-05, G Loss: 0.19008293747901917 - 17.32 s\n",
      "Epoch 1531/3000, 60/60 - D Loss: 1.3257798912036378e-05, G Loss: 0.18852591514587402 - 17.66 s\n",
      "Epoch 1532/3000, 60/60 - D Loss: 1.2346577591415553e-05, G Loss: 0.18971513211727142 - 17.69 s\n",
      "Epoch 1533/3000, 60/60 - D Loss: 1.0948068791094556e-05, G Loss: 0.18841877579689026 - 17.96 s\n",
      "Epoch 1534/3000, 60/60 - D Loss: 1.0379989618058971e-05, G Loss: 0.18982502818107605 - 17.51 s\n",
      "Epoch 1535/3000, 60/60 - D Loss: 9.355904438734797e-06, G Loss: 0.18979957699775696 - 17.70 s\n",
      "Epoch 1536/3000, 60/60 - D Loss: 9.006689793977785e-06, G Loss: 0.19075755774974823 - 17.66 s\n",
      "Epoch 1537/3000, 60/60 - D Loss: 7.900015432937835e-06, G Loss: 0.19056470692157745 - 17.68 s\n",
      "Epoch 1538/3000, 60/60 - D Loss: 7.243622889063772e-06, G Loss: 0.1903446465730667 - 17.72 s\n",
      "Epoch 1539/3000, 60/60 - D Loss: 6.6844857258274715e-06, G Loss: 0.1905643194913864 - 17.67 s\n",
      "Epoch 1540/3000, 60/60 - D Loss: 6.474886049545603e-06, G Loss: 0.18826130032539368 - 17.64 s\n",
      "Epoch 1541/3000, 60/60 - D Loss: 6.383717391145183e-06, G Loss: 0.1898491531610489 - 17.82 s\n",
      "Epoch 1542/3000, 60/60 - D Loss: 5.8338220156883835e-06, G Loss: 0.19001711905002594 - 17.48 s\n",
      "Epoch 1543/3000, 60/60 - D Loss: 5.2788221367450205e-06, G Loss: 0.18903809785842896 - 17.61 s\n",
      "Epoch 1544/3000, 60/60 - D Loss: 5.003846247575439e-06, G Loss: 0.18943138420581818 - 17.57 s\n",
      "Epoch 1545/3000, 60/60 - D Loss: 4.591447371637969e-06, G Loss: 0.18931716680526733 - 17.75 s\n",
      "Epoch 1546/3000, 60/60 - D Loss: 4.191631937544571e-06, G Loss: 0.19015048444271088 - 17.61 s\n",
      "Epoch 1547/3000, 60/60 - D Loss: 3.941360986914333e-06, G Loss: 0.1869383603334427 - 17.84 s\n",
      "Epoch 1548/3000, 60/60 - D Loss: 3.831982965607494e-06, G Loss: 0.19153867661952972 - 17.78 s\n",
      "Epoch 1549/3000, 60/60 - D Loss: 3.4613275161632373e-06, G Loss: 0.18859092891216278 - 17.73 s\n",
      "Epoch 1550/3000, 60/60 - D Loss: 3.3694248813276317e-06, G Loss: 0.18966126441955566 - 17.81 s\n",
      "Epoch 1551/3000, 60/60 - D Loss: 2.916764703542185e-06, G Loss: 0.1891476958990097 - 17.61 s\n",
      "Epoch 1552/3000, 60/60 - D Loss: 2.7388919292548053e-06, G Loss: 0.19002580642700195 - 17.87 s\n",
      "Epoch 1553/3000, 60/60 - D Loss: 2.5489199693140563e-06, G Loss: 0.18974372744560242 - 17.62 s\n",
      "Epoch 1554/3000, 60/60 - D Loss: 2.350550264651474e-06, G Loss: 0.18902453780174255 - 17.78 s\n",
      "Epoch 1555/3000, 60/60 - D Loss: 2.155116467505991e-06, G Loss: 0.19163799285888672 - 17.90 s\n",
      "Epoch 1556/3000, 60/60 - D Loss: 1.998361367938628e-06, G Loss: 0.18914097547531128 - 17.95 s\n",
      "Epoch 1557/3000, 60/60 - D Loss: 1.8347131582885368e-06, G Loss: 0.18976224958896637 - 17.87 s\n",
      "Epoch 1558/3000, 60/60 - D Loss: 1.6325148637363895e-06, G Loss: 0.1890718787908554 - 17.56 s\n",
      "Epoch 1559/3000, 60/60 - D Loss: 1.5839646847481959e-06, G Loss: 0.18837285041809082 - 17.41 s\n",
      "Epoch 1560/3000, 60/60 - D Loss: 1.4337924145735315e-06, G Loss: 0.1888711154460907 - 17.73 s\n",
      "Epoch 1561/3000, 60/60 - D Loss: 1.2815147565348184e-06, G Loss: 0.18975505232810974 - 17.73 s\n",
      "Epoch 1562/3000, 60/60 - D Loss: 1.25766426029017e-06, G Loss: 0.18906879425048828 - 17.61 s\n",
      "Epoch 1563/3000, 60/60 - D Loss: 1.1027083535175564e-06, G Loss: 0.18883906304836273 - 17.65 s\n",
      "Epoch 1564/3000, 60/60 - D Loss: 9.490829993019645e-07, G Loss: 0.19040514528751373 - 17.71 s\n",
      "Epoch 1565/3000, 60/60 - D Loss: 8.020171882172633e-07, G Loss: 0.18848244845867157 - 17.78 s\n",
      "Epoch 1566/3000, 60/60 - D Loss: 8.743735881999726e-07, G Loss: 0.19024845957756042 - 17.65 s\n",
      "Epoch 1567/3000, 60/60 - D Loss: 8.246795930588746e-07, G Loss: 0.18852989375591278 - 17.70 s\n",
      "Epoch 1568/3000, 60/60 - D Loss: 7.805512871605913e-07, G Loss: 0.18910931050777435 - 17.69 s\n",
      "Epoch 1569/3000, 60/60 - D Loss: 7.293368058380878e-07, G Loss: 0.1898035705089569 - 17.75 s\n",
      "Epoch 1570/3000, 60/60 - D Loss: 6.477586904640731e-07, G Loss: 0.19368107616901398 - 17.68 s\n",
      "Epoch 1571/3000, 60/60 - D Loss: 6.059951245251938e-07, G Loss: 0.1903480589389801 - 18.03 s\n",
      "Epoch 1572/3000, 60/60 - D Loss: 5.362756589910589e-07, G Loss: 0.1885211020708084 - 17.77 s\n",
      "Epoch 1573/3000, 60/60 - D Loss: 5.097077981908171e-07, G Loss: 0.18907557427883148 - 17.79 s\n",
      "Epoch 1574/3000, 60/60 - D Loss: 5.170127587161844e-07, G Loss: 0.1871364414691925 - 17.81 s\n",
      "Epoch 1575/3000, 60/60 - D Loss: 4.7604690378566517e-07, G Loss: 0.18996502459049225 - 17.59 s\n",
      "Epoch 1576/3000, 60/60 - D Loss: 4.326305632629257e-07, G Loss: 0.18994535505771637 - 17.69 s\n",
      "Epoch 1577/3000, 60/60 - D Loss: 4.379903576490207e-07, G Loss: 0.18947987258434296 - 17.85 s\n",
      "Epoch 1578/3000, 60/60 - D Loss: 3.6699337102596985e-07, G Loss: 0.18842270970344543 - 17.71 s\n",
      "Epoch 1579/3000, 60/60 - D Loss: 3.347618174887046e-07, G Loss: 0.19047895073890686 - 17.78 s\n",
      "Epoch 1580/3000, 60/60 - D Loss: 3.4304667023699054e-07, G Loss: 0.18879808485507965 - 17.56 s\n",
      "Epoch 1581/3000, 60/60 - D Loss: 3.1109378034743784e-07, G Loss: 0.1893049031496048 - 17.62 s\n",
      "Epoch 1582/3000, 60/60 - D Loss: 2.6454948187648597e-07, G Loss: 0.18885685503482819 - 17.79 s\n",
      "Epoch 1583/3000, 60/60 - D Loss: 2.6855838014538946e-07, G Loss: 0.19086982309818268 - 17.71 s\n",
      "Epoch 1584/3000, 60/60 - D Loss: 2.651873645398428e-07, G Loss: 0.18921415507793427 - 17.76 s\n",
      "Epoch 1585/3000, 60/60 - D Loss: 2.132134127030838e-07, G Loss: 0.18945781886577606 - 17.80 s\n",
      "Epoch 1586/3000, 60/60 - D Loss: 2.047309562991373e-07, G Loss: 0.189104825258255 - 17.89 s\n",
      "Epoch 1587/3000, 60/60 - D Loss: 1.8319648784070885e-07, G Loss: 0.18946006894111633 - 17.01 s\n",
      "Epoch 1588/3000, 60/60 - D Loss: 1.784954360706248e-07, G Loss: 0.18946148455142975 - 17.14 s\n",
      "Epoch 1589/3000, 60/60 - D Loss: 1.601507324255902e-07, G Loss: 0.19018130004405975 - 17.15 s\n",
      "Epoch 1590/3000, 60/60 - D Loss: 1.5685825821565302e-07, G Loss: 0.19009505212306976 - 17.28 s\n",
      "Epoch 1591/3000, 60/60 - D Loss: 1.390978370124918e-07, G Loss: 0.18956252932548523 - 17.31 s\n",
      "Epoch 1592/3000, 60/60 - D Loss: 1.2762475447292143e-07, G Loss: 0.18867076933383942 - 17.19 s\n",
      "Epoch 1593/3000, 60/60 - D Loss: 1.0433568012335748e-07, G Loss: 0.18921801447868347 - 17.20 s\n",
      "Epoch 1594/3000, 60/60 - D Loss: 1.0084485758954376e-07, G Loss: 0.18992574512958527 - 17.11 s\n",
      "Epoch 1595/3000, 60/60 - D Loss: 9.527700153139085e-08, G Loss: 0.18836405873298645 - 17.58 s\n",
      "Epoch 1596/3000, 60/60 - D Loss: 1.0825275305326176e-07, G Loss: 0.18933917582035065 - 17.43 s\n",
      "Epoch 1597/3000, 60/60 - D Loss: 8.896355605664263e-08, G Loss: 0.18915118277072906 - 17.40 s\n",
      "Epoch 1598/3000, 60/60 - D Loss: 7.97326762391748e-08, G Loss: 0.18873241543769836 - 17.18 s\n",
      "Epoch 1599/3000, 60/60 - D Loss: 7.32552077400972e-08, G Loss: 0.18817907571792603 - 15.77 s\n",
      "Epoch 1600/3000, 60/60 - D Loss: 8.622378058070757e-08, G Loss: 0.1892727017402649 - 17.33 s\n",
      "Epoch 1601/3000, 60/60 - D Loss: 6.410187264327405e-08, G Loss: 0.19072875380516052 - 17.46 s\n",
      "Epoch 1602/3000, 60/60 - D Loss: 5.5423832592826905e-08, G Loss: 0.18937072157859802 - 16.94 s\n",
      "Epoch 1603/3000, 60/60 - D Loss: 5.469182849660026e-08, G Loss: 0.18943001329898834 - 11.82 s\n",
      "Epoch 1604/3000, 60/60 - D Loss: 6.319751805552998e-08, G Loss: 0.19329799711704254 - 16.91 s\n",
      "Epoch 1605/3000, 60/60 - D Loss: 5.401387652079204e-08, G Loss: 0.19195818901062012 - 17.44 s\n",
      "Epoch 1606/3000, 60/60 - D Loss: 6.150560329098997e-08, G Loss: 0.18935248255729675 - 18.20 s\n",
      "Epoch 1607/3000, 60/60 - D Loss: 4.6562613407324926e-08, G Loss: 0.18896566331386566 - 17.83 s\n",
      "Epoch 1608/3000, 60/60 - D Loss: 4.767540136285664e-08, G Loss: 0.1915625035762787 - 17.55 s\n",
      "Epoch 1609/3000, 60/60 - D Loss: 4.510401054990537e-08, G Loss: 0.1893293559551239 - 17.50 s\n",
      "Epoch 1610/3000, 60/60 - D Loss: 3.881945274605836e-08, G Loss: 0.19025853276252747 - 17.64 s\n",
      "Epoch 1611/3000, 60/60 - D Loss: 3.5968566874933217e-08, G Loss: 0.19211342930793762 - 17.49 s\n",
      "Epoch 1612/3000, 60/60 - D Loss: 3.578061011713651e-08, G Loss: 0.1890251338481903 - 17.74 s\n",
      "Epoch 1613/3000, 60/60 - D Loss: 3.836418488879084e-08, G Loss: 0.18964579701423645 - 17.80 s\n",
      "Epoch 1614/3000, 60/60 - D Loss: 3.3323078085095714e-08, G Loss: 0.19117183983325958 - 17.54 s\n",
      "Epoch 1615/3000, 60/60 - D Loss: 3.132112942096088e-08, G Loss: 0.18929629027843475 - 17.47 s\n",
      "Epoch 1616/3000, 60/60 - D Loss: 3.2406192967786296e-08, G Loss: 0.18793454766273499 - 17.68 s\n",
      "Epoch 1617/3000, 60/60 - D Loss: 2.6651232723322316e-08, G Loss: 0.19055558741092682 - 17.72 s\n",
      "Epoch 1618/3000, 60/60 - D Loss: 2.5781921516969397e-08, G Loss: 0.1896798461675644 - 17.51 s\n",
      "Epoch 1619/3000, 60/60 - D Loss: 2.498122574513123e-08, G Loss: 0.18950143456459045 - 17.66 s\n",
      "Epoch 1620/3000, 60/60 - D Loss: 2.8160166036256956e-08, G Loss: 0.18875810503959656 - 17.77 s\n",
      "Epoch 1621/3000, 60/60 - D Loss: 2.13442695440845e-08, G Loss: 0.18891774117946625 - 17.65 s\n",
      "Epoch 1622/3000, 60/60 - D Loss: 1.8937252396636697e-08, G Loss: 0.19016772508621216 - 17.75 s\n",
      "Epoch 1623/3000, 60/60 - D Loss: 2.11731406574156e-08, G Loss: 0.19074387848377228 - 17.79 s\n",
      "Epoch 1624/3000, 60/60 - D Loss: 2.2189631220794967e-08, G Loss: 0.18860667943954468 - 17.72 s\n",
      "Epoch 1625/3000, 60/60 - D Loss: 1.8051789271288782e-08, G Loss: 0.18903380632400513 - 17.52 s\n",
      "Epoch 1626/3000, 60/60 - D Loss: 1.5293542195886703e-08, G Loss: 0.18999174237251282 - 17.50 s\n",
      "Epoch 1627/3000, 60/60 - D Loss: 1.4640544966247268e-08, G Loss: 0.1894630342721939 - 17.70 s\n",
      "Epoch 1628/3000, 60/60 - D Loss: 1.5474061780844026e-08, G Loss: 0.19320517778396606 - 17.46 s\n",
      "Epoch 1629/3000, 60/60 - D Loss: 1.7547005273701316e-08, G Loss: 0.18834726512432098 - 17.65 s\n",
      "Epoch 1630/3000, 60/60 - D Loss: 1.3648373498867036e-08, G Loss: 0.19104471802711487 - 17.86 s\n",
      "Epoch 1631/3000, 60/60 - D Loss: 1.3422691788351343e-08, G Loss: 0.18831519782543182 - 17.95 s\n",
      "Epoch 1632/3000, 60/60 - D Loss: 1.346872878016988e-08, G Loss: 0.1886942833662033 - 17.96 s\n",
      "Epoch 1633/3000, 60/60 - D Loss: 1.1659074129538247e-08, G Loss: 0.1882619559764862 - 17.80 s\n",
      "Epoch 1634/3000, 60/60 - D Loss: 1.0810739968698281e-08, G Loss: 0.18802489340305328 - 17.70 s\n",
      "Epoch 1635/3000, 60/60 - D Loss: 1.039268276493456e-08, G Loss: 0.19381070137023926 - 17.66 s\n",
      "Epoch 1636/3000, 60/60 - D Loss: 8.79614152381799e-09, G Loss: 0.19007247686386108 - 17.77 s\n",
      "Epoch 1637/3000, 60/60 - D Loss: 8.959760458841955e-09, G Loss: 0.19335296750068665 - 17.60 s\n",
      "Epoch 1638/3000, 60/60 - D Loss: 7.913968061554113e-09, G Loss: 0.18974314630031586 - 17.79 s\n",
      "Epoch 1639/3000, 60/60 - D Loss: 7.537647975729665e-09, G Loss: 0.1895224153995514 - 17.54 s\n",
      "Epoch 1640/3000, 60/60 - D Loss: 9.305858294020308e-09, G Loss: 0.1889074742794037 - 17.82 s\n",
      "Epoch 1641/3000, 60/60 - D Loss: 7.166324975604009e-09, G Loss: 0.18883642554283142 - 17.80 s\n",
      "Epoch 1642/3000, 60/60 - D Loss: 7.74008440673768e-09, G Loss: 0.1917491853237152 - 17.63 s\n",
      "Epoch 1643/3000, 60/60 - D Loss: 6.17713199390019e-09, G Loss: 0.18998172879219055 - 17.98 s\n",
      "Epoch 1644/3000, 60/60 - D Loss: 5.841497978257074e-09, G Loss: 0.1891474425792694 - 17.77 s\n",
      "Epoch 1645/3000, 60/60 - D Loss: 5.258607187229921e-09, G Loss: 0.189558744430542 - 17.73 s\n",
      "Epoch 1646/3000, 60/60 - D Loss: 5.196733986981717e-09, G Loss: 0.18990203738212585 - 17.70 s\n",
      "Epoch 1647/3000, 60/60 - D Loss: 5.630939248218872e-09, G Loss: 0.18870404362678528 - 17.99 s\n",
      "Epoch 1648/3000, 60/60 - D Loss: 5.939922039232019e-09, G Loss: 0.18963390588760376 - 17.70 s\n",
      "Epoch 1649/3000, 60/60 - D Loss: 4.273304703150682e-09, G Loss: 0.189078226685524 - 17.78 s\n",
      "Epoch 1650/3000, 60/60 - D Loss: 3.968526018872584e-09, G Loss: 0.18890555202960968 - 17.75 s\n",
      "Epoch 1651/3000, 60/60 - D Loss: 3.942941894101611e-09, G Loss: 0.1887841522693634 - 18.13 s\n",
      "Epoch 1652/3000, 60/60 - D Loss: 3.5284429979577994e-09, G Loss: 0.18921470642089844 - 17.65 s\n",
      "Epoch 1653/3000, 60/60 - D Loss: 3.2182363956904936e-09, G Loss: 0.189213365316391 - 17.76 s\n",
      "Epoch 1654/3000, 60/60 - D Loss: 3.1696560668032295e-09, G Loss: 0.18977096676826477 - 17.83 s\n",
      "Epoch 1655/3000, 60/60 - D Loss: 3.806237943966909e-09, G Loss: 0.18887804448604584 - 17.65 s\n",
      "Epoch 1656/3000, 60/60 - D Loss: 2.620043622542697e-09, G Loss: 0.1897132694721222 - 17.54 s\n",
      "Epoch 1657/3000, 60/60 - D Loss: 2.2222266466754605e-09, G Loss: 0.1890231966972351 - 17.57 s\n",
      "Epoch 1658/3000, 60/60 - D Loss: 2.86955012976744e-09, G Loss: 0.18842624127864838 - 17.82 s\n",
      "Epoch 1659/3000, 60/60 - D Loss: 2.1086576406016038e-09, G Loss: 0.18898393213748932 - 17.60 s\n",
      "Epoch 1660/3000, 60/60 - D Loss: 2.1901275605910007e-09, G Loss: 0.19288605451583862 - 18.01 s\n",
      "Epoch 1661/3000, 60/60 - D Loss: 2.313904973985821e-09, G Loss: 0.18916873633861542 - 17.79 s\n",
      "Epoch 1662/3000, 60/60 - D Loss: 2.1028208451313694e-09, G Loss: 0.19119705259799957 - 17.57 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1663/3000, 60/60 - D Loss: 2.0207862551542295e-09, G Loss: 0.18871252238750458 - 17.89 s\n",
      "Epoch 1664/3000, 60/60 - D Loss: 2.2169441661067687e-09, G Loss: 0.18927237391471863 - 17.72 s\n",
      "Epoch 1665/3000, 60/60 - D Loss: 1.985409066196527e-09, G Loss: 0.19201333820819855 - 17.71 s\n",
      "Epoch 1666/3000, 60/60 - D Loss: 1.5565274297197956e-09, G Loss: 0.18860043585300446 - 17.73 s\n",
      "Epoch 1667/3000, 60/60 - D Loss: 1.881186524859812e-09, G Loss: 0.18880192935466766 - 17.82 s\n",
      "Epoch 1668/3000, 60/60 - D Loss: 1.2637366168170676e-09, G Loss: 0.18956203758716583 - 17.70 s\n",
      "Epoch 1669/3000, 60/60 - D Loss: 1.7070571692922383e-09, G Loss: 0.18896976113319397 - 17.97 s\n",
      "Epoch 1670/3000, 60/60 - D Loss: 1.4871144277960632e-09, G Loss: 0.1897495836019516 - 18.12 s\n",
      "Epoch 1671/3000, 60/60 - D Loss: 1.3785296805706847e-09, G Loss: 0.19246989488601685 - 17.78 s\n",
      "Epoch 1672/3000, 60/60 - D Loss: 1.388962950216047e-09, G Loss: 0.1879955679178238 - 17.70 s\n",
      "Epoch 1673/3000, 60/60 - D Loss: 1.4963724121830422e-09, G Loss: 0.18952399492263794 - 17.68 s\n",
      "Epoch 1674/3000, 60/60 - D Loss: 1.0267973568297596e-09, G Loss: 0.18844075500965118 - 17.65 s\n",
      "Epoch 1675/3000, 60/60 - D Loss: 1.5631092221225556e-09, G Loss: 0.19124580919742584 - 17.53 s\n",
      "Epoch 1676/3000, 60/60 - D Loss: 1.2216162129184e-09, G Loss: 0.18918611109256744 - 17.40 s\n",
      "Epoch 1677/3000, 60/60 - D Loss: 1.2136249206455758e-09, G Loss: 0.18930958211421967 - 17.06 s\n",
      "Epoch 1678/3000, 60/60 - D Loss: 1.0327508704043595e-09, G Loss: 0.18883201479911804 - 17.49 s\n",
      "Epoch 1679/3000, 60/60 - D Loss: 9.961213876051155e-10, G Loss: 0.18936997652053833 - 17.06 s\n",
      "Epoch 1680/3000, 60/60 - D Loss: 1.0673037971947524e-09, G Loss: 0.18768908083438873 - 17.16 s\n",
      "Epoch 1681/3000, 60/60 - D Loss: 1.0273377028366306e-09, G Loss: 0.18865683674812317 - 17.02 s\n",
      "Epoch 1682/3000, 60/60 - D Loss: 8.743118141095723e-10, G Loss: 0.1912459433078766 - 17.07 s\n",
      "Epoch 1683/3000, 60/60 - D Loss: 9.635101015648129e-10, G Loss: 0.1895204335451126 - 17.13 s\n",
      "Epoch 1684/3000, 60/60 - D Loss: 8.375385938171748e-10, G Loss: 0.18929477035999298 - 17.05 s\n",
      "Epoch 1685/3000, 60/60 - D Loss: 7.269919520980911e-10, G Loss: 0.18881747126579285 - 17.17 s\n",
      "Epoch 1686/3000, 60/60 - D Loss: 7.669681717864204e-10, G Loss: 0.18934054672718048 - 17.02 s\n",
      "Epoch 1687/3000, 60/60 - D Loss: 8.052899703678024e-10, G Loss: 0.1890726238489151 - 17.46 s\n",
      "Epoch 1688/3000, 60/60 - D Loss: 6.152571907215533e-10, G Loss: 0.18879252672195435 - 17.79 s\n",
      "Epoch 1689/3000, 60/60 - D Loss: 8.060308570221296e-10, G Loss: 0.18903498351573944 - 17.08 s\n",
      "Epoch 1690/3000, 60/60 - D Loss: 6.526976229187086e-10, G Loss: 0.18938495218753815 - 17.00 s\n",
      "Epoch 1691/3000, 60/60 - D Loss: 6.575929018437683e-10, G Loss: 0.18887019157409668 - 17.17 s\n",
      "Epoch 1692/3000, 60/60 - D Loss: 7.201705638223499e-10, G Loss: 0.18885456025600433 - 17.10 s\n",
      "Epoch 1693/3000, 60/60 - D Loss: 4.951709213290526e-10, G Loss: 0.18865236639976501 - 17.21 s\n",
      "Epoch 1694/3000, 60/60 - D Loss: 6.523767301109401e-10, G Loss: 0.18972192704677582 - 17.18 s\n",
      "Epoch 1695/3000, 60/60 - D Loss: 5.656085147023372e-10, G Loss: 0.18882180750370026 - 17.08 s\n",
      "Epoch 1696/3000, 60/60 - D Loss: 4.877074463277261e-10, G Loss: 0.18931527435779572 - 17.30 s\n",
      "Epoch 1697/3000, 60/60 - D Loss: 4.3894686339472286e-10, G Loss: 0.18844106793403625 - 17.63 s\n",
      "Epoch 1698/3000, 60/60 - D Loss: 4.485673890639616e-10, G Loss: 0.19004949927330017 - 17.63 s\n",
      "Epoch 1699/3000, 60/60 - D Loss: 4.183935451242994e-10, G Loss: 0.1916935294866562 - 17.67 s\n",
      "Epoch 1700/3000, 60/60 - D Loss: 4.2353580348821265e-10, G Loss: 0.19046913087368011 - 17.71 s\n",
      "Epoch 1701/3000, 60/60 - D Loss: 4.217466571560403e-10, G Loss: 0.1885865330696106 - 17.70 s\n",
      "Epoch 1702/3000, 60/60 - D Loss: 4.611554554462803e-10, G Loss: 0.19092807173728943 - 17.76 s\n",
      "Epoch 1703/3000, 60/60 - D Loss: 4.054420415055388e-10, G Loss: 0.1889316737651825 - 17.84 s\n",
      "Epoch 1704/3000, 60/60 - D Loss: 4.125367752280348e-10, G Loss: 0.1895315945148468 - 17.56 s\n",
      "Epoch 1705/3000, 60/60 - D Loss: 3.5155196273186544e-10, G Loss: 0.18865059316158295 - 17.68 s\n",
      "Epoch 1706/3000, 60/60 - D Loss: 4.3095344224011105e-10, G Loss: 0.19024299085140228 - 17.83 s\n",
      "Epoch 1707/3000, 60/60 - D Loss: 3.8464232591118447e-10, G Loss: 0.18954871594905853 - 17.70 s\n",
      "Epoch 1708/3000, 60/60 - D Loss: 3.422361331367708e-10, G Loss: 0.18821315467357635 - 17.89 s\n",
      "Epoch 1709/3000, 60/60 - D Loss: 3.3630147610122247e-10, G Loss: 0.18891943991184235 - 17.67 s\n",
      "Epoch 1710/3000, 60/60 - D Loss: 3.587681191355701e-10, G Loss: 0.1902085244655609 - 17.72 s\n",
      "Epoch 1711/3000, 60/60 - D Loss: 3.404902114379979e-10, G Loss: 0.18929071724414825 - 17.51 s\n",
      "Epoch 1712/3000, 60/60 - D Loss: 3.53049112968224e-10, G Loss: 0.1903093457221985 - 17.77 s\n",
      "Epoch 1713/3000, 60/60 - D Loss: 3.3316234085837876e-10, G Loss: 0.18916669487953186 - 17.68 s\n",
      "Epoch 1714/3000, 60/60 - D Loss: 2.899000311860781e-10, G Loss: 0.18895107507705688 - 17.78 s\n",
      "Epoch 1715/3000, 60/60 - D Loss: 3.7272678497463974e-10, G Loss: 0.18949022889137268 - 17.83 s\n",
      "Epoch 1716/3000, 60/60 - D Loss: 2.7391324802685923e-10, G Loss: 0.18752560019493103 - 17.92 s\n",
      "Epoch 1717/3000, 60/60 - D Loss: 2.5740997205903526e-10, G Loss: 0.1899694800376892 - 17.50 s\n",
      "Epoch 1718/3000, 60/60 - D Loss: 2.854816068296269e-10, G Loss: 0.1890457719564438 - 17.56 s\n",
      "Epoch 1719/3000, 60/60 - D Loss: 2.601980376053584e-10, G Loss: 0.1893940269947052 - 17.67 s\n",
      "Epoch 1720/3000, 60/60 - D Loss: 2.530645544904415e-10, G Loss: 0.1879950314760208 - 17.74 s\n",
      "Epoch 1721/3000, 60/60 - D Loss: 2.658164941280036e-10, G Loss: 0.18849511444568634 - 17.70 s\n",
      "Epoch 1722/3000, 60/60 - D Loss: 2.77548750516189e-10, G Loss: 0.18867817521095276 - 17.68 s\n",
      "Epoch 1723/3000, 60/60 - D Loss: 2.4942860702082325e-10, G Loss: 0.18921653926372528 - 18.05 s\n",
      "Epoch 1724/3000, 60/60 - D Loss: 2.5723966063764993e-10, G Loss: 0.19253835082054138 - 17.74 s\n",
      "Epoch 1725/3000, 60/60 - D Loss: 2.5397991604395086e-10, G Loss: 0.18929742276668549 - 17.64 s\n",
      "Epoch 1726/3000, 60/60 - D Loss: 2.5930441051747605e-10, G Loss: 0.18841442465782166 - 17.64 s\n",
      "Epoch 1727/3000, 60/60 - D Loss: 2.3256807151075072e-10, G Loss: 0.1888481229543686 - 17.64 s\n",
      "Epoch 1728/3000, 60/60 - D Loss: 2.3923643546306815e-10, G Loss: 0.18877385556697845 - 17.69 s\n",
      "Epoch 1729/3000, 60/60 - D Loss: 2.5569211661231645e-10, G Loss: 0.18988214433193207 - 17.78 s\n",
      "Epoch 1730/3000, 60/60 - D Loss: 2.1969707539884692e-10, G Loss: 0.1887003630399704 - 17.72 s\n",
      "Epoch 1731/3000, 60/60 - D Loss: 2.48460890177885e-10, G Loss: 0.18841056525707245 - 17.47 s\n",
      "Epoch 1732/3000, 60/60 - D Loss: 2.564025237635127e-10, G Loss: 0.18995828926563263 - 17.69 s\n",
      "Epoch 1733/3000, 60/60 - D Loss: 2.5948216377975355e-10, G Loss: 0.1947440356016159 - 17.72 s\n",
      "Epoch 1734/3000, 60/60 - D Loss: 2.644080134825205e-10, G Loss: 0.18867725133895874 - 17.86 s\n",
      "Epoch 1735/3000, 60/60 - D Loss: 4.0603788659596265e-10, G Loss: 0.18908575177192688 - 17.74 s\n",
      "Epoch 1736/3000, 60/60 - D Loss: 2.2839138389627858e-10, G Loss: 0.1889376938343048 - 17.68 s\n",
      "Epoch 1737/3000, 60/60 - D Loss: 4.3616388926518985e-10, G Loss: 0.1897631287574768 - 17.75 s\n",
      "Epoch 1738/3000, 60/60 - D Loss: 2.606208428609951e-10, G Loss: 0.18863454461097717 - 17.80 s\n",
      "Epoch 1739/3000, 60/60 - D Loss: 3.4392326640880744e-10, G Loss: 0.18890808522701263 - 17.82 s\n",
      "Epoch 1740/3000, 60/60 - D Loss: 2.8280671594561315e-10, G Loss: 0.18949173390865326 - 17.13 s\n",
      "Epoch 1741/3000, 60/60 - D Loss: 2.358770296275516e-10, G Loss: 0.1884681135416031 - 17.67 s\n",
      "Epoch 1742/3000, 60/60 - D Loss: 3.954477548504887e-10, G Loss: 0.1884632110595703 - 17.78 s\n",
      "Epoch 1743/3000, 60/60 - D Loss: 2.7295607279694076e-10, G Loss: 0.1890418380498886 - 17.77 s\n",
      "Epoch 1744/3000, 60/60 - D Loss: 2.914614493798476e-10, G Loss: 0.18890060484409332 - 17.87 s\n",
      "Epoch 1745/3000, 60/60 - D Loss: 2.651272537042019e-10, G Loss: 0.18824449181556702 - 17.77 s\n",
      "Epoch 1746/3000, 60/60 - D Loss: 3.8469422697496024e-10, G Loss: 0.19032792747020721 - 17.71 s\n",
      "Epoch 1747/3000, 60/60 - D Loss: 4.490379603819779e-10, G Loss: 0.18924814462661743 - 17.60 s\n",
      "Epoch 1748/3000, 60/60 - D Loss: 2.742028116507246e-10, G Loss: 0.18916350603103638 - 17.70 s\n",
      "Epoch 1749/3000, 60/60 - D Loss: 4.2691176934089086e-10, G Loss: 0.1894320398569107 - 17.82 s\n",
      "Epoch 1750/3000, 60/60 - D Loss: 2.9674859448444095e-10, G Loss: 0.19130347669124603 - 17.69 s\n",
      "Epoch 1751/3000, 60/60 - D Loss: 2.791183007344719e-10, G Loss: 0.1885838806629181 - 17.78 s\n",
      "Epoch 1752/3000, 60/60 - D Loss: 2.4313754301218094e-10, G Loss: 0.19169574975967407 - 17.65 s\n",
      "Epoch 1753/3000, 60/60 - D Loss: 4.067487098156718e-10, G Loss: 0.18841342628002167 - 17.62 s\n",
      "Epoch 1754/3000, 60/60 - D Loss: 4.388669143066185e-10, G Loss: 0.18890012800693512 - 17.86 s\n",
      "Epoch 1755/3000, 60/60 - D Loss: 4.898771465882136e-10, G Loss: 0.19002996385097504 - 17.59 s\n",
      "Epoch 1756/3000, 60/60 - D Loss: 2.7692128277882706e-10, G Loss: 0.1885564923286438 - 17.57 s\n",
      "Epoch 1757/3000, 60/60 - D Loss: 4.1920782067194037e-10, G Loss: 0.1893789917230606 - 17.67 s\n",
      "Epoch 1758/3000, 60/60 - D Loss: 4.23380718481012e-10, G Loss: 0.1894484907388687 - 17.64 s\n",
      "Epoch 1759/3000, 60/60 - D Loss: 9.301264485112276e-10, G Loss: 0.1912161260843277 - 17.86 s\n",
      "Epoch 1760/3000, 60/60 - D Loss: 12.200956344604501, G Loss: 0.18953193724155426 - 17.60 s\n",
      "Epoch 1761/3000, 60/60 - D Loss: 0.6549152575495327, G Loss: 0.18896153569221497 - 17.72 s\n",
      "Epoch 1762/3000, 60/60 - D Loss: 0.0001229242569706912, G Loss: 0.18889257311820984 - 17.71 s\n",
      "Epoch 1763/3000, 60/60 - D Loss: 0.00021787327693800762, G Loss: 0.18910206854343414 - 17.83 s\n",
      "Epoch 1764/3000, 60/60 - D Loss: 0.0002690300623271469, G Loss: 0.19148901104927063 - 17.58 s\n",
      "Epoch 1765/3000, 60/60 - D Loss: 0.0002979933132394308, G Loss: 0.19069570302963257 - 17.43 s\n",
      "Epoch 1766/3000, 60/60 - D Loss: 0.00030018082119909195, G Loss: 0.1892854869365692 - 17.67 s\n",
      "Epoch 1767/3000, 60/60 - D Loss: 0.0002971981049313399, G Loss: 0.1886950135231018 - 17.61 s\n",
      "Epoch 1768/3000, 60/60 - D Loss: 0.00029822413537772263, G Loss: 0.18942059576511383 - 17.73 s\n",
      "Epoch 1769/3000, 60/60 - D Loss: 0.00027853634938423966, G Loss: 0.19143132865428925 - 17.99 s\n",
      "Epoch 1770/3000, 60/60 - D Loss: 0.00025650366478657816, G Loss: 0.18929174542427063 - 17.60 s\n",
      "Epoch 1771/3000, 60/60 - D Loss: 0.00024088116775544677, G Loss: 0.18816916644573212 - 17.70 s\n",
      "Epoch 1772/3000, 60/60 - D Loss: 0.00021636885330167388, G Loss: 0.1890130192041397 - 17.50 s\n",
      "Epoch 1773/3000, 60/60 - D Loss: 0.00020153391733401804, G Loss: 0.19148077070713043 - 17.68 s\n",
      "Epoch 1774/3000, 60/60 - D Loss: 0.00019371220535902012, G Loss: 0.18960478901863098 - 17.51 s\n",
      "Epoch 1775/3000, 60/60 - D Loss: 0.00017846408182720097, G Loss: 0.18869949877262115 - 17.60 s\n",
      "Epoch 1776/3000, 60/60 - D Loss: 0.0001649197710946737, G Loss: 0.18800167739391327 - 17.78 s\n",
      "Epoch 1777/3000, 60/60 - D Loss: 0.00015762581706724177, G Loss: 0.19008398056030273 - 17.70 s\n",
      "Epoch 1778/3000, 60/60 - D Loss: 0.0001521340525556525, G Loss: 0.18919137120246887 - 17.64 s\n",
      "Epoch 1779/3000, 60/60 - D Loss: 0.00014095793598301043, G Loss: 0.18913938105106354 - 17.76 s\n",
      "Epoch 1780/3000, 60/60 - D Loss: 0.00012581250803833655, G Loss: 0.1892126202583313 - 17.62 s\n",
      "Epoch 1781/3000, 60/60 - D Loss: 0.0001109970058692511, G Loss: 0.1885240226984024 - 17.60 s\n",
      "Epoch 1782/3000, 60/60 - D Loss: 0.00010683238902942094, G Loss: 0.18878863751888275 - 17.78 s\n",
      "Epoch 1783/3000, 60/60 - D Loss: 9.855049702878205e-05, G Loss: 0.18845203518867493 - 17.75 s\n",
      "Epoch 1784/3000, 60/60 - D Loss: 9.973433598986503e-05, G Loss: 0.18923211097717285 - 18.06 s\n",
      "Epoch 1785/3000, 60/60 - D Loss: 8.75239587685428e-05, G Loss: 0.18907465040683746 - 17.75 s\n",
      "Epoch 1786/3000, 60/60 - D Loss: 8.711647259929123e-05, G Loss: 0.18798254430294037 - 17.58 s\n",
      "Epoch 1787/3000, 60/60 - D Loss: 8.696993243262341e-05, G Loss: 0.18866589665412903 - 17.70 s\n",
      "Epoch 1788/3000, 60/60 - D Loss: 8.660078285771533e-05, G Loss: 0.18959003686904907 - 17.79 s\n",
      "Epoch 1789/3000, 60/60 - D Loss: 8.621399461361534e-05, G Loss: 0.18930453062057495 - 17.95 s\n",
      "Epoch 1790/3000, 60/60 - D Loss: 8.57953723922833e-05, G Loss: 0.18958398699760437 - 17.90 s\n",
      "Epoch 1791/3000, 60/60 - D Loss: 8.778993243652167e-05, G Loss: 0.18879340589046478 - 17.90 s\n",
      "Epoch 1792/3000, 60/60 - D Loss: 8.551404378209781e-05, G Loss: 0.18887139856815338 - 17.57 s\n",
      "Epoch 1793/3000, 60/60 - D Loss: 7.078640460633778e-05, G Loss: 0.18935075402259827 - 17.66 s\n",
      "Epoch 1794/3000, 60/60 - D Loss: 6.402025089968788e-05, G Loss: 0.1886138767004013 - 17.60 s\n",
      "Epoch 1795/3000, 60/60 - D Loss: 6.188871235934812e-05, G Loss: 0.1907220184803009 - 17.78 s\n",
      "Epoch 1796/3000, 60/60 - D Loss: 5.871549024050182e-05, G Loss: 0.18916037678718567 - 17.66 s\n",
      "Epoch 1797/3000, 60/60 - D Loss: 5.634532773868273e-05, G Loss: 0.18935789167881012 - 17.74 s\n",
      "Epoch 1798/3000, 60/60 - D Loss: 5.326202671440683e-05, G Loss: 0.1884215772151947 - 17.73 s\n",
      "Epoch 1799/3000, 60/60 - D Loss: 5.302195247658403e-05, G Loss: 0.18969541788101196 - 17.95 s\n",
      "Epoch 1800/3000, 60/60 - D Loss: 5.116460787490951e-05, G Loss: 0.18840478360652924 - 17.66 s\n",
      "Epoch 1801/3000, 60/60 - D Loss: 4.3305009478800116e-05, G Loss: 0.18928588926792145 - 17.63 s\n",
      "Epoch 1802/3000, 60/60 - D Loss: 3.9651180109956385e-05, G Loss: 0.1889326423406601 - 17.56 s\n",
      "Epoch 1803/3000, 60/60 - D Loss: 3.974305268883427e-05, G Loss: 0.1890188604593277 - 17.65 s\n",
      "Epoch 1804/3000, 60/60 - D Loss: 3.9373890786009724e-05, G Loss: 0.18956375122070312 - 17.62 s\n",
      "Epoch 1805/3000, 60/60 - D Loss: 3.66205147290799e-05, G Loss: 0.18795308470726013 - 17.75 s\n",
      "Epoch 1806/3000, 60/60 - D Loss: 3.649823944584796e-05, G Loss: 0.18994049727916718 - 17.68 s\n",
      "Epoch 1807/3000, 60/60 - D Loss: 3.533412924028312e-05, G Loss: 0.18926605582237244 - 17.72 s\n",
      "Epoch 1808/3000, 60/60 - D Loss: 3.28189973046733e-05, G Loss: 0.1970822513103485 - 17.65 s\n",
      "Epoch 1809/3000, 60/60 - D Loss: 3.082141526355997e-05, G Loss: 0.18969698250293732 - 17.96 s\n",
      "Epoch 1810/3000, 60/60 - D Loss: 2.785629613732965e-05, G Loss: 0.19312390685081482 - 17.57 s\n",
      "Epoch 1811/3000, 60/60 - D Loss: 2.691535290466618e-05, G Loss: 0.188619926571846 - 17.62 s\n",
      "Epoch 1812/3000, 60/60 - D Loss: 2.8278919571356198e-05, G Loss: 0.18934856355190277 - 17.60 s\n",
      "Epoch 1813/3000, 60/60 - D Loss: 2.4715102764361063e-05, G Loss: 0.18935762345790863 - 17.52 s\n",
      "Epoch 1814/3000, 60/60 - D Loss: 2.143819749811371e-05, G Loss: 0.18966299295425415 - 18.19 s\n",
      "Epoch 1815/3000, 60/60 - D Loss: 2.0528773988282012e-05, G Loss: 0.19019560515880585 - 17.70 s\n",
      "Epoch 1816/3000, 60/60 - D Loss: 2.0325457361636268e-05, G Loss: 0.18949322402477264 - 17.50 s\n",
      "Epoch 1817/3000, 60/60 - D Loss: 1.7517615791717844e-05, G Loss: 0.18915587663650513 - 17.66 s\n",
      "Epoch 1818/3000, 60/60 - D Loss: 1.699601565707809e-05, G Loss: 0.1891884058713913 - 17.61 s\n",
      "Epoch 1819/3000, 60/60 - D Loss: 1.7585067404013444e-05, G Loss: 0.18906618654727936 - 17.67 s\n",
      "Epoch 1820/3000, 60/60 - D Loss: 1.5392883655263034e-05, G Loss: 0.1890227198600769 - 17.52 s\n",
      "Epoch 1821/3000, 60/60 - D Loss: 1.4358743363196425e-05, G Loss: 0.18864747881889343 - 17.50 s\n",
      "Epoch 1822/3000, 60/60 - D Loss: 1.3626452770587605e-05, G Loss: 0.18865184485912323 - 17.71 s\n",
      "Epoch 1823/3000, 60/60 - D Loss: 1.2908013596213408e-05, G Loss: 0.18968622386455536 - 17.56 s\n",
      "Epoch 1824/3000, 60/60 - D Loss: 1.2436495097745137e-05, G Loss: 0.18816375732421875 - 17.68 s\n",
      "Epoch 1825/3000, 60/60 - D Loss: 1.154271296055498e-05, G Loss: 0.18895377218723297 - 17.72 s\n",
      "Epoch 1826/3000, 60/60 - D Loss: 1.0406124094419766e-05, G Loss: 0.1880841851234436 - 17.83 s\n",
      "Epoch 1827/3000, 60/60 - D Loss: 1.0167419721707205e-05, G Loss: 0.18972450494766235 - 17.70 s\n",
      "Epoch 1828/3000, 60/60 - D Loss: 1.0368318525966203e-05, G Loss: 0.18847176432609558 - 17.66 s\n",
      "Epoch 1829/3000, 60/60 - D Loss: 9.070338361505392e-06, G Loss: 0.1890423744916916 - 18.03 s\n",
      "Epoch 1830/3000, 60/60 - D Loss: 7.698780643661252e-06, G Loss: 0.18981698155403137 - 17.66 s\n",
      "Epoch 1831/3000, 60/60 - D Loss: 7.472079819548227e-06, G Loss: 0.18860240280628204 - 17.53 s\n",
      "Epoch 1832/3000, 60/60 - D Loss: 7.110321394554475e-06, G Loss: 0.18921785056591034 - 17.78 s\n",
      "Epoch 1833/3000, 60/60 - D Loss: 7.084758532061031e-06, G Loss: 0.18886275589466095 - 17.73 s\n",
      "Epoch 1834/3000, 60/60 - D Loss: 7.519740923278562e-06, G Loss: 0.1887994259595871 - 17.61 s\n",
      "Epoch 1835/3000, 60/60 - D Loss: 5.845184418040716e-06, G Loss: 0.1894325613975525 - 17.80 s\n",
      "Epoch 1836/3000, 60/60 - D Loss: 5.242774443137144e-06, G Loss: 0.18837399780750275 - 17.59 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1837/3000, 60/60 - D Loss: 5.432651663861421e-06, G Loss: 0.1895928829908371 - 17.91 s\n",
      "Epoch 1838/3000, 60/60 - D Loss: 5.6728784926241005e-06, G Loss: 0.18815192580223083 - 17.78 s\n",
      "Epoch 1839/3000, 60/60 - D Loss: 4.1487440427534494e-06, G Loss: 0.1888807713985443 - 17.79 s\n",
      "Epoch 1840/3000, 60/60 - D Loss: 4.429444370401825e-06, G Loss: 0.18870922923088074 - 17.78 s\n",
      "Epoch 1841/3000, 60/60 - D Loss: 3.647863104472293e-06, G Loss: 0.18899409472942352 - 17.49 s\n",
      "Epoch 1842/3000, 60/60 - D Loss: 3.5990092160931297e-06, G Loss: 0.1881098598241806 - 17.88 s\n",
      "Epoch 1843/3000, 60/60 - D Loss: 3.2071495292829155e-06, G Loss: 0.1894557625055313 - 17.75 s\n",
      "Epoch 1844/3000, 60/60 - D Loss: 2.568295772220619e-06, G Loss: 0.18868331611156464 - 17.75 s\n",
      "Epoch 1845/3000, 60/60 - D Loss: 2.561043420112874e-06, G Loss: 0.19020704925060272 - 17.91 s\n",
      "Epoch 1846/3000, 60/60 - D Loss: 2.6833698704709086e-06, G Loss: 0.1888606697320938 - 17.65 s\n",
      "Epoch 1847/3000, 60/60 - D Loss: 2.5064580725375762e-06, G Loss: 0.18844899535179138 - 17.46 s\n",
      "Epoch 1848/3000, 60/60 - D Loss: 2.5775029479646605e-06, G Loss: 0.18947912752628326 - 17.64 s\n",
      "Epoch 1849/3000, 60/60 - D Loss: 1.7020676228163278e-06, G Loss: 0.18889614939689636 - 17.62 s\n",
      "Epoch 1850/3000, 60/60 - D Loss: 1.638369687140262e-06, G Loss: 0.18758511543273926 - 17.58 s\n",
      "Epoch 1851/3000, 60/60 - D Loss: 1.5394989178740826e-06, G Loss: 0.1892576813697815 - 17.53 s\n",
      "Epoch 1852/3000, 60/60 - D Loss: 1.7322702585143013e-06, G Loss: 0.18853859603405 - 17.62 s\n",
      "Epoch 1853/3000, 60/60 - D Loss: 1.5876655977964393e-06, G Loss: 0.1898725926876068 - 17.82 s\n",
      "Epoch 1854/3000, 60/60 - D Loss: 1.1940334776472383e-06, G Loss: 0.18906430900096893 - 17.66 s\n",
      "Epoch 1855/3000, 60/60 - D Loss: 1.35204807999445e-06, G Loss: 0.18883803486824036 - 17.70 s\n",
      "Epoch 1856/3000, 60/60 - D Loss: 1.4754798067367902e-06, G Loss: 0.1898026019334793 - 17.76 s\n",
      "Epoch 1857/3000, 60/60 - D Loss: 1.0713578311311034e-06, G Loss: 0.18826112151145935 - 17.74 s\n",
      "Epoch 1858/3000, 60/60 - D Loss: 1.009850464764317e-06, G Loss: 0.19047978520393372 - 17.75 s\n",
      "Epoch 1859/3000, 60/60 - D Loss: 1.0827171346983189e-06, G Loss: 0.18994472920894623 - 17.60 s\n",
      "Epoch 1860/3000, 60/60 - D Loss: 8.651922231150966e-07, G Loss: 0.1893462985754013 - 17.75 s\n",
      "Epoch 1861/3000, 60/60 - D Loss: 7.343986208191433e-07, G Loss: 0.18871855735778809 - 17.58 s\n",
      "Epoch 1862/3000, 60/60 - D Loss: 7.28638104893764e-07, G Loss: 0.1898472011089325 - 17.63 s\n",
      "Epoch 1863/3000, 60/60 - D Loss: 6.597304461758435e-07, G Loss: 0.19033615291118622 - 17.59 s\n",
      "Epoch 1864/3000, 60/60 - D Loss: 6.316564998415375e-07, G Loss: 0.18998129665851593 - 17.74 s\n",
      "Epoch 1865/3000, 60/60 - D Loss: 5.591397748833105e-07, G Loss: 0.1879466474056244 - 17.68 s\n",
      "Epoch 1866/3000, 60/60 - D Loss: 5.345905294898423e-07, G Loss: 0.19009694457054138 - 17.70 s\n",
      "Epoch 1867/3000, 60/60 - D Loss: 5.921815813686448e-07, G Loss: 0.18892884254455566 - 17.69 s\n",
      "Epoch 1868/3000, 60/60 - D Loss: 4.518762476077298e-07, G Loss: 0.18866072595119476 - 17.78 s\n",
      "Epoch 1869/3000, 60/60 - D Loss: 3.932723919228276e-07, G Loss: 0.18948915600776672 - 17.57 s\n",
      "Epoch 1870/3000, 60/60 - D Loss: 3.7060032598984494e-07, G Loss: 0.19034017622470856 - 17.64 s\n",
      "Epoch 1871/3000, 60/60 - D Loss: 3.851277646724216e-07, G Loss: 0.18895015120506287 - 17.74 s\n",
      "Epoch 1872/3000, 60/60 - D Loss: 3.851959324910337e-07, G Loss: 0.18970486521720886 - 17.74 s\n",
      "Epoch 1873/3000, 60/60 - D Loss: 3.5210519110789873e-07, G Loss: 0.1892351657152176 - 17.85 s\n",
      "Epoch 1874/3000, 60/60 - D Loss: 2.665549782530918e-07, G Loss: 0.18879877030849457 - 17.82 s\n",
      "Epoch 1875/3000, 60/60 - D Loss: 2.409674213811608e-07, G Loss: 0.18865711987018585 - 17.83 s\n",
      "Epoch 1876/3000, 60/60 - D Loss: 2.466658127614485e-07, G Loss: 0.1888832300901413 - 17.81 s\n",
      "Epoch 1877/3000, 60/60 - D Loss: 3.2846441421624517e-07, G Loss: 0.18908999860286713 - 17.73 s\n",
      "Epoch 1878/3000, 60/60 - D Loss: 1.8388375584016403e-07, G Loss: 0.18909141421318054 - 17.62 s\n",
      "Epoch 1879/3000, 60/60 - D Loss: 1.849519618655343e-07, G Loss: 0.19019465148448944 - 17.48 s\n",
      "Epoch 1880/3000, 60/60 - D Loss: 2.2819614769152563e-07, G Loss: 0.18790459632873535 - 17.72 s\n",
      "Epoch 1881/3000, 60/60 - D Loss: 1.6398821359522864e-07, G Loss: 0.1887153685092926 - 17.61 s\n",
      "Epoch 1882/3000, 60/60 - D Loss: 1.622785492250839e-07, G Loss: 0.1906304657459259 - 17.66 s\n",
      "Epoch 1883/3000, 60/60 - D Loss: 1.4044047084819933e-07, G Loss: 0.18952837586402893 - 17.82 s\n",
      "Epoch 1884/3000, 60/60 - D Loss: 1.4510383943705518e-07, G Loss: 0.1887976974248886 - 17.77 s\n",
      "Epoch 1885/3000, 60/60 - D Loss: 1.2178499365737838e-07, G Loss: 0.1890973597764969 - 17.61 s\n",
      "Epoch 1886/3000, 60/60 - D Loss: 1.1130669527910364e-07, G Loss: 0.1886785477399826 - 17.83 s\n",
      "Epoch 1887/3000, 60/60 - D Loss: 1.0663146323384998e-07, G Loss: 0.18877069652080536 - 17.51 s\n",
      "Epoch 1888/3000, 60/60 - D Loss: 9.39637521306727e-08, G Loss: 0.18911239504814148 - 17.64 s\n",
      "Epoch 1889/3000, 60/60 - D Loss: 8.809610317454808e-08, G Loss: 0.19325360655784607 - 17.71 s\n",
      "Epoch 1890/3000, 60/60 - D Loss: 8.815358121597172e-08, G Loss: 0.18776367604732513 - 17.89 s\n",
      "Epoch 1891/3000, 60/60 - D Loss: 8.05157559374825e-08, G Loss: 0.18886247277259827 - 17.92 s\n",
      "Epoch 1892/3000, 60/60 - D Loss: 7.095308073891415e-08, G Loss: 0.18917183578014374 - 17.81 s\n",
      "Epoch 1893/3000, 60/60 - D Loss: 8.515431509405202e-08, G Loss: 0.18942531943321228 - 17.93 s\n",
      "Epoch 1894/3000, 60/60 - D Loss: 7.472110773263513e-08, G Loss: 0.19221745431423187 - 18.10 s\n",
      "Epoch 1895/3000, 60/60 - D Loss: 7.398224843553972e-08, G Loss: 0.1894376128911972 - 17.49 s\n",
      "Epoch 1896/3000, 60/60 - D Loss: 5.917911261726916e-08, G Loss: 0.1878182590007782 - 17.66 s\n",
      "Epoch 1897/3000, 60/60 - D Loss: 4.724162851145117e-08, G Loss: 0.18885959684848785 - 17.61 s\n",
      "Epoch 1898/3000, 60/60 - D Loss: 4.795846642990237e-08, G Loss: 0.19436997175216675 - 17.68 s\n",
      "Epoch 1899/3000, 60/60 - D Loss: 3.8994241533234913e-08, G Loss: 0.18932755291461945 - 17.67 s\n",
      "Epoch 1900/3000, 60/60 - D Loss: 3.3944147623341664e-08, G Loss: 0.19195154309272766 - 17.63 s\n",
      "Epoch 1901/3000, 60/60 - D Loss: 3.0670896074194015e-08, G Loss: 0.18895754218101501 - 17.67 s\n",
      "Epoch 1902/3000, 60/60 - D Loss: 3.5135308007212124e-08, G Loss: 0.18943828344345093 - 18.16 s\n",
      "Epoch 1903/3000, 60/60 - D Loss: 2.998372991731835e-08, G Loss: 0.1885107457637787 - 17.69 s\n",
      "Epoch 1904/3000, 60/60 - D Loss: 3.220545691543994e-08, G Loss: 0.18845686316490173 - 17.65 s\n",
      "Epoch 1905/3000, 60/60 - D Loss: 3.4240680827583164e-08, G Loss: 0.19078990817070007 - 17.64 s\n",
      "Epoch 1906/3000, 60/60 - D Loss: 2.4492165682935343e-08, G Loss: 0.18900294601917267 - 17.88 s\n",
      "Epoch 1907/3000, 60/60 - D Loss: 2.4831437919325267e-08, G Loss: 0.18833933770656586 - 17.67 s\n",
      "Epoch 1908/3000, 60/60 - D Loss: 2.3742494391527924e-08, G Loss: 0.1886654794216156 - 17.63 s\n",
      "Epoch 1909/3000, 60/60 - D Loss: 1.7690154980416414e-08, G Loss: 0.18974493443965912 - 17.85 s\n",
      "Epoch 1910/3000, 60/60 - D Loss: 2.0301369796837596e-08, G Loss: 0.1906283050775528 - 17.83 s\n",
      "Epoch 1911/3000, 60/60 - D Loss: 1.8948567725085227e-08, G Loss: 0.18954557180404663 - 17.82 s\n",
      "Epoch 1912/3000, 60/60 - D Loss: 2.069576550086695e-08, G Loss: 0.1891060173511505 - 17.56 s\n",
      "Epoch 1913/3000, 60/60 - D Loss: 1.826226850732554e-08, G Loss: 0.1879841834306717 - 17.87 s\n",
      "Epoch 1914/3000, 60/60 - D Loss: 1.8155016308233143e-08, G Loss: 0.18843062222003937 - 17.66 s\n",
      "Epoch 1915/3000, 60/60 - D Loss: 1.5537082679890207e-08, G Loss: 0.18903985619544983 - 17.68 s\n",
      "Epoch 1916/3000, 60/60 - D Loss: 1.6129002019496692e-08, G Loss: 0.18938112258911133 - 17.69 s\n",
      "Epoch 1917/3000, 60/60 - D Loss: 1.8116707611523508e-08, G Loss: 0.18883107602596283 - 17.72 s\n",
      "Epoch 1918/3000, 60/60 - D Loss: 1.215711233233923e-08, G Loss: 0.18830254673957825 - 17.60 s\n",
      "Epoch 1919/3000, 60/60 - D Loss: 1.3761550005119728e-08, G Loss: 0.1884610801935196 - 17.77 s\n",
      "Epoch 1920/3000, 60/60 - D Loss: 1.2056472769141416e-08, G Loss: 0.18807224929332733 - 17.85 s\n",
      "Epoch 1921/3000, 60/60 - D Loss: 1.2116201499783388e-08, G Loss: 0.1893438845872879 - 17.85 s\n",
      "Epoch 1922/3000, 60/60 - D Loss: 9.169809007600338e-09, G Loss: 0.1897691935300827 - 17.68 s\n",
      "Epoch 1923/3000, 60/60 - D Loss: 8.134509926001517e-09, G Loss: 0.18918615579605103 - 17.83 s\n",
      "Epoch 1924/3000, 60/60 - D Loss: 6.474949582361764e-09, G Loss: 0.1889914721250534 - 17.48 s\n",
      "Epoch 1925/3000, 60/60 - D Loss: 1.0111287525034311e-08, G Loss: 0.1896388679742813 - 17.45 s\n",
      "Epoch 1926/3000, 60/60 - D Loss: 6.651997148260109e-09, G Loss: 0.18834750354290009 - 17.46 s\n",
      "Epoch 1927/3000, 60/60 - D Loss: 5.079927763705118e-09, G Loss: 0.18900766968727112 - 17.57 s\n",
      "Epoch 1928/3000, 60/60 - D Loss: 5.6642265192975196e-09, G Loss: 0.18899457156658173 - 17.52 s\n",
      "Epoch 1929/3000, 60/60 - D Loss: 6.6230324102085035e-09, G Loss: 0.18934617936611176 - 17.72 s\n",
      "Epoch 1930/3000, 60/60 - D Loss: 5.41804402021508e-09, G Loss: 0.1899322122335434 - 17.63 s\n",
      "Epoch 1931/3000, 60/60 - D Loss: 5.606528933005481e-09, G Loss: 0.1903880089521408 - 17.83 s\n",
      "Epoch 1932/3000, 60/60 - D Loss: 7.122945985750702e-09, G Loss: 0.18831965327262878 - 17.70 s\n",
      "Epoch 1933/3000, 60/60 - D Loss: 4.715982147060049e-09, G Loss: 0.18995705246925354 - 17.72 s\n",
      "Epoch 1934/3000, 60/60 - D Loss: 4.2873730038742695e-09, G Loss: 0.18874144554138184 - 17.54 s\n",
      "Epoch 1935/3000, 60/60 - D Loss: 6.254487798062602e-09, G Loss: 0.18968452513217926 - 17.45 s\n",
      "Epoch 1936/3000, 60/60 - D Loss: 5.97328123918303e-09, G Loss: 0.1885342299938202 - 17.81 s\n",
      "Epoch 1937/3000, 60/60 - D Loss: 4.113538448372945e-09, G Loss: 0.18859189748764038 - 17.70 s\n",
      "Epoch 1938/3000, 60/60 - D Loss: 4.127355296049779e-09, G Loss: 0.1892082691192627 - 17.84 s\n",
      "Epoch 1939/3000, 60/60 - D Loss: 4.2268025720931714e-09, G Loss: 0.19705727696418762 - 17.61 s\n",
      "Epoch 1940/3000, 60/60 - D Loss: 5.376564925018519e-09, G Loss: 0.18861235678195953 - 17.81 s\n",
      "Epoch 1941/3000, 60/60 - D Loss: 2.905644985983771e-09, G Loss: 0.18878073990345 - 18.08 s\n",
      "Epoch 1942/3000, 60/60 - D Loss: 3.6810849172173084e-09, G Loss: 0.19101053476333618 - 17.72 s\n",
      "Epoch 1943/3000, 60/60 - D Loss: 3.1877656426809644e-09, G Loss: 0.19089746475219727 - 17.57 s\n",
      "Epoch 1944/3000, 60/60 - D Loss: 3.640309637851233e-09, G Loss: 0.19047710299491882 - 17.78 s\n",
      "Epoch 1945/3000, 60/60 - D Loss: 2.1075540686287583e-09, G Loss: 0.19152991473674774 - 17.66 s\n",
      "Epoch 1946/3000, 60/60 - D Loss: 2.8865503792828515e-09, G Loss: 0.18908105790615082 - 17.91 s\n",
      "Epoch 1947/3000, 60/60 - D Loss: 2.8204958302181955e-09, G Loss: 0.1900707185268402 - 17.68 s\n",
      "Epoch 1948/3000, 60/60 - D Loss: 2.881071936090048e-09, G Loss: 0.18936190009117126 - 17.66 s\n",
      "Epoch 1949/3000, 60/60 - D Loss: 2.650987428138912e-09, G Loss: 0.18812674283981323 - 17.77 s\n",
      "Epoch 1950/3000, 60/60 - D Loss: 3.638194735563105e-09, G Loss: 0.19024182856082916 - 17.64 s\n",
      "Epoch 1951/3000, 60/60 - D Loss: 1.5348693351176484e-09, G Loss: 0.18934516608715057 - 18.21 s\n",
      "Epoch 1952/3000, 60/60 - D Loss: 1.880610999148739e-09, G Loss: 0.1886931210756302 - 17.51 s\n",
      "Epoch 1953/3000, 60/60 - D Loss: 1.8434458916636145e-09, G Loss: 0.1889197677373886 - 17.62 s\n",
      "Epoch 1954/3000, 60/60 - D Loss: 2.250973837880643e-09, G Loss: 0.1886925846338272 - 17.72 s\n",
      "Epoch 1955/3000, 60/60 - D Loss: 2.7939523426496853e-09, G Loss: 0.18878844380378723 - 17.78 s\n",
      "Epoch 1956/3000, 60/60 - D Loss: 2.7638475675586995e-09, G Loss: 0.18904444575309753 - 17.68 s\n",
      "Epoch 1957/3000, 60/60 - D Loss: 2.1066526191001285e-09, G Loss: 0.1889326125383377 - 17.67 s\n",
      "Epoch 1958/3000, 60/60 - D Loss: 1.7374572802350812e-09, G Loss: 0.18914814293384552 - 17.72 s\n",
      "Epoch 1959/3000, 60/60 - D Loss: 1.5510513191473087e-09, G Loss: 0.18898658454418182 - 17.92 s\n",
      "Epoch 1960/3000, 60/60 - D Loss: 1.8713308733679527e-09, G Loss: 0.1880796253681183 - 18.20 s\n",
      "Epoch 1961/3000, 60/60 - D Loss: 1.939554406595349e-09, G Loss: 0.18813759088516235 - 17.68 s\n",
      "Epoch 1962/3000, 60/60 - D Loss: 1.902232674925574e-09, G Loss: 0.1889931559562683 - 17.76 s\n",
      "Epoch 1963/3000, 60/60 - D Loss: 1.929202508613409e-09, G Loss: 0.18842226266860962 - 17.84 s\n",
      "Epoch 1964/3000, 60/60 - D Loss: 1.7615259937828979e-09, G Loss: 0.18848823010921478 - 17.68 s\n",
      "Epoch 1965/3000, 60/60 - D Loss: 1.81853931745116e-09, G Loss: 0.18799883127212524 - 17.72 s\n",
      "Epoch 1966/3000, 60/60 - D Loss: 1.8130462402666386e-09, G Loss: 0.19010360538959503 - 17.91 s\n",
      "Epoch 1967/3000, 60/60 - D Loss: 1.6553043006390863e-09, G Loss: 0.186388298869133 - 18.03 s\n",
      "Epoch 1968/3000, 60/60 - D Loss: 1.6405936244194412e-09, G Loss: 0.1894846111536026 - 17.78 s\n",
      "Epoch 1969/3000, 60/60 - D Loss: 1.6657476548079969e-09, G Loss: 0.18902486562728882 - 17.74 s\n",
      "Epoch 1970/3000, 60/60 - D Loss: 1.7838538019033568e-09, G Loss: 0.18922165036201477 - 17.60 s\n",
      "Epoch 1971/3000, 60/60 - D Loss: 1.5859729771741096e-09, G Loss: 0.18830427527427673 - 17.71 s\n",
      "Epoch 1972/3000, 60/60 - D Loss: 1.7368137679598218e-09, G Loss: 0.18916644155979156 - 17.75 s\n",
      "Epoch 1973/3000, 60/60 - D Loss: 1.347506051246818e-09, G Loss: 0.1882643848657608 - 17.58 s\n",
      "Epoch 1974/3000, 60/60 - D Loss: 1.6522448914338904e-09, G Loss: 0.18961076438426971 - 17.76 s\n",
      "Epoch 1975/3000, 60/60 - D Loss: 1.788898384215724e-09, G Loss: 0.19092708826065063 - 17.68 s\n",
      "Epoch 1976/3000, 60/60 - D Loss: 1.5226810904143224e-09, G Loss: 0.1920282542705536 - 17.63 s\n",
      "Epoch 1977/3000, 60/60 - D Loss: 1.7335084196484145e-09, G Loss: 0.18935728073120117 - 17.91 s\n",
      "Epoch 1978/3000, 60/60 - D Loss: 1.7753248395972537e-09, G Loss: 0.1885402798652649 - 17.61 s\n",
      "Epoch 1979/3000, 60/60 - D Loss: 1.4895316842206564e-09, G Loss: 0.18853208422660828 - 17.76 s\n",
      "Epoch 1980/3000, 60/60 - D Loss: 1.6726156544868975e-09, G Loss: 0.1899978667497635 - 17.66 s\n",
      "Epoch 1981/3000, 60/60 - D Loss: 1.4699115608845918e-09, G Loss: 0.18900136649608612 - 17.66 s\n",
      "Epoch 1982/3000, 60/60 - D Loss: 1.3674259424634855e-09, G Loss: 0.1890297532081604 - 17.70 s\n",
      "Epoch 1983/3000, 60/60 - D Loss: 1.445007048709473e-09, G Loss: 0.18854466080665588 - 17.64 s\n",
      "Epoch 1984/3000, 60/60 - D Loss: 1.4470607309256204e-09, G Loss: 0.18821504712104797 - 17.75 s\n",
      "Epoch 1985/3000, 60/60 - D Loss: 1.3034808368171105e-09, G Loss: 0.18988868594169617 - 17.91 s\n",
      "Epoch 1986/3000, 60/60 - D Loss: 1.1506967683630324e-09, G Loss: 0.18790389597415924 - 17.85 s\n",
      "Epoch 1987/3000, 60/60 - D Loss: 1.2664564337017993e-09, G Loss: 0.18977580964565277 - 17.86 s\n",
      "Epoch 1988/3000, 60/60 - D Loss: 1.1518184701443776e-09, G Loss: 0.18959535658359528 - 17.70 s\n",
      "Epoch 1989/3000, 60/60 - D Loss: 1.140677252231949e-09, G Loss: 0.18839479982852936 - 17.44 s\n",
      "Epoch 1990/3000, 60/60 - D Loss: 1.0890755733491356e-09, G Loss: 0.1886654645204544 - 17.68 s\n",
      "Epoch 1991/3000, 60/60 - D Loss: 1.1213246219243442e-09, G Loss: 0.1891198754310608 - 17.62 s\n",
      "Epoch 1992/3000, 60/60 - D Loss: 1.1656759409521566e-09, G Loss: 0.18931534886360168 - 17.72 s\n",
      "Epoch 1993/3000, 60/60 - D Loss: 1.1409387597088633e-09, G Loss: 0.18794254958629608 - 17.79 s\n",
      "Epoch 1994/3000, 60/60 - D Loss: 1.0701229390941143e-09, G Loss: 0.1887306123971939 - 17.56 s\n",
      "Epoch 1995/3000, 60/60 - D Loss: 1.0422852781694764e-09, G Loss: 0.18863889575004578 - 17.75 s\n",
      "Epoch 1996/3000, 60/60 - D Loss: 1.0656417683868504e-09, G Loss: 0.18880079686641693 - 17.73 s\n",
      "Epoch 1997/3000, 60/60 - D Loss: 9.287835234995825e-10, G Loss: 0.18929916620254517 - 17.88 s\n",
      "Epoch 1998/3000, 60/60 - D Loss: 7.299954081075834e-10, G Loss: 0.18870043754577637 - 17.67 s\n",
      "Epoch 1999/3000, 60/60 - D Loss: 5.763328048546815e-10, G Loss: 0.18963004648685455 - 17.99 s\n",
      "Epoch 2000/3000, 60/60 - D Loss: 5.652164791568716e-10, G Loss: 0.18915089964866638 - 17.72 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\.conda\\envs\\cuda_tf\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model at epoch 2000 to saved_mode\\model_2000.h5\n",
      "Epoch 2001/3000, 60/60 - D Loss: 6.373850264113621e-10, G Loss: 0.18833546340465546 - 17.89 s\n",
      "Epoch 2002/3000, 60/60 - D Loss: 6.667596127352311e-10, G Loss: 0.18946222960948944 - 17.92 s\n",
      "Epoch 2003/3000, 60/60 - D Loss: 5.71630120821886e-10, G Loss: 0.18811796605587006 - 17.78 s\n",
      "Epoch 2004/3000, 60/60 - D Loss: 6.626557156848219e-10, G Loss: 0.18792037665843964 - 17.70 s\n",
      "Epoch 2005/3000, 60/60 - D Loss: 7.598625316744718e-10, G Loss: 0.18925131857395172 - 17.61 s\n",
      "Epoch 2006/3000, 60/60 - D Loss: 1.1169083368478571e-09, G Loss: 0.1915566772222519 - 17.72 s\n",
      "Epoch 2007/3000, 60/60 - D Loss: 2.163347332196878e-09, G Loss: 0.18870878219604492 - 17.84 s\n",
      "Epoch 2008/3000, 60/60 - D Loss: 0.01776244884240441, G Loss: 0.18892298638820648 - 17.59 s\n",
      "Epoch 2009/3000, 60/60 - D Loss: 0.0006423814920708537, G Loss: 0.1888604611158371 - 17.56 s\n",
      "Epoch 2010/3000, 60/60 - D Loss: 0.0011642534882412292, G Loss: 0.18998916447162628 - 17.89 s\n",
      "Epoch 2011/3000, 60/60 - D Loss: 0.0009414368469151668, G Loss: 0.1931544989347458 - 17.71 s\n",
      "Epoch 2012/3000, 60/60 - D Loss: 0.000781057846324984, G Loss: 0.18873676657676697 - 17.89 s\n",
      "Epoch 2013/3000, 60/60 - D Loss: 0.0007286752115760464, G Loss: 0.18795786798000336 - 17.67 s\n",
      "Epoch 2014/3000, 60/60 - D Loss: 0.0005692489685316104, G Loss: 0.18903324007987976 - 17.59 s\n",
      "Epoch 2015/3000, 60/60 - D Loss: 0.0005346706075215479, G Loss: 0.18924418091773987 - 17.69 s\n",
      "Epoch 2016/3000, 60/60 - D Loss: 0.0004977512126060901, G Loss: 0.1885923445224762 - 17.78 s\n",
      "Epoch 2017/3000, 60/60 - D Loss: 0.0004633569114957936, G Loss: 0.18810071051120758 - 17.73 s\n",
      "Epoch 2018/3000, 60/60 - D Loss: 0.00037971100573486183, G Loss: 0.18866664171218872 - 17.65 s\n",
      "Epoch 2019/3000, 60/60 - D Loss: 0.0003766751724469941, G Loss: 0.18958380818367004 - 17.88 s\n",
      "Epoch 2020/3000, 60/60 - D Loss: 0.00031709985069028335, G Loss: 0.18996840715408325 - 17.67 s\n",
      "Epoch 2021/3000, 60/60 - D Loss: 0.00030251847510953667, G Loss: 0.18904219567775726 - 17.81 s\n",
      "Epoch 2022/3000, 60/60 - D Loss: 0.000286561166831234, G Loss: 0.18826505541801453 - 17.64 s\n",
      "Epoch 2023/3000, 60/60 - D Loss: 0.00025340597039757995, G Loss: 0.18825870752334595 - 17.77 s\n",
      "Epoch 2024/3000, 60/60 - D Loss: 0.00023852719277783763, G Loss: 0.19087190926074982 - 17.64 s\n",
      "Epoch 2025/3000, 60/60 - D Loss: 0.0002331453415536089, G Loss: 0.19070610404014587 - 17.64 s\n",
      "Epoch 2026/3000, 60/60 - D Loss: 0.000206131951927091, G Loss: 0.1886470466852188 - 17.83 s\n",
      "Epoch 2027/3000, 60/60 - D Loss: 0.00019674475379360956, G Loss: 0.188231959939003 - 17.94 s\n",
      "Epoch 2028/3000, 60/60 - D Loss: 0.0001936142575686972, G Loss: 0.18898169696331024 - 17.78 s\n",
      "Epoch 2029/3000, 60/60 - D Loss: 0.00017859622448668233, G Loss: 0.18834632635116577 - 17.60 s\n",
      "Epoch 2030/3000, 60/60 - D Loss: 0.0001687478525127517, G Loss: 0.18873484432697296 - 17.87 s\n",
      "Epoch 2031/3000, 60/60 - D Loss: 0.00014981675167291542, G Loss: 0.18833522498607635 - 17.52 s\n",
      "Epoch 2032/3000, 60/60 - D Loss: 0.00012984407089788874, G Loss: 0.1877058893442154 - 17.84 s\n",
      "Epoch 2033/3000, 60/60 - D Loss: 0.00012153360444244754, G Loss: 0.19606658816337585 - 17.76 s\n",
      "Epoch 2034/3000, 60/60 - D Loss: 0.00011467204353721172, G Loss: 0.18879903852939606 - 17.92 s\n",
      "Epoch 2035/3000, 60/60 - D Loss: 0.00010403087480881368, G Loss: 0.18924543261528015 - 17.39 s\n",
      "Epoch 2036/3000, 60/60 - D Loss: 0.00010144906514142349, G Loss: 0.1886695921421051 - 17.73 s\n",
      "Epoch 2037/3000, 60/60 - D Loss: 9.868268580248696e-05, G Loss: 0.1884656399488449 - 17.63 s\n",
      "Epoch 2038/3000, 60/60 - D Loss: 9.073794717551209e-05, G Loss: 0.1885145753622055 - 17.93 s\n",
      "Epoch 2039/3000, 60/60 - D Loss: 7.48751440369233e-05, G Loss: 0.19105678796768188 - 17.68 s\n",
      "Epoch 2040/3000, 60/60 - D Loss: 0.00010361525824009732, G Loss: 0.189004048705101 - 17.68 s\n",
      "Epoch 2041/3000, 60/60 - D Loss: 7.39967055096713e-05, G Loss: 0.18878579139709473 - 17.69 s\n",
      "Epoch 2042/3000, 60/60 - D Loss: 6.808270109104342e-05, G Loss: 0.18895944952964783 - 18.05 s\n",
      "Epoch 2043/3000, 60/60 - D Loss: 8.1092932532556e-05, G Loss: 0.1888977587223053 - 17.74 s\n",
      "Epoch 2044/3000, 60/60 - D Loss: 7.40726813432957e-05, G Loss: 0.18907643854618073 - 17.71 s\n",
      "Epoch 2045/3000, 60/60 - D Loss: 7.054026048081141e-05, G Loss: 0.1902158260345459 - 17.60 s\n",
      "Epoch 2046/3000, 60/60 - D Loss: 6.095404586403674e-05, G Loss: 0.18839003145694733 - 17.56 s\n",
      "Epoch 2047/3000, 60/60 - D Loss: 5.3694754740263306e-05, G Loss: 0.1898488998413086 - 17.65 s\n",
      "Epoch 2048/3000, 60/60 - D Loss: 5.288825596494462e-05, G Loss: 0.18998108804225922 - 17.61 s\n",
      "Epoch 2049/3000, 60/60 - D Loss: 4.431549507444288e-05, G Loss: 0.19079408049583435 - 17.61 s\n",
      "Epoch 2050/3000, 60/60 - D Loss: 3.869783122922854e-05, G Loss: 0.18790698051452637 - 17.60 s\n",
      "Epoch 2051/3000, 60/60 - D Loss: 3.214708227972096e-05, G Loss: 0.18903903663158417 - 17.75 s\n",
      "Epoch 2052/3000, 60/60 - D Loss: 2.929191165890188e-05, G Loss: 0.19081883132457733 - 17.66 s\n",
      "Epoch 2053/3000, 60/60 - D Loss: 2.0355494797286156e-05, G Loss: 0.18925224244594574 - 17.75 s\n",
      "Epoch 2054/3000, 60/60 - D Loss: 2.78334493799548e-05, G Loss: 0.18906016647815704 - 17.78 s\n",
      "Epoch 2055/3000, 60/60 - D Loss: 2.2383068881026702e-05, G Loss: 0.18889345228672028 - 17.74 s\n",
      "Epoch 2056/3000, 60/60 - D Loss: 1.929641446452024e-05, G Loss: 0.18943503499031067 - 18.05 s\n",
      "Epoch 2057/3000, 60/60 - D Loss: 1.6435883232190918e-05, G Loss: 0.18902744352817535 - 17.81 s\n",
      "Epoch 2058/3000, 60/60 - D Loss: 1.3020026479182434e-05, G Loss: 0.18866726756095886 - 17.98 s\n",
      "Epoch 2059/3000, 60/60 - D Loss: 1.0480731056361492e-05, G Loss: 0.18843890726566315 - 17.82 s\n",
      "Epoch 2060/3000, 60/60 - D Loss: 1.0602912794865915e-05, G Loss: 0.19155509769916534 - 17.73 s\n",
      "Epoch 2061/3000, 60/60 - D Loss: 7.43514734935502e-06, G Loss: 0.1895800232887268 - 17.62 s\n",
      "Epoch 2062/3000, 60/60 - D Loss: 6.617682561937954e-06, G Loss: 0.18906909227371216 - 17.65 s\n",
      "Epoch 2063/3000, 60/60 - D Loss: 6.012679232725304e-06, G Loss: 0.189038947224617 - 17.60 s\n",
      "Epoch 2064/3000, 60/60 - D Loss: 5.110722433698811e-06, G Loss: 0.18883249163627625 - 17.59 s\n",
      "Epoch 2065/3000, 60/60 - D Loss: 4.731937300661571e-06, G Loss: 0.18820855021476746 - 17.75 s\n",
      "Epoch 2066/3000, 60/60 - D Loss: 3.916088882505164e-06, G Loss: 0.1882292777299881 - 17.59 s\n",
      "Epoch 2067/3000, 60/60 - D Loss: 3.5394266824084752e-06, G Loss: 0.18906766176223755 - 17.61 s\n",
      "Epoch 2068/3000, 60/60 - D Loss: 3.1269144322720877e-06, G Loss: 0.18845102190971375 - 17.68 s\n",
      "Epoch 2069/3000, 60/60 - D Loss: 2.9418983977791413e-06, G Loss: 0.18819065392017365 - 17.70 s\n",
      "Epoch 2070/3000, 60/60 - D Loss: 2.8627790413793264e-06, G Loss: 0.18838851153850555 - 17.51 s\n",
      "Epoch 2071/3000, 60/60 - D Loss: 2.5845790680989467e-06, G Loss: 0.1894073635339737 - 17.90 s\n",
      "Epoch 2072/3000, 60/60 - D Loss: 3.002438903720872e-06, G Loss: 0.18837113678455353 - 17.63 s\n",
      "Epoch 2073/3000, 60/60 - D Loss: 2.141324495497088e-06, G Loss: 0.19057849049568176 - 18.05 s\n",
      "Epoch 2074/3000, 60/60 - D Loss: 2.020170175143221e-06, G Loss: 0.19101674854755402 - 17.66 s\n",
      "Epoch 2075/3000, 60/60 - D Loss: 1.714823553378153e-06, G Loss: 0.18840967118740082 - 17.79 s\n",
      "Epoch 2076/3000, 60/60 - D Loss: 1.6151326311941716e-06, G Loss: 0.18832330405712128 - 17.74 s\n",
      "Epoch 2077/3000, 60/60 - D Loss: 1.3849176672131591e-06, G Loss: 0.188719242811203 - 17.71 s\n",
      "Epoch 2078/3000, 60/60 - D Loss: 1.3041957407367022e-06, G Loss: 0.19268101453781128 - 17.69 s\n",
      "Epoch 2079/3000, 60/60 - D Loss: 1.2701001570469828e-06, G Loss: 0.18860673904418945 - 17.83 s\n",
      "Epoch 2080/3000, 60/60 - D Loss: 1.2328809391171092e-06, G Loss: 0.18891580402851105 - 17.83 s\n",
      "Epoch 2081/3000, 60/60 - D Loss: 1.3230914461814791e-06, G Loss: 0.18863220512866974 - 17.97 s\n",
      "Epoch 2082/3000, 60/60 - D Loss: 1.2326379885685412e-06, G Loss: 0.19089731574058533 - 17.65 s\n",
      "Epoch 2083/3000, 60/60 - D Loss: 1.0390080191768902e-06, G Loss: 0.18884854018688202 - 17.55 s\n",
      "Epoch 2084/3000, 60/60 - D Loss: 1.2193124958592705e-06, G Loss: 0.18869958817958832 - 17.73 s\n",
      "Epoch 2085/3000, 60/60 - D Loss: 9.726364791351472e-07, G Loss: 0.1887071579694748 - 17.83 s\n",
      "Epoch 2086/3000, 60/60 - D Loss: 1.0121960258846485e-06, G Loss: 0.18995575606822968 - 17.80 s\n",
      "Epoch 2087/3000, 60/60 - D Loss: 8.626676594181149e-07, G Loss: 0.18776218593120575 - 17.74 s\n",
      "Epoch 2088/3000, 60/60 - D Loss: 7.208195252905725e-07, G Loss: 0.18884509801864624 - 18.03 s\n",
      "Epoch 2089/3000, 60/60 - D Loss: 7.525962273380316e-07, G Loss: 0.18907715380191803 - 17.57 s\n",
      "Epoch 2090/3000, 60/60 - D Loss: 6.397167361393841e-07, G Loss: 0.19006885588169098 - 17.67 s\n",
      "Epoch 2091/3000, 60/60 - D Loss: 5.632475279515603e-07, G Loss: 0.188584566116333 - 17.63 s\n",
      "Epoch 2092/3000, 60/60 - D Loss: 5.436609782361046e-07, G Loss: 0.18887551128864288 - 17.44 s\n",
      "Epoch 2093/3000, 60/60 - D Loss: 5.207864421929997e-07, G Loss: 0.1899002343416214 - 17.55 s\n",
      "Epoch 2094/3000, 60/60 - D Loss: 4.5729664721605445e-07, G Loss: 0.18772189319133759 - 17.58 s\n",
      "Epoch 2095/3000, 60/60 - D Loss: 4.5619393251872253e-07, G Loss: 0.1884266436100006 - 17.69 s\n",
      "Epoch 2096/3000, 60/60 - D Loss: 3.897235127947596e-07, G Loss: 0.1884198784828186 - 17.67 s\n",
      "Epoch 2097/3000, 60/60 - D Loss: 3.42703388389598e-07, G Loss: 0.1964501291513443 - 17.72 s\n",
      "Epoch 2098/3000, 60/60 - D Loss: 3.595745301510078e-07, G Loss: 0.18929505348205566 - 17.73 s\n",
      "Epoch 2099/3000, 60/60 - D Loss: 3.5499785444592646e-07, G Loss: 0.18917964398860931 - 17.73 s\n",
      "Epoch 2100/3000, 60/60 - D Loss: 2.837533441546469e-07, G Loss: 0.19069823622703552 - 17.57 s\n",
      "Epoch 2101/3000, 60/60 - D Loss: 3.2144950234958003e-07, G Loss: 0.18901270627975464 - 17.67 s\n",
      "Epoch 2102/3000, 60/60 - D Loss: 3.242957377080913e-07, G Loss: 0.1887539178133011 - 17.80 s\n",
      "Epoch 2103/3000, 60/60 - D Loss: 2.9710153078010837e-07, G Loss: 0.18900585174560547 - 17.84 s\n",
      "Epoch 2104/3000, 60/60 - D Loss: 2.516872927432745e-07, G Loss: 0.19014587998390198 - 17.82 s\n",
      "Epoch 2105/3000, 60/60 - D Loss: 2.08450116417902e-07, G Loss: 0.19074460864067078 - 17.62 s\n",
      "Epoch 2106/3000, 60/60 - D Loss: 1.8208299876892875e-07, G Loss: 0.1891651302576065 - 17.73 s\n",
      "Epoch 2107/3000, 60/60 - D Loss: 1.8916497176307834e-07, G Loss: 0.18819323182106018 - 17.77 s\n",
      "Epoch 2108/3000, 60/60 - D Loss: 1.6693825793945294e-07, G Loss: 0.1899574100971222 - 17.82 s\n",
      "Epoch 2109/3000, 60/60 - D Loss: 1.644213164370134e-07, G Loss: 0.18903379142284393 - 17.74 s\n",
      "Epoch 2110/3000, 60/60 - D Loss: 1.4412251113371521e-07, G Loss: 0.18776144087314606 - 17.67 s\n",
      "Epoch 2111/3000, 60/60 - D Loss: 1.3982261400091733e-07, G Loss: 0.1885075867176056 - 17.69 s\n",
      "Epoch 2112/3000, 60/60 - D Loss: 1.4002443360361383e-07, G Loss: 0.1887388527393341 - 17.59 s\n",
      "Epoch 2113/3000, 60/60 - D Loss: 1.1997010836151123e-07, G Loss: 0.19525183737277985 - 17.82 s\n",
      "Epoch 2114/3000, 60/60 - D Loss: 1.5119779726152638e-07, G Loss: 0.1884283870458603 - 17.74 s\n",
      "Epoch 2115/3000, 60/60 - D Loss: 1.3000432447923815e-07, G Loss: 0.18878917396068573 - 17.60 s\n",
      "Epoch 2116/3000, 60/60 - D Loss: 1.1542098284078595e-07, G Loss: 0.18940430879592896 - 17.75 s\n",
      "Epoch 2117/3000, 60/60 - D Loss: 9.985110990123891e-08, G Loss: 0.18833087384700775 - 17.82 s\n",
      "Epoch 2118/3000, 60/60 - D Loss: 9.957275541594424e-08, G Loss: 0.18758626282215118 - 17.75 s\n",
      "Epoch 2119/3000, 60/60 - D Loss: 8.578780095677696e-08, G Loss: 0.18851502239704132 - 17.94 s\n",
      "Epoch 2120/3000, 60/60 - D Loss: 7.808105289067169e-08, G Loss: 0.1878024786710739 - 17.57 s\n",
      "Epoch 2121/3000, 60/60 - D Loss: 7.214383409073766e-08, G Loss: 0.1885690540075302 - 17.64 s\n",
      "Epoch 2122/3000, 60/60 - D Loss: 6.026345552440082e-08, G Loss: 0.1891613006591797 - 17.88 s\n",
      "Epoch 2123/3000, 60/60 - D Loss: 6.52220521445146e-08, G Loss: 0.18817591667175293 - 17.84 s\n",
      "Epoch 2124/3000, 60/60 - D Loss: 4.931919726086642e-08, G Loss: 0.18830572068691254 - 17.82 s\n",
      "Epoch 2125/3000, 60/60 - D Loss: 5.454730194441593e-08, G Loss: 0.18867243826389313 - 18.04 s\n",
      "Epoch 2126/3000, 60/60 - D Loss: 5.1965068386360114e-08, G Loss: 0.18863840401172638 - 17.76 s\n",
      "Epoch 2127/3000, 60/60 - D Loss: 3.0184438970862204e-08, G Loss: 0.18857693672180176 - 17.95 s\n",
      "Epoch 2128/3000, 60/60 - D Loss: 3.174110060726043e-08, G Loss: 0.18896540999412537 - 17.76 s\n",
      "Epoch 2129/3000, 60/60 - D Loss: 3.6416601411487504e-08, G Loss: 0.19084420800209045 - 17.68 s\n",
      "Epoch 2130/3000, 60/60 - D Loss: 3.444504116482949e-08, G Loss: 0.18795353174209595 - 17.62 s\n",
      "Epoch 2131/3000, 60/60 - D Loss: 3.173035710655331e-08, G Loss: 0.19013859331607819 - 17.54 s\n",
      "Epoch 2132/3000, 60/60 - D Loss: 2.9011939346561266e-08, G Loss: 0.1882220059633255 - 17.73 s\n",
      "Epoch 2133/3000, 60/60 - D Loss: 3.19551934162797e-08, G Loss: 0.1888589709997177 - 17.56 s\n",
      "Epoch 2134/3000, 60/60 - D Loss: 2.9414955179940527e-08, G Loss: 0.18767835199832916 - 18.04 s\n",
      "Epoch 2135/3000, 60/60 - D Loss: 2.7780754522765266e-08, G Loss: 0.18885962665081024 - 17.90 s\n",
      "Epoch 2136/3000, 60/60 - D Loss: 2.5629685115316145e-08, G Loss: 0.18826402723789215 - 17.94 s\n",
      "Epoch 2137/3000, 60/60 - D Loss: 2.5170587049979515e-08, G Loss: 0.18907301127910614 - 17.64 s\n",
      "Epoch 2138/3000, 60/60 - D Loss: 1.9814948590789194e-08, G Loss: 0.18831056356430054 - 17.78 s\n",
      "Epoch 2139/3000, 60/60 - D Loss: 2.114075554459499e-08, G Loss: 0.1884758472442627 - 17.64 s\n",
      "Epoch 2140/3000, 60/60 - D Loss: 2.3661063367711593e-08, G Loss: 0.1888517290353775 - 17.58 s\n",
      "Epoch 2141/3000, 60/60 - D Loss: 2.144424771045525e-08, G Loss: 0.18797165155410767 - 17.66 s\n",
      "Epoch 2142/3000, 60/60 - D Loss: 1.924894038477301e-08, G Loss: 0.18906554579734802 - 17.78 s\n",
      "Epoch 2143/3000, 60/60 - D Loss: 2.031628258160334e-08, G Loss: 0.19191811978816986 - 17.76 s\n",
      "Epoch 2144/3000, 60/60 - D Loss: 1.8129796869499615e-08, G Loss: 0.18822821974754333 - 17.66 s\n",
      "Epoch 2145/3000, 60/60 - D Loss: 1.5954553974403124e-08, G Loss: 0.18943269550800323 - 17.57 s\n",
      "Epoch 2146/3000, 60/60 - D Loss: 1.375102071793366e-08, G Loss: 0.18781407177448273 - 17.68 s\n",
      "Epoch 2147/3000, 60/60 - D Loss: 1.4596516865637496e-08, G Loss: 0.1894223988056183 - 17.48 s\n",
      "Epoch 2148/3000, 60/60 - D Loss: 1.4890165771067922e-08, G Loss: 0.1892412304878235 - 17.65 s\n",
      "Epoch 2149/3000, 60/60 - D Loss: 1.4387331549216287e-08, G Loss: 0.18904288113117218 - 18.00 s\n",
      "Epoch 2150/3000, 60/60 - D Loss: 1.241636937926796e-08, G Loss: 0.1884465217590332 - 17.87 s\n",
      "Epoch 2151/3000, 60/60 - D Loss: 1.237063267502201e-08, G Loss: 0.18872374296188354 - 17.71 s\n",
      "Epoch 2152/3000, 60/60 - D Loss: 1.1974012800708943e-08, G Loss: 0.19101795554161072 - 17.64 s\n",
      "Epoch 2153/3000, 60/60 - D Loss: 1.0310155871412485e-08, G Loss: 0.18892930448055267 - 17.71 s\n",
      "Epoch 2154/3000, 60/60 - D Loss: 9.036424316287428e-09, G Loss: 0.18980026245117188 - 17.81 s\n",
      "Epoch 2155/3000, 60/60 - D Loss: 7.3881793322128075e-09, G Loss: 0.18832312524318695 - 17.57 s\n",
      "Epoch 2156/3000, 60/60 - D Loss: 7.510215789324412e-09, G Loss: 0.18860894441604614 - 17.65 s\n",
      "Epoch 2157/3000, 60/60 - D Loss: 7.45690465495455e-09, G Loss: 0.18798184394836426 - 17.76 s\n",
      "Epoch 2158/3000, 60/60 - D Loss: 8.30048607885353e-09, G Loss: 0.18822771310806274 - 17.71 s\n",
      "Epoch 2159/3000, 60/60 - D Loss: 8.33818759047103e-09, G Loss: 0.18901292979717255 - 17.61 s\n",
      "Epoch 2160/3000, 60/60 - D Loss: 6.675327748895121e-09, G Loss: 0.1887149065732956 - 17.38 s\n",
      "Epoch 2161/3000, 60/60 - D Loss: 6.586439366780203e-09, G Loss: 0.1883888840675354 - 17.55 s\n",
      "Epoch 2162/3000, 60/60 - D Loss: 5.497979652818209e-09, G Loss: 0.18931785225868225 - 17.56 s\n",
      "Epoch 2163/3000, 60/60 - D Loss: 4.965297313824606e-09, G Loss: 0.18949396908283234 - 17.82 s\n",
      "Epoch 2164/3000, 60/60 - D Loss: 5.1614474515693405e-09, G Loss: 0.18985213339328766 - 17.94 s\n",
      "Epoch 2165/3000, 60/60 - D Loss: 5.3068199361273015e-09, G Loss: 0.18855835497379303 - 17.70 s\n",
      "Epoch 2166/3000, 60/60 - D Loss: 5.497087212508189e-09, G Loss: 0.1891208440065384 - 17.85 s\n",
      "Epoch 2167/3000, 60/60 - D Loss: 4.432360091910677e-09, G Loss: 0.18854060769081116 - 17.73 s\n",
      "Epoch 2168/3000, 60/60 - D Loss: 4.061899853415821e-09, G Loss: 0.18918633460998535 - 17.62 s\n",
      "Epoch 2169/3000, 60/60 - D Loss: 4.143537444742645e-09, G Loss: 0.18910323083400726 - 17.64 s\n",
      "Epoch 2170/3000, 60/60 - D Loss: 3.885126294137821e-09, G Loss: 0.18874190747737885 - 17.65 s\n",
      "Epoch 2171/3000, 60/60 - D Loss: 3.1232896351130433e-09, G Loss: 0.18806375563144684 - 17.46 s\n",
      "Epoch 2172/3000, 60/60 - D Loss: 3.4587934835304205e-09, G Loss: 0.18754588067531586 - 17.44 s\n",
      "Epoch 2173/3000, 60/60 - D Loss: 3.1289130178946065e-09, G Loss: 0.18865375220775604 - 17.60 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2174/3000, 60/60 - D Loss: 2.9547902912407595e-09, G Loss: 0.1912929117679596 - 17.49 s\n",
      "Epoch 2175/3000, 60/60 - D Loss: 2.97554832775224e-09, G Loss: 0.18840864300727844 - 17.65 s\n",
      "Epoch 2176/3000, 60/60 - D Loss: 2.671170595965836e-09, G Loss: 0.18857915699481964 - 17.74 s\n",
      "Epoch 2177/3000, 60/60 - D Loss: 2.7263245520750892e-09, G Loss: 0.18863600492477417 - 17.76 s\n",
      "Epoch 2178/3000, 60/60 - D Loss: 2.778458037944317e-09, G Loss: 0.18847863376140594 - 17.52 s\n",
      "Epoch 2179/3000, 60/60 - D Loss: 2.6142618561196453e-09, G Loss: 0.18834301829338074 - 17.32 s\n",
      "Epoch 2180/3000, 60/60 - D Loss: 2.0635215213806162e-09, G Loss: 0.18904966115951538 - 17.55 s\n",
      "Epoch 2181/3000, 60/60 - D Loss: 1.8231758841622415e-09, G Loss: 0.18830356001853943 - 17.68 s\n",
      "Epoch 2182/3000, 60/60 - D Loss: 1.995204169604912e-09, G Loss: 0.18822796642780304 - 17.74 s\n",
      "Epoch 2183/3000, 60/60 - D Loss: 1.994732746384356e-09, G Loss: 0.18933944404125214 - 17.75 s\n",
      "Epoch 2184/3000, 60/60 - D Loss: 1.839812961267756e-09, G Loss: 0.18937431275844574 - 17.69 s\n",
      "Epoch 2185/3000, 60/60 - D Loss: 1.8511140582617297e-09, G Loss: 0.19243541359901428 - 17.64 s\n",
      "Epoch 2186/3000, 60/60 - D Loss: 2.0483592787576147e-09, G Loss: 0.18789653480052948 - 17.89 s\n",
      "Epoch 2187/3000, 60/60 - D Loss: 1.985485731040163e-09, G Loss: 0.18811331689357758 - 17.84 s\n",
      "Epoch 2188/3000, 60/60 - D Loss: 1.870531273195095e-09, G Loss: 0.1891496628522873 - 17.67 s\n",
      "Epoch 2189/3000, 60/60 - D Loss: 1.659430878739852e-09, G Loss: 0.18875902891159058 - 17.78 s\n",
      "Epoch 2190/3000, 60/60 - D Loss: 1.7881092472985622e-09, G Loss: 0.19019562005996704 - 17.62 s\n",
      "Epoch 2191/3000, 60/60 - D Loss: 1.5033055249041253e-09, G Loss: 0.18862377107143402 - 17.86 s\n",
      "Epoch 2192/3000, 60/60 - D Loss: 1.4509978792895699e-09, G Loss: 0.18891778588294983 - 17.52 s\n",
      "Epoch 2193/3000, 60/60 - D Loss: 1.4603495137601678e-09, G Loss: 0.1886330097913742 - 17.62 s\n",
      "Epoch 2194/3000, 60/60 - D Loss: 1.3821112869973257e-09, G Loss: 0.1886330395936966 - 17.96 s\n",
      "Epoch 2195/3000, 60/60 - D Loss: 1.4762884367253886e-09, G Loss: 0.1899288147687912 - 17.87 s\n",
      "Epoch 2196/3000, 60/60 - D Loss: 1.4279985298214616e-09, G Loss: 0.18985939025878906 - 17.98 s\n",
      "Epoch 2197/3000, 60/60 - D Loss: 1.4220905846543076e-09, G Loss: 0.18884263932704926 - 17.83 s\n",
      "Epoch 2198/3000, 60/60 - D Loss: 1.4512690492982181e-09, G Loss: 0.18892934918403625 - 17.79 s\n",
      "Epoch 2199/3000, 60/60 - D Loss: 1.3757050558812654e-09, G Loss: 0.1892949491739273 - 17.77 s\n",
      "Epoch 2200/3000, 60/60 - D Loss: 1.125719285312559e-09, G Loss: 0.19178658723831177 - 17.59 s\n",
      "Epoch 2201/3000, 60/60 - D Loss: 1.201055965688075e-09, G Loss: 0.18966040015220642 - 17.65 s\n",
      "Epoch 2202/3000, 60/60 - D Loss: 1.1358396112155496e-09, G Loss: 0.18860188126564026 - 17.67 s\n",
      "Epoch 2203/3000, 60/60 - D Loss: 1.2500929992838414e-09, G Loss: 0.1882953643798828 - 17.72 s\n",
      "Epoch 2204/3000, 60/60 - D Loss: 1.043374598829925e-09, G Loss: 0.18946070969104767 - 17.47 s\n",
      "Epoch 2205/3000, 60/60 - D Loss: 1.1079919869574494e-09, G Loss: 0.18854506313800812 - 17.51 s\n",
      "Epoch 2206/3000, 60/60 - D Loss: 1.0710234308317686e-09, G Loss: 0.18878836929798126 - 17.66 s\n",
      "Epoch 2207/3000, 60/60 - D Loss: 9.963788385463778e-10, G Loss: 0.18844880163669586 - 18.12 s\n",
      "Epoch 2208/3000, 60/60 - D Loss: 9.643494550775863e-10, G Loss: 0.1883922964334488 - 17.55 s\n",
      "Epoch 2209/3000, 60/60 - D Loss: 8.37887582792507e-10, G Loss: 0.18897904455661774 - 17.45 s\n",
      "Epoch 2210/3000, 60/60 - D Loss: 8.524612399742721e-10, G Loss: 0.18844376504421234 - 17.87 s\n",
      "Epoch 2211/3000, 60/60 - D Loss: 8.62728346536179e-10, G Loss: 0.18816177546977997 - 17.76 s\n",
      "Epoch 2212/3000, 60/60 - D Loss: 9.437005207600733e-10, G Loss: 0.1885371208190918 - 17.60 s\n",
      "Epoch 2213/3000, 60/60 - D Loss: 8.85229863278994e-10, G Loss: 0.18931028246879578 - 17.64 s\n",
      "Epoch 2214/3000, 60/60 - D Loss: 9.297156435592957e-10, G Loss: 0.18781165778636932 - 17.58 s\n",
      "Epoch 2215/3000, 60/60 - D Loss: 8.350055448309531e-10, G Loss: 0.18861816823482513 - 17.51 s\n",
      "Epoch 2216/3000, 60/60 - D Loss: 9.58966642859981e-10, G Loss: 0.18827125430107117 - 17.61 s\n",
      "Epoch 2217/3000, 60/60 - D Loss: 7.72757291326204e-10, G Loss: 0.18806283175945282 - 17.91 s\n",
      "Epoch 2218/3000, 60/60 - D Loss: 8.673034698296323e-10, G Loss: 0.18901458382606506 - 17.78 s\n",
      "Epoch 2219/3000, 60/60 - D Loss: 7.117405346053441e-10, G Loss: 0.18810124695301056 - 17.79 s\n",
      "Epoch 2220/3000, 60/60 - D Loss: 6.995129972548522e-10, G Loss: 0.19070814549922943 - 17.84 s\n",
      "Epoch 2221/3000, 60/60 - D Loss: 7.150226145888065e-10, G Loss: 0.18836061656475067 - 18.00 s\n",
      "Epoch 2222/3000, 60/60 - D Loss: 7.498638016353523e-10, G Loss: 0.18841496109962463 - 17.91 s\n",
      "Epoch 2223/3000, 60/60 - D Loss: 7.228387367508397e-10, G Loss: 0.18859341740608215 - 17.66 s\n",
      "Epoch 2224/3000, 60/60 - D Loss: 7.490395424597632e-10, G Loss: 0.18856383860111237 - 17.65 s\n",
      "Epoch 2225/3000, 60/60 - D Loss: 7.179493033136514e-10, G Loss: 0.18998782336711884 - 18.26 s\n",
      "Epoch 2226/3000, 60/60 - D Loss: 8.000491240417543e-10, G Loss: 0.1882200837135315 - 17.58 s\n",
      "Epoch 2227/3000, 60/60 - D Loss: 7.458678117712762e-10, G Loss: 0.190998375415802 - 17.79 s\n",
      "Epoch 2228/3000, 60/60 - D Loss: 7.73168029360839e-10, G Loss: 0.18860264122486115 - 17.68 s\n",
      "Epoch 2229/3000, 60/60 - D Loss: 7.30229680735939e-10, G Loss: 0.18977223336696625 - 17.61 s\n",
      "Epoch 2230/3000, 60/60 - D Loss: 8.286189767343422e-10, G Loss: 0.1894608587026596 - 17.92 s\n",
      "Epoch 2231/3000, 60/60 - D Loss: 7.493459237432253e-10, G Loss: 0.18841682374477386 - 17.86 s\n",
      "Epoch 2232/3000, 60/60 - D Loss: 6.88372133725762e-10, G Loss: 0.18832853436470032 - 17.64 s\n",
      "Epoch 2233/3000, 60/60 - D Loss: 6.687922609870811e-10, G Loss: 0.1881898045539856 - 17.80 s\n",
      "Epoch 2234/3000, 60/60 - D Loss: 7.514492292744733e-10, G Loss: 0.1880825310945511 - 17.73 s\n",
      "Epoch 2235/3000, 60/60 - D Loss: 6.921732202702126e-10, G Loss: 0.18932655453681946 - 17.65 s\n",
      "Epoch 2236/3000, 60/60 - D Loss: 7.526745337104991e-10, G Loss: 0.1894066333770752 - 17.66 s\n",
      "Epoch 2237/3000, 60/60 - D Loss: 8.052230036812593e-10, G Loss: 0.18886923789978027 - 17.72 s\n",
      "Epoch 2238/3000, 60/60 - D Loss: 7.585640728110518e-10, G Loss: 0.18867561221122742 - 17.83 s\n",
      "Epoch 2239/3000, 60/60 - D Loss: 7.102991415494067e-10, G Loss: 0.1877342164516449 - 17.73 s\n",
      "Epoch 2240/3000, 60/60 - D Loss: 7.262774447975163e-10, G Loss: 0.19467788934707642 - 17.97 s\n",
      "Epoch 2241/3000, 60/60 - D Loss: 7.906164483141112e-10, G Loss: 0.18824829161167145 - 17.81 s\n",
      "Epoch 2242/3000, 60/60 - D Loss: 7.655744840749998e-10, G Loss: 0.1884349286556244 - 17.58 s\n",
      "Epoch 2243/3000, 60/60 - D Loss: 7.627238640460732e-10, G Loss: 0.1887630820274353 - 17.56 s\n",
      "Epoch 2244/3000, 60/60 - D Loss: 7.610913364817755e-10, G Loss: 0.1896100640296936 - 17.74 s\n",
      "Epoch 2245/3000, 60/60 - D Loss: 8.269341296186298e-10, G Loss: 0.18838812410831451 - 17.64 s\n",
      "Epoch 2246/3000, 60/60 - D Loss: 6.976054639160745e-10, G Loss: 0.1886727511882782 - 17.47 s\n",
      "Epoch 2247/3000, 60/60 - D Loss: 7.691093614862625e-10, G Loss: 0.18873366713523865 - 17.79 s\n",
      "Epoch 2248/3000, 60/60 - D Loss: 6.914320690824138e-10, G Loss: 0.19333666563034058 - 17.88 s\n",
      "Epoch 2249/3000, 60/60 - D Loss: 7.650485665001972e-10, G Loss: 0.18836075067520142 - 17.99 s\n",
      "Epoch 2250/3000, 60/60 - D Loss: 7.429359842144308e-10, G Loss: 0.18832530081272125 - 17.70 s\n",
      "Epoch 2251/3000, 60/60 - D Loss: 7.809397061117078e-10, G Loss: 0.18767902255058289 - 17.82 s\n",
      "Epoch 2252/3000, 60/60 - D Loss: 7.292559300859882e-10, G Loss: 0.1883823275566101 - 17.68 s\n",
      "Epoch 2253/3000, 60/60 - D Loss: 7.225099582755493e-10, G Loss: 0.1878678947687149 - 17.74 s\n",
      "Epoch 2254/3000, 60/60 - D Loss: 8.074737662727601e-10, G Loss: 0.18815504014492035 - 17.85 s\n",
      "Epoch 2255/3000, 60/60 - D Loss: 6.928380318784155e-10, G Loss: 0.18824931979179382 - 18.14 s\n",
      "Epoch 2256/3000, 60/60 - D Loss: 7.583220724105861e-10, G Loss: 0.18986858427524567 - 17.77 s\n",
      "Epoch 2257/3000, 60/60 - D Loss: 6.965918873575073e-10, G Loss: 0.1884421855211258 - 17.72 s\n",
      "Epoch 2258/3000, 60/60 - D Loss: 7.313367185677049e-10, G Loss: 0.18766354024410248 - 17.66 s\n",
      "Epoch 2259/3000, 60/60 - D Loss: 6.8566858782641e-10, G Loss: 0.18848475813865662 - 17.81 s\n",
      "Epoch 2260/3000, 60/60 - D Loss: 7.503131770092666e-10, G Loss: 0.18814070522785187 - 17.77 s\n",
      "Epoch 2261/3000, 60/60 - D Loss: 6.959567338278352e-10, G Loss: 0.18998312950134277 - 17.91 s\n",
      "Epoch 2262/3000, 60/60 - D Loss: 7.557889227430638e-10, G Loss: 0.19298557937145233 - 17.75 s\n",
      "Epoch 2263/3000, 60/60 - D Loss: 6.305918571619255e-10, G Loss: 0.18873213231563568 - 17.67 s\n",
      "Epoch 2264/3000, 60/60 - D Loss: 6.760655817157019e-10, G Loss: 0.18817643821239471 - 17.71 s\n",
      "Epoch 2265/3000, 60/60 - D Loss: 6.334251902733086e-10, G Loss: 0.18814204633235931 - 17.69 s\n",
      "Epoch 2266/3000, 60/60 - D Loss: 6.730317718532166e-10, G Loss: 0.1886291801929474 - 17.85 s\n",
      "Epoch 2267/3000, 60/60 - D Loss: 6.241897881895851e-10, G Loss: 0.1883932501077652 - 17.73 s\n",
      "Epoch 2268/3000, 60/60 - D Loss: 6.603386390086062e-10, G Loss: 0.1888478696346283 - 17.63 s\n",
      "Epoch 2269/3000, 60/60 - D Loss: 6.233248128966219e-10, G Loss: 0.18945561349391937 - 17.65 s\n",
      "Epoch 2270/3000, 60/60 - D Loss: 7.074793194859763e-10, G Loss: 0.19016557931900024 - 17.95 s\n",
      "Epoch 2271/3000, 60/60 - D Loss: 6.594685558956363e-10, G Loss: 0.18880857527256012 - 17.64 s\n",
      "Epoch 2272/3000, 60/60 - D Loss: 5.915225671715635e-10, G Loss: 0.1879865676164627 - 17.71 s\n",
      "Epoch 2273/3000, 60/60 - D Loss: 6.300542792004927e-10, G Loss: 0.18826647102832794 - 17.58 s\n",
      "Epoch 2274/3000, 60/60 - D Loss: 6.143234006749747e-10, G Loss: 0.1894691437482834 - 17.69 s\n",
      "Epoch 2275/3000, 60/60 - D Loss: 5.727331025703154e-10, G Loss: 0.18674635887145996 - 17.62 s\n",
      "Epoch 2276/3000, 60/60 - D Loss: 6.230690836887384e-10, G Loss: 0.18808922171592712 - 17.60 s\n",
      "Epoch 2277/3000, 60/60 - D Loss: 5.709318395656637e-10, G Loss: 0.18915213644504547 - 17.84 s\n",
      "Epoch 2278/3000, 60/60 - D Loss: 5.673986566118365e-10, G Loss: 0.18854913115501404 - 17.79 s\n",
      "Epoch 2279/3000, 60/60 - D Loss: 5.929203203298464e-10, G Loss: 0.18807388842105865 - 17.60 s\n",
      "Epoch 2280/3000, 60/60 - D Loss: 5.619296851576545e-10, G Loss: 0.1884225457906723 - 17.63 s\n",
      "Epoch 2281/3000, 60/60 - D Loss: 5.788721350823332e-10, G Loss: 0.18828511238098145 - 17.87 s\n",
      "Epoch 2282/3000, 60/60 - D Loss: 5.761565823575461e-10, G Loss: 0.19232405722141266 - 17.96 s\n",
      "Epoch 2283/3000, 60/60 - D Loss: 5.95112584255053e-10, G Loss: 0.18791967630386353 - 17.76 s\n",
      "Epoch 2284/3000, 60/60 - D Loss: 5.636125627317116e-10, G Loss: 0.18791715800762177 - 17.78 s\n",
      "Epoch 2285/3000, 60/60 - D Loss: 5.164047712692772e-10, G Loss: 0.18939109146595 - 17.65 s\n",
      "Epoch 2286/3000, 60/60 - D Loss: 5.068133838159095e-10, G Loss: 0.18692436814308167 - 18.02 s\n",
      "Epoch 2287/3000, 60/60 - D Loss: 5.424198165473484e-10, G Loss: 0.18808871507644653 - 17.54 s\n",
      "Epoch 2288/3000, 60/60 - D Loss: 5.165957941488593e-10, G Loss: 0.1922159641981125 - 17.87 s\n",
      "Epoch 2289/3000, 60/60 - D Loss: 4.993410765911307e-10, G Loss: 0.18824060261249542 - 17.77 s\n",
      "Epoch 2290/3000, 60/60 - D Loss: 4.875763306703018e-10, G Loss: 0.18716296553611755 - 17.87 s\n",
      "Epoch 2291/3000, 60/60 - D Loss: 5.10431317322442e-10, G Loss: 0.18874309957027435 - 17.79 s\n",
      "Epoch 2292/3000, 60/60 - D Loss: 5.025050788911852e-10, G Loss: 0.18909089267253876 - 17.63 s\n",
      "Epoch 2293/3000, 60/60 - D Loss: 5.113953012974285e-10, G Loss: 0.18877139687538147 - 17.82 s\n",
      "Epoch 2294/3000, 60/60 - D Loss: 5.267849635118353e-10, G Loss: 0.1888965666294098 - 17.50 s\n",
      "Epoch 2295/3000, 60/60 - D Loss: 4.934271805306371e-10, G Loss: 0.18854165077209473 - 17.69 s\n",
      "Epoch 2296/3000, 60/60 - D Loss: 4.752748233602001e-10, G Loss: 0.18810947239398956 - 17.57 s\n",
      "Epoch 2297/3000, 60/60 - D Loss: 4.981785953796632e-10, G Loss: 0.1894092559814453 - 17.46 s\n",
      "Epoch 2298/3000, 60/60 - D Loss: 4.36753628855309e-10, G Loss: 0.18925662338733673 - 17.55 s\n",
      "Epoch 2299/3000, 60/60 - D Loss: 3.906694120630632e-10, G Loss: 0.18892306089401245 - 17.77 s\n",
      "Epoch 2300/3000, 60/60 - D Loss: 3.9865743466787643e-10, G Loss: 0.18823541700839996 - 17.62 s\n",
      "Epoch 2301/3000, 60/60 - D Loss: 4.138852486024753e-10, G Loss: 0.1891166865825653 - 17.84 s\n",
      "Epoch 2302/3000, 60/60 - D Loss: 4.205468974975243e-10, G Loss: 0.1886034458875656 - 17.65 s\n",
      "Epoch 2303/3000, 60/60 - D Loss: 4.3591405506531787e-10, G Loss: 0.1889389455318451 - 17.58 s\n",
      "Epoch 2304/3000, 60/60 - D Loss: 4.443778902971849e-10, G Loss: 0.1881604641675949 - 17.75 s\n",
      "Epoch 2305/3000, 60/60 - D Loss: 1.2899530190679798e-06, G Loss: 0.18840639293193817 - 17.65 s\n",
      "Epoch 2306/3000, 60/60 - D Loss: 0.016222399630350992, G Loss: 0.18833774328231812 - 17.60 s\n",
      "Epoch 2307/3000, 60/60 - D Loss: 0.005129254117491655, G Loss: 0.19162653386592865 - 17.58 s\n",
      "Epoch 2308/3000, 60/60 - D Loss: 0.005199539344175719, G Loss: 0.1884889453649521 - 17.56 s\n",
      "Epoch 2309/3000, 60/60 - D Loss: 0.004096200806088746, G Loss: 0.18912538886070251 - 17.53 s\n",
      "Epoch 2310/3000, 60/60 - D Loss: 0.004517046461842256, G Loss: 0.1881190687417984 - 17.72 s\n",
      "Epoch 2311/3000, 60/60 - D Loss: 0.003667705022962764, G Loss: 0.18785816431045532 - 17.81 s\n",
      "Epoch 2312/3000, 60/60 - D Loss: 0.0030895316958776675, G Loss: 0.18818967044353485 - 17.93 s\n",
      "Epoch 2313/3000, 60/60 - D Loss: 0.0028097999638703186, G Loss: 0.18591763079166412 - 17.88 s\n",
      "Epoch 2314/3000, 60/60 - D Loss: 0.0027525520890776534, G Loss: 0.18835090100765228 - 17.67 s\n",
      "Epoch 2315/3000, 60/60 - D Loss: 0.0023808431014913367, G Loss: 0.18975143134593964 - 17.80 s\n",
      "Epoch 2316/3000, 60/60 - D Loss: 0.002175696649828751, G Loss: 0.18786044418811798 - 17.86 s\n",
      "Epoch 2317/3000, 60/60 - D Loss: 0.0013716637313336832, G Loss: 0.18813228607177734 - 17.68 s\n",
      "Epoch 2318/3000, 60/60 - D Loss: 0.001166325939266244, G Loss: 0.1874101608991623 - 17.57 s\n",
      "Epoch 2319/3000, 60/60 - D Loss: 0.001154250757281261, G Loss: 0.1881708800792694 - 17.93 s\n",
      "Epoch 2320/3000, 60/60 - D Loss: 0.0006799828397561214, G Loss: 0.18755167722702026 - 17.69 s\n",
      "Epoch 2321/3000, 60/60 - D Loss: 0.0005364330595512001, G Loss: 0.1886044591665268 - 17.67 s\n",
      "Epoch 2322/3000, 60/60 - D Loss: 0.0005297067482388229, G Loss: 0.1884429007768631 - 17.90 s\n",
      "Epoch 2323/3000, 60/60 - D Loss: 0.000606793464612565, G Loss: 0.18987244367599487 - 17.43 s\n",
      "Epoch 2324/3000, 60/60 - D Loss: 0.00038351664534275187, G Loss: 0.18781085312366486 - 17.79 s\n",
      "Epoch 2325/3000, 60/60 - D Loss: 0.0004314913030611933, G Loss: 0.18801766633987427 - 17.90 s\n",
      "Epoch 2326/3000, 60/60 - D Loss: 0.00029564835404016776, G Loss: 0.18848589062690735 - 17.63 s\n",
      "Epoch 2327/3000, 60/60 - D Loss: 0.0002467946665092313, G Loss: 0.18911948800086975 - 17.67 s\n",
      "Epoch 2328/3000, 60/60 - D Loss: 0.00020921607188029157, G Loss: 0.18838556110858917 - 17.47 s\n",
      "Epoch 2329/3000, 60/60 - D Loss: 0.00017787453020901012, G Loss: 0.18893299996852875 - 17.72 s\n",
      "Epoch 2330/3000, 60/60 - D Loss: 0.00022183457008395635, G Loss: 0.18731436133384705 - 18.08 s\n",
      "Epoch 2331/3000, 60/60 - D Loss: 0.0001333804352725565, G Loss: 0.18924587965011597 - 17.91 s\n",
      "Epoch 2332/3000, 60/60 - D Loss: 0.00012769529030265403, G Loss: 0.18959996104240417 - 17.71 s\n",
      "Epoch 2333/3000, 60/60 - D Loss: 0.00010494885987100133, G Loss: 0.18856333196163177 - 17.58 s\n",
      "Epoch 2334/3000, 60/60 - D Loss: 0.00012960306685272371, G Loss: 0.18852554261684418 - 17.68 s\n",
      "Epoch 2335/3000, 60/60 - D Loss: 7.526461695306352e-05, G Loss: 0.18816636502742767 - 17.72 s\n",
      "Epoch 2336/3000, 60/60 - D Loss: 0.00011648931354102388, G Loss: 0.18897856771945953 - 17.60 s\n",
      "Epoch 2337/3000, 60/60 - D Loss: 6.876133579680754e-05, G Loss: 0.1876680850982666 - 17.84 s\n",
      "Epoch 2338/3000, 60/60 - D Loss: 8.250379170249289e-05, G Loss: 0.18862126767635345 - 17.60 s\n",
      "Epoch 2339/3000, 60/60 - D Loss: 5.426142047326721e-05, G Loss: 0.18851684033870697 - 17.68 s\n",
      "Epoch 2340/3000, 60/60 - D Loss: 5.304084618273919e-05, G Loss: 0.18800534307956696 - 17.71 s\n",
      "Epoch 2341/3000, 60/60 - D Loss: 3.869281601964758e-05, G Loss: 0.18898701667785645 - 17.62 s\n",
      "Epoch 2342/3000, 60/60 - D Loss: 3.760319850698579e-05, G Loss: 0.19055840373039246 - 17.51 s\n",
      "Epoch 2343/3000, 60/60 - D Loss: 4.185089397878983e-05, G Loss: 0.1882033795118332 - 17.55 s\n",
      "Epoch 2344/3000, 60/60 - D Loss: 3.487190593887135e-05, G Loss: 0.1891292929649353 - 17.77 s\n",
      "Epoch 2345/3000, 60/60 - D Loss: 3.5445901517050515e-05, G Loss: 0.18886062502861023 - 17.49 s\n",
      "Epoch 2346/3000, 60/60 - D Loss: 2.7453956533918245e-05, G Loss: 0.18877443671226501 - 17.79 s\n",
      "Epoch 2347/3000, 60/60 - D Loss: 2.8056099665718648e-05, G Loss: 0.18877729773521423 - 17.91 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2348/3000, 60/60 - D Loss: 3.070909619395934e-05, G Loss: 0.18844643235206604 - 17.76 s\n",
      "Epoch 2349/3000, 60/60 - D Loss: 2.3889044115321667e-05, G Loss: 0.18830230832099915 - 17.88 s\n",
      "Epoch 2350/3000, 60/60 - D Loss: 2.0157386728669735e-05, G Loss: 0.189915731549263 - 17.61 s\n",
      "Epoch 2351/3000, 60/60 - D Loss: 1.9206540287086682e-05, G Loss: 0.18951615691184998 - 17.84 s\n",
      "Epoch 2352/3000, 60/60 - D Loss: 1.4773372498666504e-05, G Loss: 0.18828897178173065 - 17.63 s\n",
      "Epoch 2353/3000, 60/60 - D Loss: 1.8032500605613677e-05, G Loss: 0.1889525204896927 - 17.76 s\n",
      "Epoch 2354/3000, 60/60 - D Loss: 1.819863834384705e-05, G Loss: 0.18828894197940826 - 17.57 s\n",
      "Epoch 2355/3000, 60/60 - D Loss: 1.104738633728175e-05, G Loss: 0.18907544016838074 - 17.66 s\n",
      "Epoch 2356/3000, 60/60 - D Loss: 7.946894797328241e-06, G Loss: 0.18853794038295746 - 17.78 s\n",
      "Epoch 2357/3000, 60/60 - D Loss: 7.613549897200755e-06, G Loss: 0.1884279102087021 - 17.70 s\n",
      "Epoch 2358/3000, 60/60 - D Loss: 6.388396634804394e-06, G Loss: 0.18833360075950623 - 17.60 s\n",
      "Epoch 2359/3000, 60/60 - D Loss: 7.311444761626262e-06, G Loss: 0.18676503002643585 - 17.90 s\n",
      "Epoch 2360/3000, 60/60 - D Loss: 4.305684221606043e-06, G Loss: 0.1889740377664566 - 17.91 s\n",
      "Epoch 2361/3000, 60/60 - D Loss: 4.707812621518315e-06, G Loss: 0.20152373611927032 - 17.80 s\n",
      "Epoch 2362/3000, 60/60 - D Loss: 3.326934855607533e-06, G Loss: 0.18852688372135162 - 17.79 s\n",
      "Epoch 2363/3000, 60/60 - D Loss: 4.971519864227503e-06, G Loss: 0.1888783723115921 - 17.57 s\n",
      "Epoch 2364/3000, 60/60 - D Loss: 3.65536342883388e-06, G Loss: 0.1888047605752945 - 17.72 s\n",
      "Epoch 2365/3000, 60/60 - D Loss: 3.428464083299332e-06, G Loss: 0.18793202936649323 - 17.60 s\n",
      "Epoch 2366/3000, 60/60 - D Loss: 3.7992518002738507e-06, G Loss: 0.18848466873168945 - 17.54 s\n",
      "Epoch 2367/3000, 60/60 - D Loss: 2.3977751375525713e-06, G Loss: 0.18827572464942932 - 17.85 s\n",
      "Epoch 2368/3000, 60/60 - D Loss: 2.73021317553912e-06, G Loss: 0.18845385313034058 - 17.44 s\n",
      "Epoch 2369/3000, 60/60 - D Loss: 8.166272650100836e-06, G Loss: 0.18827295303344727 - 17.65 s\n",
      "Epoch 2370/3000, 60/60 - D Loss: 1.3403923624366598e-06, G Loss: 0.18847085535526276 - 17.65 s\n",
      "Epoch 2371/3000, 60/60 - D Loss: 1.4186128787230246e-06, G Loss: 0.1880509853363037 - 17.71 s\n",
      "Epoch 2372/3000, 60/60 - D Loss: 1.2687638459851769e-06, G Loss: 0.18993809819221497 - 17.57 s\n",
      "Epoch 2373/3000, 60/60 - D Loss: 1.1712490408655185e-06, G Loss: 0.18812556564807892 - 17.60 s\n",
      "Epoch 2374/3000, 60/60 - D Loss: 9.765573256714788e-07, G Loss: 0.18849359452724457 - 17.83 s\n",
      "Epoch 2375/3000, 60/60 - D Loss: 1.0789945257272393e-06, G Loss: 0.18752282857894897 - 17.58 s\n",
      "Epoch 2376/3000, 60/60 - D Loss: 9.165458543236582e-07, G Loss: 0.18878503143787384 - 17.77 s\n",
      "Epoch 2377/3000, 60/60 - D Loss: 8.663217929694156e-07, G Loss: 0.18898265063762665 - 18.07 s\n",
      "Epoch 2378/3000, 60/60 - D Loss: 7.161631589980288e-07, G Loss: 0.1891414225101471 - 17.75 s\n",
      "Epoch 2379/3000, 60/60 - D Loss: 3.834321468332291e-06, G Loss: 0.18980947136878967 - 17.67 s\n",
      "Epoch 2380/3000, 60/60 - D Loss: 1.114046335715102e-05, G Loss: 0.18845821917057037 - 17.81 s\n",
      "Epoch 2381/3000, 60/60 - D Loss: 8.827355308937967e-07, G Loss: 0.18850816786289215 - 17.70 s\n",
      "Epoch 2382/3000, 60/60 - D Loss: 6.472297089388235e-07, G Loss: 0.18801896274089813 - 17.45 s\n",
      "Epoch 2383/3000, 60/60 - D Loss: 2.332910305824498e-06, G Loss: 0.1881224513053894 - 17.83 s\n",
      "Epoch 2384/3000, 60/60 - D Loss: 4.910027060489597e-07, G Loss: 0.1883840262889862 - 17.74 s\n",
      "Epoch 2385/3000, 60/60 - D Loss: 9.910557405845566e-07, G Loss: 0.18651512265205383 - 17.84 s\n",
      "Epoch 2386/3000, 60/60 - D Loss: 3.750841588434639e-07, G Loss: 0.18827788531780243 - 17.80 s\n",
      "Epoch 2387/3000, 60/60 - D Loss: 4.909241351214177e-07, G Loss: 0.18898890912532806 - 17.54 s\n",
      "Epoch 2388/3000, 60/60 - D Loss: 2.4016775457269546e-07, G Loss: 0.1893417239189148 - 17.95 s\n",
      "Epoch 2389/3000, 60/60 - D Loss: 5.555806543711128e-07, G Loss: 0.18951697647571564 - 17.70 s\n",
      "Epoch 2390/3000, 60/60 - D Loss: 3.213186059447537e-07, G Loss: 0.18887820839881897 - 17.79 s\n",
      "Epoch 2391/3000, 60/60 - D Loss: 3.7454632417777134e-07, G Loss: 0.1888122409582138 - 17.73 s\n",
      "Epoch 2392/3000, 60/60 - D Loss: 3.763909817156019e-07, G Loss: 0.19118525087833405 - 17.88 s\n",
      "Epoch 2393/3000, 60/60 - D Loss: 2.5786002133543207e-07, G Loss: 0.1880418360233307 - 17.69 s\n",
      "Epoch 2394/3000, 60/60 - D Loss: 3.028890205314383e-07, G Loss: 0.18966886401176453 - 17.66 s\n",
      "Epoch 2395/3000, 60/60 - D Loss: 2.3134635585009278e-07, G Loss: 0.18858405947685242 - 17.42 s\n",
      "Epoch 2396/3000, 60/60 - D Loss: 1.7750220027146213e-07, G Loss: 0.1872512251138687 - 17.78 s\n",
      "Epoch 2397/3000, 60/60 - D Loss: 2.1757039214698182e-07, G Loss: 0.1873457431793213 - 17.87 s\n",
      "Epoch 2398/3000, 60/60 - D Loss: 1.765358399463679e-07, G Loss: 0.18791761994361877 - 17.70 s\n",
      "Epoch 2399/3000, 60/60 - D Loss: 1.3956500954925843e-07, G Loss: 0.18839266896247864 - 17.71 s\n",
      "Epoch 2400/3000, 60/60 - D Loss: 1.606056858616256e-07, G Loss: 0.18889077007770538 - 17.50 s\n",
      "Epoch 2401/3000, 60/60 - D Loss: 1.6632625210455387e-07, G Loss: 0.18861635029315948 - 17.77 s\n",
      "Epoch 2402/3000, 60/60 - D Loss: 1.2179683306356992e-07, G Loss: 0.18939517438411713 - 17.73 s\n",
      "Epoch 2403/3000, 60/60 - D Loss: 1.2056355980227806e-07, G Loss: 0.1887662410736084 - 17.60 s\n",
      "Epoch 2404/3000, 60/60 - D Loss: 9.984003690455268e-08, G Loss: 0.18791529536247253 - 17.74 s\n",
      "Epoch 2405/3000, 60/60 - D Loss: 9.951127610818844e-08, G Loss: 0.1882566511631012 - 17.79 s\n",
      "Epoch 2406/3000, 60/60 - D Loss: 1.0062467858951152e-07, G Loss: 0.18757383525371552 - 17.78 s\n",
      "Epoch 2407/3000, 60/60 - D Loss: 6.622356146790409e-08, G Loss: 0.18798677623271942 - 18.03 s\n",
      "Epoch 2408/3000, 60/60 - D Loss: 8.608005763743876e-08, G Loss: 0.18772928416728973 - 17.84 s\n",
      "Epoch 2409/3000, 60/60 - D Loss: 9.06881617579458e-08, G Loss: 0.18876515328884125 - 17.63 s\n",
      "Epoch 2410/3000, 60/60 - D Loss: 8.237830340096508e-08, G Loss: 0.18803419172763824 - 17.68 s\n",
      "Epoch 2411/3000, 60/60 - D Loss: 6.016478620862742e-08, G Loss: 0.18880265951156616 - 17.67 s\n",
      "Epoch 2412/3000, 60/60 - D Loss: 6.669882013365935e-08, G Loss: 0.18771043419837952 - 17.58 s\n",
      "Epoch 2413/3000, 60/60 - D Loss: 4.91643665778696e-08, G Loss: 0.18931815028190613 - 17.49 s\n",
      "Epoch 2414/3000, 60/60 - D Loss: 4.2734764671426007e-08, G Loss: 0.1889358013868332 - 17.94 s\n",
      "Epoch 2415/3000, 60/60 - D Loss: 5.387633916775769e-08, G Loss: 0.1885693371295929 - 17.77 s\n",
      "Epoch 2416/3000, 60/60 - D Loss: 4.2986800008470993e-08, G Loss: 0.18760313093662262 - 17.65 s\n",
      "Epoch 2417/3000, 60/60 - D Loss: 4.345335874456069e-08, G Loss: 0.18922549486160278 - 17.61 s\n",
      "Epoch 2418/3000, 60/60 - D Loss: 4.234119976853634e-08, G Loss: 0.18814274668693542 - 17.54 s\n",
      "Epoch 2419/3000, 60/60 - D Loss: 4.191206720571827e-08, G Loss: 0.18782848119735718 - 17.53 s\n",
      "Epoch 2420/3000, 60/60 - D Loss: 4.5826153705075257e-08, G Loss: 0.1883552074432373 - 17.41 s\n",
      "Epoch 2421/3000, 60/60 - D Loss: 3.712551915890927e-08, G Loss: 0.18819551169872284 - 17.62 s\n",
      "Epoch 2422/3000, 60/60 - D Loss: 4.2189609528175964e-08, G Loss: 0.1881263107061386 - 17.57 s\n",
      "Epoch 2423/3000, 60/60 - D Loss: 3.534038234276338e-08, G Loss: 0.18913309276103973 - 17.92 s\n",
      "Epoch 2424/3000, 60/60 - D Loss: 3.638298379238236e-08, G Loss: 0.18890100717544556 - 17.63 s\n",
      "Epoch 2425/3000, 60/60 - D Loss: 4.000791076208543e-08, G Loss: 0.18993191421031952 - 17.59 s\n",
      "Epoch 2426/3000, 60/60 - D Loss: 2.6186239454606408e-08, G Loss: 0.18802529573440552 - 17.77 s\n",
      "Epoch 2427/3000, 60/60 - D Loss: 2.3807567861577894e-08, G Loss: 0.18775716423988342 - 17.58 s\n",
      "Epoch 2428/3000, 60/60 - D Loss: 2.6399442937252715e-08, G Loss: 0.1872531920671463 - 17.48 s\n",
      "Epoch 2429/3000, 60/60 - D Loss: 3.341346574425863e-08, G Loss: 0.18892642855644226 - 17.42 s\n",
      "Epoch 2430/3000, 60/60 - D Loss: 2.598982769275171e-08, G Loss: 0.1930074840784073 - 17.55 s\n",
      "Epoch 2431/3000, 60/60 - D Loss: 1.9087235916814826e-08, G Loss: 0.1888747662305832 - 17.61 s\n",
      "Epoch 2432/3000, 60/60 - D Loss: 1.9126430154947638e-08, G Loss: 0.1885235458612442 - 17.69 s\n",
      "Epoch 2433/3000, 60/60 - D Loss: 2.359522625894339e-08, G Loss: 0.1892041265964508 - 17.70 s\n",
      "Epoch 2434/3000, 60/60 - D Loss: 1.1838232436001128e-08, G Loss: 0.1880028247833252 - 17.57 s\n",
      "Epoch 2435/3000, 60/60 - D Loss: 1.1810261671407751e-08, G Loss: 0.1887998729944229 - 17.55 s\n",
      "Epoch 2436/3000, 60/60 - D Loss: 1.4805429639219714e-08, G Loss: 0.18877911567687988 - 16.78 s\n",
      "Epoch 2437/3000, 60/60 - D Loss: 1.001453020163523e-08, G Loss: 0.18754905462265015 - 17.78 s\n",
      "Epoch 2438/3000, 60/60 - D Loss: 1.281430460353672e-08, G Loss: 0.1879696398973465 - 17.88 s\n",
      "Epoch 2439/3000, 60/60 - D Loss: 1.5804508080724533e-08, G Loss: 0.18797603249549866 - 17.60 s\n",
      "Epoch 2440/3000, 60/60 - D Loss: 1.164989825094831e-08, G Loss: 0.18851250410079956 - 17.52 s\n",
      "Epoch 2441/3000, 60/60 - D Loss: 9.554773465481059e-09, G Loss: 0.1878548115491867 - 17.54 s\n",
      "Epoch 2442/3000, 60/60 - D Loss: 9.532945037526996e-09, G Loss: 0.19605490565299988 - 17.52 s\n",
      "Epoch 2443/3000, 60/60 - D Loss: 1.0754707371191108e-08, G Loss: 0.1878461390733719 - 17.73 s\n",
      "Epoch 2444/3000, 60/60 - D Loss: 4.633641220719298e-09, G Loss: 0.1877632439136505 - 17.52 s\n",
      "Epoch 2445/3000, 60/60 - D Loss: 4.9450341061546554e-09, G Loss: 0.18741661310195923 - 17.69 s\n",
      "Epoch 2446/3000, 60/60 - D Loss: 7.733898493844649e-09, G Loss: 0.18797090649604797 - 17.74 s\n",
      "Epoch 2447/3000, 60/60 - D Loss: 6.415357517788878e-09, G Loss: 0.1881559044122696 - 17.62 s\n",
      "Epoch 2448/3000, 60/60 - D Loss: 6.598627562762194e-09, G Loss: 0.1897110790014267 - 17.84 s\n",
      "Epoch 2449/3000, 60/60 - D Loss: 6.163055026389697e-09, G Loss: 0.18841210007667542 - 17.47 s\n",
      "Epoch 2450/3000, 60/60 - D Loss: 6.044234781080182e-09, G Loss: 0.1890554130077362 - 17.66 s\n",
      "Epoch 2451/3000, 60/60 - D Loss: 3.928235428780902e-09, G Loss: 0.1879780888557434 - 17.71 s\n",
      "Epoch 2452/3000, 60/60 - D Loss: 4.328798530565947e-09, G Loss: 0.18851561844348907 - 17.75 s\n",
      "Epoch 2453/3000, 60/60 - D Loss: 4.060619114391872e-09, G Loss: 0.18909326195716858 - 17.92 s\n",
      "Epoch 2454/3000, 60/60 - D Loss: 5.2445465106160105e-09, G Loss: 0.1886451095342636 - 17.59 s\n",
      "Epoch 2455/3000, 60/60 - D Loss: 4.139930457475183e-09, G Loss: 0.18982508778572083 - 17.71 s\n",
      "Epoch 2456/3000, 60/60 - D Loss: 4.616538348649335e-09, G Loss: 0.18883468210697174 - 17.92 s\n",
      "Epoch 2457/3000, 60/60 - D Loss: 4.111270678931744e-09, G Loss: 0.18937639892101288 - 17.64 s\n",
      "Epoch 2458/3000, 60/60 - D Loss: 2.381416081692622e-09, G Loss: 0.18856686353683472 - 17.49 s\n",
      "Epoch 2459/3000, 60/60 - D Loss: 1.3426285387294068e-09, G Loss: 0.18819373846054077 - 17.72 s\n",
      "Epoch 2460/3000, 60/60 - D Loss: 4.159862049442353e-09, G Loss: 0.18896818161010742 - 17.70 s\n",
      "Epoch 2461/3000, 60/60 - D Loss: 3.6615562890723874e-09, G Loss: 0.18865622580051422 - 17.69 s\n",
      "Epoch 2462/3000, 60/60 - D Loss: 2.492738034216846e-09, G Loss: 0.1885039359331131 - 17.44 s\n",
      "Epoch 2463/3000, 60/60 - D Loss: 3.1841785852991522e-09, G Loss: 0.189163476228714 - 16.74 s\n",
      "Epoch 2464/3000, 60/60 - D Loss: 1.8387458838517445e-09, G Loss: 0.1897747963666916 - 16.15 s\n",
      "Epoch 2465/3000, 60/60 - D Loss: 2.6002712747753842e-09, G Loss: 0.18823625147342682 - 16.00 s\n",
      "Epoch 2466/3000, 60/60 - D Loss: 2.8941251387602784e-09, G Loss: 0.18807551264762878 - 16.53 s\n",
      "Epoch 2467/3000, 60/60 - D Loss: 4.227274442384493e-09, G Loss: 0.18900823593139648 - 17.31 s\n",
      "Epoch 2468/3000, 60/60 - D Loss: 2.2842937311660927e-09, G Loss: 0.18880939483642578 - 17.79 s\n",
      "Epoch 2469/3000, 60/60 - D Loss: 1.9031163129776263e-09, G Loss: 0.18839047849178314 - 18.06 s\n",
      "Epoch 2470/3000, 60/60 - D Loss: 2.6551246821819585e-09, G Loss: 0.1874794363975525 - 17.67 s\n",
      "Epoch 2471/3000, 60/60 - D Loss: 2.034210523985114e-09, G Loss: 0.18864788115024567 - 17.78 s\n",
      "Epoch 2472/3000, 60/60 - D Loss: 2.76698008949694e-09, G Loss: 0.18950575590133667 - 17.43 s\n",
      "Epoch 2473/3000, 60/60 - D Loss: 2.5432717589909948e-09, G Loss: 0.188262477517128 - 17.57 s\n",
      "Epoch 2474/3000, 60/60 - D Loss: 1.9316704802548046e-09, G Loss: 0.18834462761878967 - 17.82 s\n",
      "Epoch 2475/3000, 60/60 - D Loss: 1.5477075543477575e-09, G Loss: 0.1890452802181244 - 17.83 s\n",
      "Epoch 2476/3000, 60/60 - D Loss: 1.3985206593052774e-09, G Loss: 0.1886296421289444 - 17.72 s\n",
      "Epoch 2477/3000, 60/60 - D Loss: 1.59202989636828e-09, G Loss: 0.18767376244068146 - 17.71 s\n",
      "Epoch 2478/3000, 60/60 - D Loss: 1.9921496033749667e-09, G Loss: 0.18864017724990845 - 17.65 s\n",
      "Epoch 2479/3000, 60/60 - D Loss: 1.5077456564076969e-09, G Loss: 0.18873260915279388 - 17.59 s\n",
      "Epoch 2480/3000, 60/60 - D Loss: 1.6028670317023638e-09, G Loss: 0.18961414694786072 - 17.54 s\n",
      "Epoch 2481/3000, 60/60 - D Loss: 1.324861895971724e-09, G Loss: 0.18829520046710968 - 17.50 s\n",
      "Epoch 2482/3000, 60/60 - D Loss: 2.431201473864289e-09, G Loss: 0.18786700069904327 - 16.96 s\n",
      "Epoch 2483/3000, 60/60 - D Loss: 1.418357371872972e-09, G Loss: 0.1910354346036911 - 16.93 s\n",
      "Epoch 2484/3000, 60/60 - D Loss: 1.9665169904807545e-09, G Loss: 0.18874719738960266 - 16.76 s\n",
      "Epoch 2485/3000, 60/60 - D Loss: 1.6781788036036166e-09, G Loss: 0.1882292926311493 - 17.40 s\n",
      "Epoch 2486/3000, 60/60 - D Loss: 9.227504979732182e-10, G Loss: 0.1875312328338623 - 17.61 s\n",
      "Epoch 2487/3000, 60/60 - D Loss: 1.5474298684873405e-09, G Loss: 0.18774670362472534 - 17.65 s\n",
      "Epoch 2488/3000, 60/60 - D Loss: 5.543013890767223e-10, G Loss: 0.1878073513507843 - 17.67 s\n",
      "Epoch 2489/3000, 60/60 - D Loss: 6.271775406961264e-10, G Loss: 0.1878877580165863 - 17.70 s\n",
      "Epoch 2490/3000, 60/60 - D Loss: 5.732923994469091e-10, G Loss: 0.18860772252082825 - 17.79 s\n",
      "Epoch 2491/3000, 60/60 - D Loss: 1.0449347701403742e-09, G Loss: 0.18869811296463013 - 17.53 s\n",
      "Epoch 2492/3000, 60/60 - D Loss: 1.0264474687967165e-09, G Loss: 0.18825234472751617 - 17.69 s\n",
      "Epoch 2493/3000, 60/60 - D Loss: 7.571611416730233e-10, G Loss: 0.1885857731103897 - 17.54 s\n",
      "Epoch 2494/3000, 60/60 - D Loss: 7.170506578367715e-10, G Loss: 0.18955866992473602 - 17.69 s\n",
      "Epoch 2495/3000, 60/60 - D Loss: 7.195424403332561e-10, G Loss: 0.18730942904949188 - 17.67 s\n",
      "Epoch 2496/3000, 60/60 - D Loss: 9.463637695218165e-10, G Loss: 0.1868312805891037 - 17.84 s\n",
      "Epoch 2497/3000, 60/60 - D Loss: 5.432796666964695e-10, G Loss: 0.18835826218128204 - 17.73 s\n",
      "Epoch 2498/3000, 60/60 - D Loss: 6.806270408930316e-10, G Loss: 0.18832828104496002 - 17.80 s\n",
      "Epoch 2499/3000, 60/60 - D Loss: 4.468480980742741e-10, G Loss: 0.19125840067863464 - 17.99 s\n",
      "Epoch 2500/3000, 60/60 - D Loss: 5.664644279904443e-10, G Loss: 0.18882636725902557 - 17.77 s\n",
      "Saved model at epoch 2500 to saved_mode\\model_2500.h5\n",
      "Epoch 2501/3000, 60/60 - D Loss: 4.267243458003396e-10, G Loss: 0.18937702476978302 - 17.70 s\n",
      "Epoch 2502/3000, 60/60 - D Loss: 1.2273833858988947e-09, G Loss: 0.18837107717990875 - 17.57 s\n",
      "Epoch 2503/3000, 60/60 - D Loss: 6.165837342650762e-10, G Loss: 0.18746154010295868 - 17.75 s\n",
      "Epoch 2504/3000, 60/60 - D Loss: 5.25709905539436e-10, G Loss: 0.1899745613336563 - 17.87 s\n",
      "Epoch 2505/3000, 60/60 - D Loss: 8.214386020347075e-10, G Loss: 0.18893809616565704 - 17.73 s\n",
      "Epoch 2506/3000, 60/60 - D Loss: 5.760432198245995e-10, G Loss: 0.18870855867862701 - 17.93 s\n",
      "Epoch 2507/3000, 60/60 - D Loss: 2.9155820672251836e-10, G Loss: 0.1884862631559372 - 17.78 s\n",
      "Epoch 2508/3000, 60/60 - D Loss: 2.828070286091909e-10, G Loss: 0.19042618572711945 - 17.61 s\n",
      "Epoch 2509/3000, 60/60 - D Loss: 2.042044536666298e-10, G Loss: 0.18853190541267395 - 17.78 s\n",
      "Epoch 2510/3000, 60/60 - D Loss: 2.71376420223372e-10, G Loss: 0.18783703446388245 - 17.70 s\n",
      "Epoch 2511/3000, 60/60 - D Loss: 3.13413917350314e-10, G Loss: 0.18874910473823547 - 17.44 s\n",
      "Epoch 2512/3000, 60/60 - D Loss: 1.5725724994658345e-10, G Loss: 0.18836314976215363 - 17.72 s\n",
      "Epoch 2513/3000, 60/60 - D Loss: 3.468654132638641e-10, G Loss: 0.18899475038051605 - 17.04 s\n",
      "Epoch 2514/3000, 60/60 - D Loss: 2.8678659813141487e-10, G Loss: 0.18765905499458313 - 17.25 s\n",
      "Epoch 2515/3000, 60/60 - D Loss: 2.4504981582047693e-10, G Loss: 0.18783031404018402 - 17.65 s\n",
      "Epoch 2516/3000, 60/60 - D Loss: 3.462314018657948e-10, G Loss: 0.18902142345905304 - 17.61 s\n",
      "Epoch 2517/3000, 60/60 - D Loss: 1.7953648046324308e-10, G Loss: 0.1886623501777649 - 17.82 s\n",
      "Epoch 2518/3000, 60/60 - D Loss: 2.845478557503755e-10, G Loss: 0.18964342772960663 - 17.81 s\n",
      "Epoch 2519/3000, 60/60 - D Loss: 1.3407040308130967e-10, G Loss: 0.1895359307527542 - 17.79 s\n",
      "Epoch 2520/3000, 60/60 - D Loss: 2.378725610718358e-10, G Loss: 0.18855935335159302 - 17.65 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2521/3000, 60/60 - D Loss: 1.8427290112376076e-10, G Loss: 0.1887187361717224 - 17.37 s\n",
      "Epoch 2522/3000, 60/60 - D Loss: 1.821462127448662e-10, G Loss: 0.18770958483219147 - 17.32 s\n",
      "Epoch 2523/3000, 60/60 - D Loss: 2.232283597742744e-10, G Loss: 0.18911303579807281 - 17.57 s\n",
      "Epoch 2524/3000, 60/60 - D Loss: 2.1193616122055364e-10, G Loss: 0.1889965683221817 - 17.47 s\n",
      "Epoch 2525/3000, 60/60 - D Loss: 1.9472106955689293e-10, G Loss: 0.18777327239513397 - 17.79 s\n",
      "Epoch 2526/3000, 60/60 - D Loss: 2.0364992466903944e-10, G Loss: 0.18789604306221008 - 17.95 s\n",
      "Epoch 2527/3000, 60/60 - D Loss: 2.2511165353393103e-10, G Loss: 0.1902753859758377 - 17.92 s\n",
      "Epoch 2528/3000, 60/60 - D Loss: 2.718397557971647e-10, G Loss: 0.1878253072500229 - 17.72 s\n",
      "Epoch 2529/3000, 60/60 - D Loss: 2.306682271813144e-10, G Loss: 0.18795450031757355 - 18.23 s\n",
      "Epoch 2530/3000, 60/60 - D Loss: 1.786938044230765e-10, G Loss: 0.19169673323631287 - 17.60 s\n",
      "Epoch 2531/3000, 60/60 - D Loss: 1.995478293305539e-10, G Loss: 0.1888386309146881 - 17.65 s\n",
      "Epoch 2532/3000, 60/60 - D Loss: 2.358798334472145e-10, G Loss: 0.18888860940933228 - 17.65 s\n",
      "Epoch 2533/3000, 60/60 - D Loss: 1.9317852884421756e-10, G Loss: 0.1887214332818985 - 17.57 s\n",
      "Epoch 2534/3000, 60/60 - D Loss: 2.3574180811386427e-10, G Loss: 0.18855521082878113 - 17.80 s\n",
      "Epoch 2535/3000, 60/60 - D Loss: 2.1880648648600924e-10, G Loss: 0.18771353363990784 - 17.61 s\n",
      "Epoch 2536/3000, 60/60 - D Loss: 1.5279340956391363e-10, G Loss: 0.18843697011470795 - 17.63 s\n",
      "Epoch 2537/3000, 60/60 - D Loss: 1.5988116073981573e-10, G Loss: 0.18780827522277832 - 17.76 s\n",
      "Epoch 2538/3000, 60/60 - D Loss: 1.9516705431805873e-10, G Loss: 0.1883397102355957 - 17.81 s\n",
      "Epoch 2539/3000, 60/60 - D Loss: 1.998083983220677e-10, G Loss: 0.18729056417942047 - 17.63 s\n",
      "Epoch 2540/3000, 60/60 - D Loss: 2.90249092806344e-10, G Loss: 0.18762406706809998 - 17.61 s\n",
      "Epoch 2541/3000, 60/60 - D Loss: 1.2830801611140555e-10, G Loss: 0.18834374845027924 - 17.67 s\n",
      "Epoch 2542/3000, 60/60 - D Loss: 1.287245450172275e-10, G Loss: 0.1883402168750763 - 17.71 s\n",
      "Epoch 2543/3000, 60/60 - D Loss: 1.20613855972881e-10, G Loss: 0.18899013102054596 - 17.95 s\n",
      "Epoch 2544/3000, 60/60 - D Loss: 1.329074639108913e-10, G Loss: 0.1884722113609314 - 17.65 s\n",
      "Epoch 2545/3000, 60/60 - D Loss: 1.1263598360218216e-10, G Loss: 0.18766117095947266 - 17.96 s\n",
      "Epoch 2546/3000, 60/60 - D Loss: 1.5177939774560804e-10, G Loss: 0.18856048583984375 - 17.75 s\n",
      "Epoch 2547/3000, 60/60 - D Loss: 1.4651723855441806e-10, G Loss: 0.18814195692539215 - 17.61 s\n",
      "Epoch 2548/3000, 60/60 - D Loss: 1.9216328128954523e-10, G Loss: 0.18849916756153107 - 17.71 s\n",
      "Epoch 2549/3000, 60/60 - D Loss: 2.1062195079861522e-10, G Loss: 0.1891956776380539 - 17.54 s\n",
      "Epoch 2550/3000, 60/60 - D Loss: 1.7767959815988754e-10, G Loss: 0.18945321440696716 - 17.67 s\n",
      "Epoch 2551/3000, 60/60 - D Loss: 2.000034100602165e-10, G Loss: 0.18827427923679352 - 17.75 s\n",
      "Epoch 2552/3000, 60/60 - D Loss: 1.6060882414257473e-10, G Loss: 0.18819259107112885 - 17.61 s\n",
      "Epoch 2553/3000, 60/60 - D Loss: 1.360323885073899e-10, G Loss: 0.1880091279745102 - 17.78 s\n",
      "Epoch 2554/3000, 60/60 - D Loss: 1.3689934624581433e-10, G Loss: 0.18924307823181152 - 17.59 s\n",
      "Epoch 2555/3000, 60/60 - D Loss: 1.3608872151266702e-10, G Loss: 0.190956249833107 - 17.84 s\n",
      "Epoch 2556/3000, 60/60 - D Loss: 1.0909464215215611e-10, G Loss: 0.1882190853357315 - 17.78 s\n",
      "Epoch 2557/3000, 60/60 - D Loss: 1.0138870418482438e-10, G Loss: 0.1880197376012802 - 17.57 s\n",
      "Epoch 2558/3000, 60/60 - D Loss: 1.32649992503619e-10, G Loss: 0.18886461853981018 - 17.57 s\n",
      "Epoch 2559/3000, 60/60 - D Loss: 9.269499538841345e-11, G Loss: 0.18963438272476196 - 17.45 s\n",
      "Epoch 2560/3000, 60/60 - D Loss: 9.682161700634389e-11, G Loss: 0.18826110661029816 - 17.90 s\n",
      "Epoch 2561/3000, 60/60 - D Loss: 1.2022966602265556e-10, G Loss: 0.18824051320552826 - 17.67 s\n",
      "Epoch 2562/3000, 60/60 - D Loss: 1.0425453911760496e-10, G Loss: 0.18760773539543152 - 17.75 s\n",
      "Epoch 2563/3000, 60/60 - D Loss: 8.741513817732112e-11, G Loss: 0.1884961873292923 - 17.68 s\n",
      "Epoch 2564/3000, 60/60 - D Loss: 1.49894211029627e-10, G Loss: 0.18808835744857788 - 17.68 s\n",
      "Epoch 2565/3000, 60/60 - D Loss: 9.286331246263362e-06, G Loss: 0.18995778262615204 - 17.76 s\n",
      "Epoch 2566/3000, 60/60 - D Loss: 5.923994391698884e-07, G Loss: 0.18924270570278168 - 17.81 s\n",
      "Epoch 2567/3000, 60/60 - D Loss: 6.334410656005929e-07, G Loss: 0.18845829367637634 - 17.62 s\n",
      "Epoch 2568/3000, 60/60 - D Loss: 2.839086968877552e-06, G Loss: 0.18785762786865234 - 17.29 s\n",
      "Epoch 2569/3000, 60/60 - D Loss: 2.838407277203081e-06, G Loss: 0.18780042231082916 - 17.59 s\n",
      "Epoch 2570/3000, 60/60 - D Loss: 2.8277916986878954e-06, G Loss: 0.18909281492233276 - 17.98 s\n",
      "Epoch 2571/3000, 60/60 - D Loss: 2.8085355992905903e-06, G Loss: 0.18972602486610413 - 17.66 s\n",
      "Epoch 2572/3000, 60/60 - D Loss: 2.812508961180099e-06, G Loss: 0.18875782191753387 - 17.77 s\n",
      "Epoch 2573/3000, 60/60 - D Loss: 2.8163414943493947e-06, G Loss: 0.18900935351848602 - 17.75 s\n",
      "Epoch 2574/3000, 60/60 - D Loss: 2.802241686290438e-06, G Loss: 0.18840885162353516 - 17.81 s\n",
      "Epoch 2575/3000, 60/60 - D Loss: 2.8028051368700346e-06, G Loss: 0.1874668449163437 - 17.94 s\n",
      "Epoch 2576/3000, 60/60 - D Loss: 2.7822876859651124e-06, G Loss: 0.18825817108154297 - 17.73 s\n",
      "Epoch 2577/3000, 60/60 - D Loss: 2.7670489619116556e-06, G Loss: 0.19005483388900757 - 17.46 s\n",
      "Epoch 2578/3000, 60/60 - D Loss: 2.754993310694759e-06, G Loss: 0.18840834498405457 - 17.57 s\n",
      "Epoch 2579/3000, 60/60 - D Loss: 2.7501427529198763e-06, G Loss: 0.1893877387046814 - 17.52 s\n",
      "Epoch 2580/3000, 60/60 - D Loss: 2.745906293639272e-06, G Loss: 0.18833500146865845 - 17.77 s\n",
      "Epoch 2581/3000, 60/60 - D Loss: 2.721914306332701e-06, G Loss: 0.1882394254207611 - 17.73 s\n",
      "Epoch 2582/3000, 60/60 - D Loss: 2.701145907991337e-06, G Loss: 0.1881590634584427 - 17.80 s\n",
      "Epoch 2583/3000, 60/60 - D Loss: 2.681518269390115e-06, G Loss: 0.18811164796352386 - 17.55 s\n",
      "Epoch 2584/3000, 60/60 - D Loss: 2.6657117683341613e-06, G Loss: 0.18795765936374664 - 17.52 s\n",
      "Epoch 2585/3000, 60/60 - D Loss: 2.6449220364328785e-06, G Loss: 0.19002430140972137 - 17.53 s\n",
      "Epoch 2586/3000, 60/60 - D Loss: 2.617090700603175e-06, G Loss: 0.18932931125164032 - 17.97 s\n",
      "Epoch 2587/3000, 60/60 - D Loss: 2.5769807769186748e-06, G Loss: 0.1885690838098526 - 17.72 s\n",
      "Epoch 2588/3000, 60/60 - D Loss: 2.5695352952812e-06, G Loss: 0.18852441012859344 - 17.65 s\n",
      "Epoch 2589/3000, 60/60 - D Loss: 2.5737223109494056e-06, G Loss: 0.18795034289360046 - 17.56 s\n",
      "Epoch 2590/3000, 60/60 - D Loss: 2.54666830337702e-06, G Loss: 0.18889114260673523 - 17.95 s\n",
      "Epoch 2591/3000, 60/60 - D Loss: 2.5357454850848116e-06, G Loss: 0.18771357834339142 - 17.61 s\n",
      "Epoch 2592/3000, 60/60 - D Loss: 2.5413232380387674e-06, G Loss: 0.18793699145317078 - 17.63 s\n",
      "Epoch 2593/3000, 60/60 - D Loss: 2.5344346591717193e-06, G Loss: 0.18921798467636108 - 17.73 s\n",
      "Epoch 2594/3000, 60/60 - D Loss: 2.5507759837256616e-06, G Loss: 0.1880190670490265 - 17.63 s\n",
      "Epoch 2595/3000, 60/60 - D Loss: 2.5390563318343495e-06, G Loss: 0.1882682740688324 - 17.74 s\n",
      "Epoch 2596/3000, 60/60 - D Loss: 2.4887365011935086e-06, G Loss: 0.18856658041477203 - 17.60 s\n",
      "Epoch 2597/3000, 60/60 - D Loss: 2.441227185493236e-06, G Loss: 0.18864917755126953 - 17.48 s\n",
      "Epoch 2598/3000, 60/60 - D Loss: 2.4090533628316063e-06, G Loss: 0.18829938769340515 - 17.55 s\n",
      "Epoch 2599/3000, 60/60 - D Loss: 2.3772291438284993e-06, G Loss: 0.1873011589050293 - 17.64 s\n",
      "Epoch 2600/3000, 60/60 - D Loss: 2.3694392933624325e-06, G Loss: 0.18945233523845673 - 17.52 s\n",
      "Epoch 2601/3000, 60/60 - D Loss: 2.3608449801412745e-06, G Loss: 0.18820318579673767 - 17.74 s\n",
      "Epoch 2602/3000, 60/60 - D Loss: 2.3400628916224464e-06, G Loss: 0.1889348328113556 - 17.54 s\n",
      "Epoch 2603/3000, 60/60 - D Loss: 2.328253993422874e-06, G Loss: 0.19001808762550354 - 17.89 s\n",
      "Epoch 2604/3000, 60/60 - D Loss: 2.268092013620396e-06, G Loss: 0.18780119717121124 - 17.85 s\n",
      "Epoch 2605/3000, 60/60 - D Loss: 2.2544832203753168e-06, G Loss: 0.18824204802513123 - 17.85 s\n",
      "Epoch 2606/3000, 60/60 - D Loss: 2.2233770141134868e-06, G Loss: 0.1883058249950409 - 17.69 s\n",
      "Epoch 2607/3000, 60/60 - D Loss: 2.2169505166022563e-06, G Loss: 0.18967467546463013 - 17.60 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2608/3000, 60/60 - D Loss: 2.1862359522745872e-06, G Loss: 0.1886233389377594 - 17.38 s\n",
      "Epoch 2609/3000, 60/60 - D Loss: 2.0905137388697523e-06, G Loss: 0.188328817486763 - 17.56 s\n",
      "Epoch 2610/3000, 60/60 - D Loss: 2.0189964065279314e-06, G Loss: 0.18841847777366638 - 17.75 s\n",
      "Epoch 2611/3000, 60/60 - D Loss: 1.9638347364516265e-06, G Loss: 0.18796110153198242 - 17.71 s\n",
      "Epoch 2612/3000, 60/60 - D Loss: 1.934202618611436e-06, G Loss: 0.18831388652324677 - 17.50 s\n",
      "Epoch 2613/3000, 60/60 - D Loss: 1.8896701760289653e-06, G Loss: 0.18882662057876587 - 17.84 s\n",
      "Epoch 2614/3000, 60/60 - D Loss: 1.841115176239895e-06, G Loss: 0.1875966340303421 - 17.72 s\n",
      "Epoch 2615/3000, 60/60 - D Loss: 1.8563314384510698e-06, G Loss: 0.18801210820674896 - 17.48 s\n",
      "Epoch 2616/3000, 60/60 - D Loss: 1.8162697200865136e-06, G Loss: 0.19251154363155365 - 17.56 s\n",
      "Epoch 2617/3000, 60/60 - D Loss: 1.707139106471231e-06, G Loss: 0.18896891176700592 - 17.63 s\n",
      "Epoch 2618/3000, 60/60 - D Loss: 1.6666814638228653e-06, G Loss: 0.18793295323848724 - 17.81 s\n",
      "Epoch 2619/3000, 60/60 - D Loss: 1.6372895774007662e-06, G Loss: 0.18791921436786652 - 17.71 s\n",
      "Epoch 2620/3000, 60/60 - D Loss: 1.6370601405909374e-06, G Loss: 0.1890886127948761 - 18.19 s\n",
      "Epoch 2621/3000, 60/60 - D Loss: 1.5806739415875903e-06, G Loss: 0.18916453421115875 - 17.58 s\n",
      "Epoch 2622/3000, 60/60 - D Loss: 1.542906551315129e-06, G Loss: 0.1878383606672287 - 17.63 s\n",
      "Epoch 2623/3000, 60/60 - D Loss: 1.479441219755153e-06, G Loss: 0.1893269270658493 - 17.64 s\n",
      "Epoch 2624/3000, 60/60 - D Loss: 1.3855929472815671e-06, G Loss: 0.18761669099330902 - 17.75 s\n",
      "Epoch 2625/3000, 60/60 - D Loss: 1.2996182822636732e-06, G Loss: 0.18752576410770416 - 17.77 s\n",
      "Epoch 2626/3000, 60/60 - D Loss: 1.3722483209448882e-06, G Loss: 0.18806548416614532 - 17.51 s\n",
      "Epoch 2627/3000, 60/60 - D Loss: 1.2400914312195917e-06, G Loss: 0.18840841948986053 - 17.71 s\n",
      "Epoch 2628/3000, 60/60 - D Loss: 1.1953521873273205e-06, G Loss: 0.1880631297826767 - 17.62 s\n",
      "Epoch 2629/3000, 60/60 - D Loss: 1.102493076551381e-06, G Loss: 0.1889704167842865 - 17.69 s\n",
      "Epoch 2630/3000, 60/60 - D Loss: 1.0356120147677825e-06, G Loss: 0.18898656964302063 - 17.76 s\n",
      "Epoch 2631/3000, 60/60 - D Loss: 9.483262805983156e-07, G Loss: 0.1895708590745926 - 17.73 s\n",
      "Epoch 2632/3000, 60/60 - D Loss: 8.799905579386959e-07, G Loss: 0.19017909467220306 - 17.99 s\n",
      "Epoch 2633/3000, 60/60 - D Loss: 8.242779023766387e-07, G Loss: 0.1884208619594574 - 17.70 s\n",
      "Epoch 2634/3000, 60/60 - D Loss: 7.646095109292445e-07, G Loss: 0.18817998468875885 - 17.59 s\n",
      "Epoch 2635/3000, 60/60 - D Loss: 7.070053375700857e-07, G Loss: 0.18888384103775024 - 17.71 s\n",
      "Epoch 2636/3000, 60/60 - D Loss: 7.002453070034057e-07, G Loss: 0.18936720490455627 - 17.89 s\n",
      "Epoch 2637/3000, 60/60 - D Loss: 6.883809233425558e-07, G Loss: 0.18891699612140656 - 17.61 s\n",
      "Epoch 2638/3000, 60/60 - D Loss: 6.390034125623657e-07, G Loss: 0.188884437084198 - 17.83 s\n",
      "Epoch 2639/3000, 60/60 - D Loss: 5.916629705382038e-07, G Loss: 0.18765570223331451 - 17.52 s\n",
      "Epoch 2640/3000, 60/60 - D Loss: 6.182333981266446e-07, G Loss: 0.18815390765666962 - 17.69 s\n",
      "Epoch 2641/3000, 60/60 - D Loss: 5.590203147566781e-07, G Loss: 0.1880338191986084 - 17.78 s\n",
      "Epoch 2642/3000, 60/60 - D Loss: 5.261711270692629e-07, G Loss: 0.18896745145320892 - 17.78 s\n",
      "Epoch 2643/3000, 60/60 - D Loss: 4.853316615219423e-07, G Loss: 0.18829917907714844 - 17.61 s\n",
      "Epoch 2644/3000, 60/60 - D Loss: 4.26776062280437e-07, G Loss: 0.18786264955997467 - 17.43 s\n",
      "Epoch 2645/3000, 60/60 - D Loss: 3.7950932049313354e-07, G Loss: 0.1896144449710846 - 17.64 s\n",
      "Epoch 2646/3000, 60/60 - D Loss: 4.3342495864679103e-07, G Loss: 0.19152283668518066 - 17.64 s\n",
      "Epoch 2647/3000, 60/60 - D Loss: 3.5445314450974615e-07, G Loss: 0.18813103437423706 - 17.85 s\n",
      "Epoch 2648/3000, 60/60 - D Loss: 3.097476863368279e-07, G Loss: 0.18912094831466675 - 17.75 s\n",
      "Epoch 2649/3000, 60/60 - D Loss: 2.57394058407935e-07, G Loss: 0.18764027953147888 - 17.72 s\n",
      "Epoch 2650/3000, 60/60 - D Loss: 2.2185549232697617e-07, G Loss: 0.1883050501346588 - 17.74 s\n",
      "Epoch 2651/3000, 60/60 - D Loss: 2.464334125873686e-07, G Loss: 0.1874171644449234 - 17.97 s\n",
      "Epoch 2652/3000, 60/60 - D Loss: 2.0680666621267612e-07, G Loss: 0.18765889108181 - 17.88 s\n",
      "Epoch 2653/3000, 60/60 - D Loss: 1.8635553062445627e-07, G Loss: 0.18860134482383728 - 17.83 s\n",
      "Epoch 2654/3000, 60/60 - D Loss: 1.836682057687165e-07, G Loss: 0.18937727808952332 - 17.75 s\n",
      "Epoch 2655/3000, 60/60 - D Loss: 1.6966234539880464e-07, G Loss: 0.1877870410680771 - 17.68 s\n",
      "Epoch 2656/3000, 60/60 - D Loss: 1.69181892609059e-07, G Loss: 0.18773393332958221 - 17.59 s\n",
      "Epoch 2657/3000, 60/60 - D Loss: 1.5726496449604846e-07, G Loss: 0.18792805075645447 - 17.83 s\n",
      "Epoch 2658/3000, 60/60 - D Loss: 1.4632185662087373e-07, G Loss: 0.18876448273658752 - 17.79 s\n",
      "Epoch 2659/3000, 60/60 - D Loss: 1.343063082915726e-07, G Loss: 0.18821470439434052 - 17.73 s\n",
      "Epoch 2660/3000, 60/60 - D Loss: 1.1110278106244614e-07, G Loss: 0.18839479982852936 - 17.81 s\n",
      "Epoch 2661/3000, 60/60 - D Loss: 1.0347772244337243e-07, G Loss: 0.18922112882137299 - 17.79 s\n",
      "Epoch 2662/3000, 60/60 - D Loss: 8.835433776911495e-08, G Loss: 0.1883949339389801 - 17.83 s\n",
      "Epoch 2663/3000, 60/60 - D Loss: 8.239622525176446e-08, G Loss: 0.18864662945270538 - 17.54 s\n",
      "Epoch 2664/3000, 60/60 - D Loss: 8.163350502993465e-08, G Loss: 0.18792420625686646 - 17.49 s\n",
      "Epoch 2665/3000, 60/60 - D Loss: 8.0020410774024e-08, G Loss: 0.18811997771263123 - 17.45 s\n",
      "Epoch 2666/3000, 60/60 - D Loss: 6.7894592854924e-08, G Loss: 0.18841056525707245 - 17.94 s\n",
      "Epoch 2667/3000, 60/60 - D Loss: 6.729787802284946e-08, G Loss: 0.18849581480026245 - 17.74 s\n",
      "Epoch 2668/3000, 60/60 - D Loss: 6.112658743165934e-08, G Loss: 0.1885688155889511 - 17.67 s\n",
      "Epoch 2669/3000, 60/60 - D Loss: 5.912868555466414e-08, G Loss: 0.18773455917835236 - 17.71 s\n",
      "Epoch 2670/3000, 60/60 - D Loss: 5.789018457527098e-08, G Loss: 0.18814076483249664 - 17.62 s\n",
      "Epoch 2671/3000, 60/60 - D Loss: 4.962593986575988e-08, G Loss: 0.18848496675491333 - 17.73 s\n",
      "Epoch 2672/3000, 60/60 - D Loss: 4.55402231625013e-08, G Loss: 0.18815667927265167 - 17.75 s\n",
      "Epoch 2673/3000, 60/60 - D Loss: 4.5094329091225365e-08, G Loss: 0.18875564634799957 - 17.64 s\n",
      "Epoch 2674/3000, 60/60 - D Loss: 4.037022140966875e-08, G Loss: 0.18877273797988892 - 17.51 s\n",
      "Epoch 2675/3000, 60/60 - D Loss: 4.100991608809284e-08, G Loss: 0.1885978728532791 - 17.65 s\n",
      "Epoch 2676/3000, 60/60 - D Loss: 4.0724330932467225e-08, G Loss: 0.18785186111927032 - 17.61 s\n",
      "Epoch 2677/3000, 60/60 - D Loss: 3.8758951994058164e-08, G Loss: 0.1878724843263626 - 17.61 s\n",
      "Epoch 2678/3000, 60/60 - D Loss: 3.33807368506021e-08, G Loss: 0.18866123259067535 - 17.65 s\n",
      "Epoch 2679/3000, 60/60 - D Loss: 3.169827085320877e-08, G Loss: 0.18844637274742126 - 17.83 s\n",
      "Epoch 2680/3000, 60/60 - D Loss: 3.237708468137685e-08, G Loss: 0.18815483152866364 - 17.67 s\n",
      "Epoch 2681/3000, 60/60 - D Loss: 3.2308662625058586e-08, G Loss: 0.18743543326854706 - 18.23 s\n",
      "Epoch 2682/3000, 60/60 - D Loss: 2.7653591540823763e-08, G Loss: 0.18893958628177643 - 17.66 s\n",
      "Epoch 2683/3000, 60/60 - D Loss: 2.786628892338163e-08, G Loss: 0.1881701648235321 - 17.58 s\n",
      "Epoch 2684/3000, 60/60 - D Loss: 2.8608948192340812e-08, G Loss: 0.19062750041484833 - 17.42 s\n",
      "Epoch 2685/3000, 60/60 - D Loss: 2.2645247541768035e-08, G Loss: 0.18975834548473358 - 17.83 s\n",
      "Epoch 2686/3000, 60/60 - D Loss: 2.3932006671370292e-08, G Loss: 0.18732134997844696 - 17.60 s\n",
      "Epoch 2687/3000, 60/60 - D Loss: 2.1632187671256566e-08, G Loss: 0.18904948234558105 - 17.79 s\n",
      "Epoch 2688/3000, 60/60 - D Loss: 2.176187058385979e-08, G Loss: 0.18972836434841156 - 17.73 s\n",
      "Epoch 2689/3000, 60/60 - D Loss: 1.9139431400156845e-08, G Loss: 0.1912676990032196 - 18.12 s\n",
      "Epoch 2690/3000, 60/60 - D Loss: 2.0165172674010633e-08, G Loss: 0.18876488506793976 - 17.69 s\n",
      "Epoch 2691/3000, 60/60 - D Loss: 1.6744115478787277e-08, G Loss: 0.18766489624977112 - 17.61 s\n",
      "Epoch 2692/3000, 60/60 - D Loss: 1.5533968791702103e-08, G Loss: 0.18760429322719574 - 17.68 s\n",
      "Epoch 2693/3000, 60/60 - D Loss: 1.50341126484609e-08, G Loss: 0.18971291184425354 - 17.59 s\n",
      "Epoch 2694/3000, 60/60 - D Loss: 1.4179478048623113e-08, G Loss: 0.1895056813955307 - 17.74 s\n",
      "Epoch 2695/3000, 60/60 - D Loss: 1.2452020760725965e-08, G Loss: 0.18822506070137024 - 17.74 s\n",
      "Epoch 2696/3000, 60/60 - D Loss: 1.0571854514167455e-08, G Loss: 0.18803662061691284 - 17.67 s\n",
      "Epoch 2697/3000, 60/60 - D Loss: 1.2828748741036737e-08, G Loss: 0.18855585157871246 - 17.92 s\n",
      "Epoch 2698/3000, 60/60 - D Loss: 1.1848858788441667e-08, G Loss: 0.188905268907547 - 17.58 s\n",
      "Epoch 2699/3000, 60/60 - D Loss: 1.154751139649303e-08, G Loss: 0.1874210685491562 - 17.79 s\n",
      "Epoch 2700/3000, 60/60 - D Loss: 9.521655467769581e-09, G Loss: 0.18794962763786316 - 17.67 s\n",
      "Epoch 2701/3000, 60/60 - D Loss: 8.67683791816775e-09, G Loss: 0.1882784515619278 - 17.34 s\n",
      "Epoch 2702/3000, 60/60 - D Loss: 8.429277722446244e-09, G Loss: 0.186568945646286 - 17.74 s\n",
      "Epoch 2703/3000, 60/60 - D Loss: 8.520743224249498e-09, G Loss: 0.1914319097995758 - 17.39 s\n",
      "Epoch 2704/3000, 60/60 - D Loss: 6.615628490883224e-09, G Loss: 0.18831446766853333 - 17.77 s\n",
      "Epoch 2705/3000, 60/60 - D Loss: 6.235411299632615e-09, G Loss: 0.18944278359413147 - 17.99 s\n",
      "Epoch 2706/3000, 60/60 - D Loss: 5.71075720132773e-09, G Loss: 0.18821419775485992 - 17.73 s\n",
      "Epoch 2707/3000, 60/60 - D Loss: 6.045585810929681e-09, G Loss: 0.18769511580467224 - 17.68 s\n",
      "Epoch 2708/3000, 60/60 - D Loss: 5.018432781174705e-09, G Loss: 0.18852011859416962 - 17.71 s\n",
      "Epoch 2709/3000, 60/60 - D Loss: 4.660403175111109e-09, G Loss: 0.18849632143974304 - 17.74 s\n",
      "Epoch 2710/3000, 60/60 - D Loss: 4.651631968829399e-09, G Loss: 0.18866978585720062 - 17.58 s\n",
      "Epoch 2711/3000, 60/60 - D Loss: 4.0379770640786236e-09, G Loss: 0.18899855017662048 - 17.51 s\n",
      "Epoch 2712/3000, 60/60 - D Loss: 3.746813082221057e-09, G Loss: 0.18832477927207947 - 18.03 s\n",
      "Epoch 2713/3000, 60/60 - D Loss: 3.6612766153453156e-09, G Loss: 0.18811194598674774 - 17.67 s\n",
      "Epoch 2714/3000, 60/60 - D Loss: 3.4916041170626485e-09, G Loss: 0.18780754506587982 - 17.63 s\n",
      "Epoch 2715/3000, 60/60 - D Loss: 3.2000608829814154e-09, G Loss: 0.18790744245052338 - 17.68 s\n",
      "Epoch 2716/3000, 60/60 - D Loss: 3.5281573219807336e-09, G Loss: 0.18840591609477997 - 17.88 s\n",
      "Epoch 2717/3000, 60/60 - D Loss: 2.9084730180548656e-09, G Loss: 0.18859246373176575 - 17.66 s\n",
      "Epoch 2718/3000, 60/60 - D Loss: 2.6778199658142666e-09, G Loss: 0.18774445354938507 - 17.58 s\n",
      "Epoch 2719/3000, 60/60 - D Loss: 2.587131175985167e-09, G Loss: 0.18746814131736755 - 17.78 s\n",
      "Epoch 2720/3000, 60/60 - D Loss: 2.6920550233864175e-09, G Loss: 0.1877720206975937 - 17.60 s\n",
      "Epoch 2721/3000, 60/60 - D Loss: 2.9332603013433615e-09, G Loss: 0.18736591935157776 - 17.85 s\n",
      "Epoch 2722/3000, 60/60 - D Loss: 2.8590052548121556e-09, G Loss: 0.18647828698158264 - 17.69 s\n",
      "Epoch 2723/3000, 60/60 - D Loss: 2.6016286902972337e-09, G Loss: 0.18836709856987 - 17.74 s\n",
      "Epoch 2724/3000, 60/60 - D Loss: 2.360089901301298e-09, G Loss: 0.1883014589548111 - 17.87 s\n",
      "Epoch 2725/3000, 60/60 - D Loss: 2.0702353165170658e-09, G Loss: 0.18839427828788757 - 17.74 s\n",
      "Epoch 2726/3000, 60/60 - D Loss: 2.3757835698914783e-09, G Loss: 0.18882529437541962 - 17.81 s\n",
      "Epoch 2727/3000, 60/60 - D Loss: 1.9514017068977994e-09, G Loss: 0.18893732130527496 - 17.89 s\n",
      "Epoch 2728/3000, 60/60 - D Loss: 2.1584629639275135e-09, G Loss: 0.18824712932109833 - 17.64 s\n",
      "Epoch 2729/3000, 60/60 - D Loss: 1.9400967499255514e-09, G Loss: 0.1878928691148758 - 17.67 s\n",
      "Epoch 2730/3000, 60/60 - D Loss: 2.1348633971764145e-09, G Loss: 0.18824388086795807 - 17.73 s\n",
      "Epoch 2731/3000, 60/60 - D Loss: 2.016178779489278e-09, G Loss: 0.18881787359714508 - 17.75 s\n",
      "Epoch 2732/3000, 60/60 - D Loss: 1.96870275637481e-09, G Loss: 0.18843072652816772 - 17.99 s\n",
      "Epoch 2733/3000, 60/60 - D Loss: 1.6670459457318737e-09, G Loss: 0.1878160834312439 - 17.81 s\n",
      "Epoch 2734/3000, 60/60 - D Loss: 1.7010081121446487e-09, G Loss: 0.18953053653240204 - 17.71 s\n",
      "Epoch 2735/3000, 60/60 - D Loss: 1.6516807921378163e-09, G Loss: 0.18805088102817535 - 17.83 s\n",
      "Epoch 2736/3000, 60/60 - D Loss: 1.681884298501329e-09, G Loss: 0.1880577951669693 - 17.69 s\n",
      "Epoch 2737/3000, 60/60 - D Loss: 1.4279049054280304e-09, G Loss: 0.18841205537319183 - 17.68 s\n",
      "Epoch 2738/3000, 60/60 - D Loss: 1.5200889436084287e-09, G Loss: 0.1896909922361374 - 17.68 s\n",
      "Epoch 2739/3000, 60/60 - D Loss: 1.319300002578572e-09, G Loss: 0.18898944556713104 - 17.72 s\n",
      "Epoch 2740/3000, 60/60 - D Loss: 1.319985454274703e-09, G Loss: 0.19240953028202057 - 17.71 s\n",
      "Epoch 2741/3000, 60/60 - D Loss: 1.1598427773767263e-09, G Loss: 0.19060641527175903 - 17.85 s\n",
      "Epoch 2742/3000, 60/60 - D Loss: 1.153095063877649e-09, G Loss: 0.18896979093551636 - 17.96 s\n",
      "Epoch 2743/3000, 60/60 - D Loss: 1.0376456360819871e-09, G Loss: 0.1880442500114441 - 17.68 s\n",
      "Epoch 2744/3000, 60/60 - D Loss: 8.925686856288735e-10, G Loss: 0.18789128959178925 - 17.71 s\n",
      "Epoch 2745/3000, 60/60 - D Loss: 9.700670267066893e-10, G Loss: 0.18788670003414154 - 17.67 s\n",
      "Epoch 2746/3000, 60/60 - D Loss: 1.0418790274970118e-09, G Loss: 0.18788374960422516 - 17.71 s\n",
      "Epoch 2747/3000, 60/60 - D Loss: 9.648490895132846e-10, G Loss: 0.1879090517759323 - 17.64 s\n",
      "Epoch 2748/3000, 60/60 - D Loss: 7.85134235403793e-10, G Loss: 0.18851551413536072 - 17.66 s\n",
      "Epoch 2749/3000, 60/60 - D Loss: 8.634474801817044e-10, G Loss: 0.1898496001958847 - 17.46 s\n",
      "Epoch 2750/3000, 60/60 - D Loss: 8.299881337770555e-10, G Loss: 0.18938475847244263 - 17.69 s\n",
      "Epoch 2751/3000, 60/60 - D Loss: 8.246553995229172e-10, G Loss: 0.1891191452741623 - 17.89 s\n",
      "Epoch 2752/3000, 60/60 - D Loss: 8.732971568115572e-10, G Loss: 0.1880490481853485 - 17.78 s\n",
      "Epoch 2753/3000, 60/60 - D Loss: 8.358195802138896e-10, G Loss: 0.18737195432186127 - 17.80 s\n",
      "Epoch 2754/3000, 60/60 - D Loss: 7.688574776843373e-10, G Loss: 0.18847863376140594 - 17.85 s\n",
      "Epoch 2755/3000, 60/60 - D Loss: 7.574856297765614e-10, G Loss: 0.18904832005500793 - 17.56 s\n",
      "Epoch 2756/3000, 60/60 - D Loss: 7.625691744728572e-10, G Loss: 0.18976439535617828 - 17.76 s\n",
      "Epoch 2757/3000, 60/60 - D Loss: 7.77793218720376e-10, G Loss: 0.18847042322158813 - 17.92 s\n",
      "Epoch 2758/3000, 60/60 - D Loss: 7.558581538447478e-10, G Loss: 0.18828891217708588 - 17.56 s\n",
      "Epoch 2759/3000, 60/60 - D Loss: 6.104525218653964e-10, G Loss: 0.18812474608421326 - 17.64 s\n",
      "Epoch 2760/3000, 60/60 - D Loss: 9.506987419528578e-10, G Loss: 0.18843214213848114 - 17.74 s\n",
      "Epoch 2761/3000, 60/60 - D Loss: 0.0008603408788303568, G Loss: 0.18787731230258942 - 17.48 s\n",
      "Epoch 2762/3000, 60/60 - D Loss: 1.7872534797902517e-05, G Loss: 0.18775419890880585 - 17.80 s\n",
      "Epoch 2763/3000, 60/60 - D Loss: 1.8189625166087353e-06, G Loss: 0.1890956312417984 - 17.75 s\n",
      "Epoch 2764/3000, 60/60 - D Loss: 2.3710729806225572e-06, G Loss: 0.19057472050189972 - 17.89 s\n",
      "Epoch 2765/3000, 60/60 - D Loss: 2.687534589540519e-06, G Loss: 0.18896378576755524 - 17.73 s\n",
      "Epoch 2766/3000, 60/60 - D Loss: 2.746380914686597e-06, G Loss: 0.18929710984230042 - 17.84 s\n",
      "Epoch 2767/3000, 60/60 - D Loss: 2.6492599261018768e-06, G Loss: 0.18737497925758362 - 17.78 s\n",
      "Epoch 2768/3000, 60/60 - D Loss: 2.5117732889157196e-06, G Loss: 0.1883011758327484 - 17.82 s\n",
      "Epoch 2769/3000, 60/60 - D Loss: 2.3923159062633204e-06, G Loss: 0.18768563866615295 - 17.63 s\n",
      "Epoch 2770/3000, 60/60 - D Loss: 2.292241703116815e-06, G Loss: 0.18746410310268402 - 17.63 s\n",
      "Epoch 2771/3000, 60/60 - D Loss: 2.268324195142668e-06, G Loss: 0.1884276568889618 - 17.84 s\n",
      "Epoch 2772/3000, 60/60 - D Loss: 2.2052632786540016e-06, G Loss: 0.18761712312698364 - 17.71 s\n",
      "Epoch 2773/3000, 60/60 - D Loss: 2.243343942609499e-06, G Loss: 0.1885499805212021 - 17.98 s\n",
      "Epoch 2774/3000, 60/60 - D Loss: 2.1458850341105062e-06, G Loss: 0.18810011446475983 - 17.82 s\n",
      "Epoch 2775/3000, 60/60 - D Loss: 2.2240510233473287e-06, G Loss: 0.188041552901268 - 17.54 s\n",
      "Epoch 2776/3000, 60/60 - D Loss: 2.199322700846551e-06, G Loss: 0.18817073106765747 - 17.53 s\n",
      "Epoch 2777/3000, 60/60 - D Loss: 2.1572296233784982e-06, G Loss: 0.1892816424369812 - 17.57 s\n",
      "Epoch 2778/3000, 60/60 - D Loss: 2.063913473904222e-06, G Loss: 0.18812957406044006 - 17.70 s\n",
      "Epoch 2779/3000, 60/60 - D Loss: 1.981496460246035e-06, G Loss: 0.18854345381259918 - 17.78 s\n",
      "Epoch 2780/3000, 60/60 - D Loss: 1.9812826579368448e-06, G Loss: 0.1881885528564453 - 17.81 s\n",
      "Epoch 2781/3000, 60/60 - D Loss: 2.1430505015018753e-06, G Loss: 0.19077448546886444 - 17.91 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2782/3000, 60/60 - D Loss: 2.212944181678722e-06, G Loss: 0.18736211955547333 - 17.79 s\n",
      "Epoch 2783/3000, 60/60 - D Loss: 2.079314878500327e-06, G Loss: 0.18849973380565643 - 17.71 s\n",
      "Epoch 2784/3000, 60/60 - D Loss: 2.0154459576815498e-06, G Loss: 0.18754450976848602 - 17.72 s\n",
      "Epoch 2785/3000, 60/60 - D Loss: 1.970982005161659e-06, G Loss: 0.18869170546531677 - 17.66 s\n",
      "Epoch 2786/3000, 60/60 - D Loss: 2.0265538687169737e-06, G Loss: 0.18781164288520813 - 17.76 s\n",
      "Epoch 2787/3000, 60/60 - D Loss: 1.945224124000333e-06, G Loss: 0.18760773539543152 - 17.72 s\n",
      "Epoch 2788/3000, 60/60 - D Loss: 1.8913697310551925e-06, G Loss: 0.18770574033260345 - 18.06 s\n",
      "Epoch 2789/3000, 60/60 - D Loss: 1.566176770495531e-06, G Loss: 0.18715573847293854 - 17.55 s\n",
      "Epoch 2790/3000, 60/60 - D Loss: 1.6081873681628167e-06, G Loss: 0.18596665561199188 - 17.83 s\n",
      "Epoch 2791/3000, 60/60 - D Loss: 1.6446115722601462e-06, G Loss: 0.188058540225029 - 17.75 s\n",
      "Epoch 2792/3000, 60/60 - D Loss: 1.5767681169620573e-06, G Loss: 0.1891505867242813 - 17.57 s\n",
      "Epoch 2793/3000, 60/60 - D Loss: 1.5706127420855864e-06, G Loss: 0.1906837373971939 - 17.67 s\n",
      "Epoch 2794/3000, 60/60 - D Loss: 1.6947425418578632e-06, G Loss: 0.1886521726846695 - 17.46 s\n",
      "Epoch 2795/3000, 60/60 - D Loss: 1.4565293628976406e-06, G Loss: 0.18769361078739166 - 17.60 s\n",
      "Epoch 2796/3000, 60/60 - D Loss: 1.2821742014779147e-06, G Loss: 0.18890346586704254 - 17.55 s\n",
      "Epoch 2797/3000, 60/60 - D Loss: 1.333262069636021e-06, G Loss: 0.18600128591060638 - 17.85 s\n",
      "Epoch 2798/3000, 60/60 - D Loss: 1.2218283131915086e-06, G Loss: 0.18780674040317535 - 17.73 s\n",
      "Epoch 2799/3000, 60/60 - D Loss: 1.1240508159815477e-06, G Loss: 0.1871953308582306 - 17.71 s\n",
      "Epoch 2800/3000, 60/60 - D Loss: 1.1481178887606802e-06, G Loss: 0.18800869584083557 - 17.52 s\n",
      "Epoch 2801/3000, 60/60 - D Loss: 1.305411117868971e-06, G Loss: 0.18948210775852203 - 17.77 s\n",
      "Epoch 2802/3000, 60/60 - D Loss: 1.216272068838009e-06, G Loss: 0.18876314163208008 - 17.78 s\n",
      "Epoch 2803/3000, 60/60 - D Loss: 1.0618957482222413e-06, G Loss: 0.18799926340579987 - 17.80 s\n",
      "Epoch 2804/3000, 60/60 - D Loss: 1.1977918878969263e-06, G Loss: 0.18732640147209167 - 17.65 s\n",
      "Epoch 2805/3000, 60/60 - D Loss: 1.1169352269746469e-06, G Loss: 0.18942496180534363 - 17.59 s\n",
      "Epoch 2806/3000, 60/60 - D Loss: 1.1305030360730939e-06, G Loss: 0.18845194578170776 - 17.74 s\n",
      "Epoch 2807/3000, 60/60 - D Loss: 1.0918590040986942e-06, G Loss: 0.18706542253494263 - 17.60 s\n",
      "Epoch 2808/3000, 60/60 - D Loss: 8.941008395879635e-07, G Loss: 0.18940214812755585 - 17.67 s\n",
      "Epoch 2809/3000, 60/60 - D Loss: 7.496855491950782e-07, G Loss: 0.18892116844654083 - 17.61 s\n",
      "Epoch 2810/3000, 60/60 - D Loss: 7.411914522137408e-07, G Loss: 0.18792909383773804 - 17.80 s\n",
      "Epoch 2811/3000, 60/60 - D Loss: 6.437148210025612e-07, G Loss: 0.1860988289117813 - 17.95 s\n",
      "Epoch 2812/3000, 60/60 - D Loss: 6.646448236224956e-07, G Loss: 0.1881871074438095 - 17.89 s\n",
      "Epoch 2813/3000, 60/60 - D Loss: 6.876234173169848e-07, G Loss: 0.19269602000713348 - 17.48 s\n",
      "Epoch 2814/3000, 60/60 - D Loss: 6.057804138315248e-07, G Loss: 0.18763235211372375 - 17.81 s\n",
      "Epoch 2815/3000, 60/60 - D Loss: 5.675614485056713e-07, G Loss: 0.18656502664089203 - 17.49 s\n",
      "Epoch 2816/3000, 60/60 - D Loss: 5.068327744162104e-07, G Loss: 0.18839025497436523 - 17.57 s\n",
      "Epoch 2817/3000, 60/60 - D Loss: 4.685810512938815e-07, G Loss: 0.1898658722639084 - 17.73 s\n",
      "Epoch 2818/3000, 60/60 - D Loss: 4.302966982727696e-07, G Loss: 0.19074684381484985 - 18.10 s\n",
      "Epoch 2819/3000, 60/60 - D Loss: 5.154472177970604e-07, G Loss: 0.187998965382576 - 17.86 s\n",
      "Epoch 2820/3000, 60/60 - D Loss: 3.9312877397890134e-07, G Loss: 0.18839505314826965 - 17.64 s\n",
      "Epoch 2821/3000, 60/60 - D Loss: 3.6904008116067644e-07, G Loss: 0.18789167702198029 - 17.68 s\n",
      "Epoch 2822/3000, 60/60 - D Loss: 4.483569568947132e-07, G Loss: 0.18788596987724304 - 17.74 s\n",
      "Epoch 2823/3000, 60/60 - D Loss: 5.002992209401924e-07, G Loss: 0.18780729174613953 - 17.66 s\n",
      "Epoch 2824/3000, 60/60 - D Loss: 4.7246735102035586e-07, G Loss: 0.1880810260772705 - 17.00 s\n",
      "Epoch 2825/3000, 60/60 - D Loss: 3.936306802732048e-07, G Loss: 0.18624305725097656 - 17.46 s\n",
      "Epoch 2826/3000, 60/60 - D Loss: 4.778923665682555e-07, G Loss: 0.18808917701244354 - 17.74 s\n",
      "Epoch 2827/3000, 60/60 - D Loss: 3.321317763038678e-07, G Loss: 0.189256489276886 - 17.42 s\n",
      "Epoch 2828/3000, 60/60 - D Loss: 4.81135459584614e-07, G Loss: 0.1869439333677292 - 17.72 s\n",
      "Epoch 2829/3000, 60/60 - D Loss: 2.737417429399258e-07, G Loss: 0.18796835839748383 - 17.69 s\n",
      "Epoch 2830/3000, 60/60 - D Loss: 3.24677300822529e-07, G Loss: 0.18930907547473907 - 17.80 s\n",
      "Epoch 2831/3000, 60/60 - D Loss: 2.6613117143359233e-07, G Loss: 0.18786980211734772 - 17.64 s\n",
      "Epoch 2832/3000, 60/60 - D Loss: 3.0921729488042615e-07, G Loss: 0.18789881467819214 - 17.65 s\n",
      "Epoch 2833/3000, 60/60 - D Loss: 3.7397338537026314e-07, G Loss: 0.18747669458389282 - 17.88 s\n",
      "Epoch 2834/3000, 60/60 - D Loss: 4.0248931232333973e-07, G Loss: 0.1876477152109146 - 16.61 s\n",
      "Epoch 2835/3000, 60/60 - D Loss: 2.6769940686399707e-07, G Loss: 0.18816788494586945 - 17.80 s\n",
      "Epoch 2836/3000, 60/60 - D Loss: 3.497465184132409e-07, G Loss: 0.1877005249261856 - 17.52 s\n",
      "Epoch 2837/3000, 60/60 - D Loss: 2.63392123112105e-07, G Loss: 0.18773601949214935 - 17.78 s\n",
      "Epoch 2838/3000, 60/60 - D Loss: 2.700867217653169e-07, G Loss: 0.18870258331298828 - 17.69 s\n",
      "Epoch 2839/3000, 60/60 - D Loss: 2.984140543560443e-07, G Loss: 0.187957763671875 - 17.93 s\n",
      "Epoch 2840/3000, 60/60 - D Loss: 2.919209945440038e-07, G Loss: 0.18764543533325195 - 17.86 s\n",
      "Epoch 2841/3000, 60/60 - D Loss: 2.900068310496273e-07, G Loss: 0.18836243450641632 - 17.76 s\n",
      "Epoch 2842/3000, 60/60 - D Loss: 2.692483459521e-07, G Loss: 0.1944112479686737 - 17.75 s\n",
      "Epoch 2843/3000, 60/60 - D Loss: 2.2798134324819586e-07, G Loss: 0.18811720609664917 - 17.51 s\n",
      "Epoch 2844/3000, 60/60 - D Loss: 1.9378518378854181e-07, G Loss: 0.1878594011068344 - 17.26 s\n",
      "Epoch 2845/3000, 60/60 - D Loss: 1.7832073470613885e-07, G Loss: 0.19072338938713074 - 18.26 s\n",
      "Epoch 2846/3000, 60/60 - D Loss: 1.6223304555307294e-07, G Loss: 0.18998704850673676 - 17.68 s\n",
      "Epoch 2847/3000, 60/60 - D Loss: 1.625346154054963e-07, G Loss: 0.1884930282831192 - 17.72 s\n",
      "Epoch 2848/3000, 60/60 - D Loss: 1.5297545141734759e-07, G Loss: 0.18784572184085846 - 17.80 s\n",
      "Epoch 2849/3000, 60/60 - D Loss: 1.758588418498963e-07, G Loss: 0.188066303730011 - 17.72 s\n",
      "Epoch 2850/3000, 60/60 - D Loss: 1.3537734366853016e-07, G Loss: 0.18799401819705963 - 17.58 s\n",
      "Epoch 2851/3000, 60/60 - D Loss: 2.3322294595562454e-07, G Loss: 0.1882036030292511 - 17.61 s\n",
      "Epoch 2852/3000, 60/60 - D Loss: 1.7205170074752196e-07, G Loss: 0.18864580988883972 - 17.53 s\n",
      "Epoch 2853/3000, 60/60 - D Loss: 1.6513081310676192e-07, G Loss: 0.18766282498836517 - 17.56 s\n",
      "Epoch 2854/3000, 60/60 - D Loss: 1.279704044121477e-07, G Loss: 0.1877087950706482 - 17.63 s\n",
      "Epoch 2855/3000, 60/60 - D Loss: 1.3279718788505868e-07, G Loss: 0.18844841420650482 - 17.64 s\n",
      "Epoch 2856/3000, 60/60 - D Loss: 1.1285335649313666e-07, G Loss: 0.1889120489358902 - 17.81 s\n",
      "Epoch 2857/3000, 60/60 - D Loss: 8.221843877942447e-08, G Loss: 0.18781138956546783 - 17.78 s\n",
      "Epoch 2858/3000, 60/60 - D Loss: 9.849515555937494e-08, G Loss: 0.18836712837219238 - 17.66 s\n",
      "Epoch 2859/3000, 60/60 - D Loss: 1.0301954597030649e-07, G Loss: 0.188295379281044 - 17.49 s\n",
      "Epoch 2860/3000, 60/60 - D Loss: 1.0332084351277748e-07, G Loss: 0.1882777214050293 - 17.57 s\n",
      "Epoch 2861/3000, 60/60 - D Loss: 1.0656465727898468e-07, G Loss: 0.1860039234161377 - 17.54 s\n",
      "Epoch 2862/3000, 60/60 - D Loss: 7.971374179488455e-08, G Loss: 0.1956891566514969 - 17.63 s\n",
      "Epoch 2863/3000, 60/60 - D Loss: 7.533824299479908e-08, G Loss: 0.18779152631759644 - 17.52 s\n",
      "Epoch 2864/3000, 60/60 - D Loss: 7.761361042052384e-08, G Loss: 0.18852798640727997 - 17.93 s\n",
      "Epoch 2865/3000, 60/60 - D Loss: 8.790689774718086e-08, G Loss: 0.18738050758838654 - 17.58 s\n",
      "Epoch 2866/3000, 60/60 - D Loss: 9.218097882929266e-08, G Loss: 0.1875482201576233 - 17.70 s\n",
      "Epoch 2867/3000, 60/60 - D Loss: 7.407183825677843e-08, G Loss: 0.1881484091281891 - 17.69 s\n",
      "Epoch 2868/3000, 60/60 - D Loss: 9.36387034196351e-08, G Loss: 0.18721190094947815 - 17.75 s\n",
      "Epoch 2869/3000, 60/60 - D Loss: 5.109927840585282e-08, G Loss: 0.18797117471694946 - 17.68 s\n",
      "Epoch 2870/3000, 60/60 - D Loss: 8.051685345539239e-08, G Loss: 0.1876915991306305 - 17.61 s\n",
      "Epoch 2871/3000, 60/60 - D Loss: 4.960236772016957e-08, G Loss: 0.18786169588565826 - 17.39 s\n",
      "Epoch 2872/3000, 60/60 - D Loss: 4.9835808042797414e-08, G Loss: 0.18855926394462585 - 17.56 s\n",
      "Epoch 2873/3000, 60/60 - D Loss: 4.964163887247186e-08, G Loss: 0.18764260411262512 - 17.89 s\n",
      "Epoch 2874/3000, 60/60 - D Loss: 6.048029046502867e-08, G Loss: 0.1878792643547058 - 17.75 s\n",
      "Epoch 2875/3000, 60/60 - D Loss: 3.847703849112205e-08, G Loss: 0.18825967609882355 - 17.47 s\n",
      "Epoch 2876/3000, 60/60 - D Loss: 4.217790447111347e-08, G Loss: 0.18915846943855286 - 17.65 s\n",
      "Epoch 2877/3000, 60/60 - D Loss: 3.96291666114601e-08, G Loss: 0.18734049797058105 - 17.84 s\n",
      "Epoch 2878/3000, 60/60 - D Loss: 3.866209605414561e-08, G Loss: 0.18846143782138824 - 17.74 s\n",
      "Epoch 2879/3000, 60/60 - D Loss: 4.495109420278587e-08, G Loss: 0.18899938464164734 - 17.94 s\n",
      "Epoch 2880/3000, 60/60 - D Loss: 2.784489388195366e-08, G Loss: 0.18927296996116638 - 17.76 s\n",
      "Epoch 2881/3000, 60/60 - D Loss: 2.9200861314908344e-08, G Loss: 0.18765957653522491 - 17.75 s\n",
      "Epoch 2882/3000, 60/60 - D Loss: 2.8859401798864148e-08, G Loss: 0.18990777432918549 - 17.65 s\n",
      "Epoch 2883/3000, 60/60 - D Loss: 2.573484307426266e-08, G Loss: 0.18803271651268005 - 17.80 s\n",
      "Epoch 2884/3000, 60/60 - D Loss: 2.9570792921089495e-08, G Loss: 0.1883200854063034 - 17.58 s\n",
      "Epoch 2885/3000, 60/60 - D Loss: 3.398422673307755e-08, G Loss: 0.18841670453548431 - 17.91 s\n",
      "Epoch 2886/3000, 60/60 - D Loss: 3.3367479087935414e-08, G Loss: 0.18817466497421265 - 17.75 s\n",
      "Epoch 2887/3000, 60/60 - D Loss: 3.109744920338359e-08, G Loss: 0.18852604925632477 - 17.75 s\n",
      "Epoch 2888/3000, 60/60 - D Loss: 2.132011764550447e-08, G Loss: 0.18775254487991333 - 17.69 s\n",
      "Epoch 2889/3000, 60/60 - D Loss: 2.6142129783395668e-08, G Loss: 0.1878812164068222 - 17.81 s\n",
      "Epoch 2890/3000, 60/60 - D Loss: 1.63587480675291e-08, G Loss: 0.18723399937152863 - 17.63 s\n",
      "Epoch 2891/3000, 60/60 - D Loss: 2.030899183807361e-08, G Loss: 0.18789887428283691 - 17.63 s\n",
      "Epoch 2892/3000, 60/60 - D Loss: 1.7092452691928883e-08, G Loss: 0.19077032804489136 - 17.50 s\n",
      "Epoch 2893/3000, 60/60 - D Loss: 1.8370964726451694e-08, G Loss: 0.18819890916347504 - 17.91 s\n",
      "Epoch 2894/3000, 60/60 - D Loss: 1.4919467000154216e-08, G Loss: 0.1910853087902069 - 18.05 s\n",
      "Epoch 2895/3000, 60/60 - D Loss: 1.0771806818916929e-08, G Loss: 0.1878967434167862 - 17.58 s\n",
      "Epoch 2896/3000, 60/60 - D Loss: 1.0130503167457908e-08, G Loss: 0.18785709142684937 - 17.58 s\n",
      "Epoch 2897/3000, 60/60 - D Loss: 1.330541791349539e-08, G Loss: 0.18795695900917053 - 17.55 s\n",
      "Epoch 2898/3000, 60/60 - D Loss: 1.1500986871193497e-08, G Loss: 0.1882397085428238 - 17.56 s\n",
      "Epoch 2899/3000, 60/60 - D Loss: 1.0448002760047106e-08, G Loss: 0.18868468701839447 - 17.67 s\n",
      "Epoch 2900/3000, 60/60 - D Loss: 9.6328053816961e-09, G Loss: 0.1874101161956787 - 17.54 s\n",
      "Epoch 2901/3000, 60/60 - D Loss: 6.514365407999864e-09, G Loss: 0.18834909796714783 - 17.56 s\n",
      "Epoch 2902/3000, 60/60 - D Loss: 6.736049002224395e-09, G Loss: 0.1881020963191986 - 17.69 s\n",
      "Epoch 2903/3000, 60/60 - D Loss: 7.401257203680858e-09, G Loss: 0.1878446489572525 - 17.76 s\n",
      "Epoch 2904/3000, 60/60 - D Loss: 6.412161813020359e-09, G Loss: 0.18795283138751984 - 17.66 s\n",
      "Epoch 2905/3000, 60/60 - D Loss: 9.049255275239004e-09, G Loss: 0.18957734107971191 - 17.65 s\n",
      "Epoch 2906/3000, 60/60 - D Loss: 4.920774721722809e-09, G Loss: 0.187601238489151 - 17.62 s\n",
      "Epoch 2907/3000, 60/60 - D Loss: 6.357663907699887e-09, G Loss: 0.18781064450740814 - 17.58 s\n",
      "Epoch 2908/3000, 60/60 - D Loss: 6.441430980188428e-09, G Loss: 0.18815694749355316 - 17.90 s\n",
      "Epoch 2909/3000, 60/60 - D Loss: 4.357306460632017e-09, G Loss: 0.18810592591762543 - 17.93 s\n",
      "Epoch 2910/3000, 60/60 - D Loss: 5.533689539580222e-09, G Loss: 0.18767043948173523 - 17.61 s\n",
      "Epoch 2911/3000, 60/60 - D Loss: 5.413334824453872e-09, G Loss: 0.1894378960132599 - 17.78 s\n",
      "Epoch 2912/3000, 60/60 - D Loss: 4.8407054873854036e-09, G Loss: 0.18805544078350067 - 17.67 s\n",
      "Epoch 2913/3000, 60/60 - D Loss: 5.3086931011076324e-09, G Loss: 0.18773455917835236 - 17.68 s\n",
      "Epoch 2914/3000, 60/60 - D Loss: 4.456524191830905e-09, G Loss: 0.18837027251720428 - 17.70 s\n",
      "Epoch 2915/3000, 60/60 - D Loss: 4.21069795935973e-09, G Loss: 0.18960189819335938 - 17.65 s\n",
      "Epoch 2916/3000, 60/60 - D Loss: 5.612380099128279e-09, G Loss: 0.18791453540325165 - 17.66 s\n",
      "Epoch 2917/3000, 60/60 - D Loss: 5.416286871860309e-09, G Loss: 0.18735656142234802 - 17.72 s\n",
      "Epoch 2918/3000, 60/60 - D Loss: 3.684829079668092e-09, G Loss: 0.18915706872940063 - 17.72 s\n",
      "Epoch 2919/3000, 60/60 - D Loss: 3.6502142111336476e-09, G Loss: 0.1880638748407364 - 17.44 s\n",
      "Epoch 2920/3000, 60/60 - D Loss: 3.770648656698829e-09, G Loss: 0.18876738846302032 - 17.55 s\n",
      "Epoch 2921/3000, 60/60 - D Loss: 3.3576498655811093e-09, G Loss: 0.18860679864883423 - 17.86 s\n",
      "Epoch 2922/3000, 60/60 - D Loss: 3.047715528136253e-09, G Loss: 0.18927572667598724 - 17.58 s\n",
      "Epoch 2923/3000, 60/60 - D Loss: 2.117228979240998e-09, G Loss: 0.18843604624271393 - 17.62 s\n",
      "Epoch 2924/3000, 60/60 - D Loss: 3.314731377373299e-09, G Loss: 0.18678860366344452 - 17.62 s\n",
      "Epoch 2925/3000, 60/60 - D Loss: 2.2083523511370537e-09, G Loss: 0.18848837912082672 - 17.96 s\n",
      "Epoch 2926/3000, 60/60 - D Loss: 3.727524848300915e-09, G Loss: 0.18753866851329803 - 17.55 s\n",
      "Epoch 2927/3000, 60/60 - D Loss: 1.9825140604417656e-09, G Loss: 0.1881321221590042 - 17.54 s\n",
      "Epoch 2928/3000, 60/60 - D Loss: 1.7291364376849448e-09, G Loss: 0.1874108910560608 - 17.60 s\n",
      "Epoch 2929/3000, 60/60 - D Loss: 1.6622710685134864e-09, G Loss: 0.18817634880542755 - 17.54 s\n",
      "Epoch 2930/3000, 60/60 - D Loss: 1.899992387813937e-09, G Loss: 0.18735437095165253 - 17.53 s\n",
      "Epoch 2931/3000, 60/60 - D Loss: 2.428661641695924e-09, G Loss: 0.187811478972435 - 17.79 s\n",
      "Epoch 2932/3000, 60/60 - D Loss: 2.5961686228820422e-09, G Loss: 0.1910250037908554 - 17.64 s\n",
      "Epoch 2933/3000, 60/60 - D Loss: 2.0767172484437418e-09, G Loss: 0.18879953026771545 - 17.53 s\n",
      "Epoch 2934/3000, 60/60 - D Loss: 2.2256686880051547e-09, G Loss: 0.1874251812696457 - 17.73 s\n",
      "Epoch 2935/3000, 60/60 - D Loss: 2.1202061102067963e-09, G Loss: 0.18856802582740784 - 17.66 s\n",
      "Epoch 2936/3000, 60/60 - D Loss: 1.6525031451184412e-09, G Loss: 0.1889425814151764 - 17.72 s\n",
      "Epoch 2937/3000, 60/60 - D Loss: 1.44077050337726e-09, G Loss: 0.1878376007080078 - 17.82 s\n",
      "Epoch 2938/3000, 60/60 - D Loss: 1.8787809122312264e-09, G Loss: 0.18945330381393433 - 17.72 s\n",
      "Epoch 2939/3000, 60/60 - D Loss: 1.7887881582597572e-09, G Loss: 0.18792293965816498 - 17.75 s\n",
      "Epoch 2940/3000, 60/60 - D Loss: 1.5324719287678726e-09, G Loss: 0.18913912773132324 - 17.86 s\n",
      "Epoch 2941/3000, 60/60 - D Loss: 1.2129110891112745e-09, G Loss: 0.18790079653263092 - 17.68 s\n",
      "Epoch 2942/3000, 60/60 - D Loss: 1.2109764757734193e-09, G Loss: 0.1878560334444046 - 17.61 s\n",
      "Epoch 2943/3000, 60/60 - D Loss: 9.533570226799093e-10, G Loss: 0.18706734478473663 - 17.65 s\n",
      "Epoch 2944/3000, 60/60 - D Loss: 8.638246094922158e-10, G Loss: 0.18810300529003143 - 17.82 s\n",
      "Epoch 2945/3000, 60/60 - D Loss: 6.709091390015814e-10, G Loss: 0.1878097951412201 - 17.64 s\n",
      "Epoch 2946/3000, 60/60 - D Loss: 8.494905797404913e-10, G Loss: 0.18799683451652527 - 17.68 s\n",
      "Epoch 2947/3000, 60/60 - D Loss: 7.061931957651614e-10, G Loss: 0.1878180056810379 - 17.35 s\n",
      "Epoch 2948/3000, 60/60 - D Loss: 5.67628871979268e-10, G Loss: 0.19139441847801208 - 17.86 s\n",
      "Epoch 2949/3000, 60/60 - D Loss: 5.912770085830452e-10, G Loss: 0.18895050883293152 - 17.65 s\n",
      "Epoch 2950/3000, 60/60 - D Loss: 7.346324459197554e-10, G Loss: 0.18711549043655396 - 17.80 s\n",
      "Epoch 2951/3000, 60/60 - D Loss: 5.64060426404972e-10, G Loss: 0.1871042400598526 - 17.63 s\n",
      "Epoch 2952/3000, 60/60 - D Loss: 6.1166547689131e-10, G Loss: 0.18771639466285706 - 17.93 s\n",
      "Epoch 2953/3000, 60/60 - D Loss: 4.5830144870858e-10, G Loss: 0.18809406459331512 - 17.66 s\n",
      "Epoch 2954/3000, 60/60 - D Loss: 4.791229475203896e-10, G Loss: 0.18926803767681122 - 17.75 s\n",
      "Epoch 2955/3000, 60/60 - D Loss: 4.4949576426226595e-10, G Loss: 0.18743272125720978 - 18.05 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2956/3000, 60/60 - D Loss: 4.829917057191176e-10, G Loss: 0.18752281367778778 - 17.59 s\n",
      "Epoch 2957/3000, 60/60 - D Loss: 4.0153764138350597e-10, G Loss: 0.18843625485897064 - 17.76 s\n",
      "Epoch 2958/3000, 60/60 - D Loss: 3.840925172215331e-10, G Loss: 0.18851333856582642 - 17.60 s\n",
      "Epoch 2959/3000, 60/60 - D Loss: 4.714592494174826e-10, G Loss: 0.18865086138248444 - 17.73 s\n",
      "Epoch 2960/3000, 60/60 - D Loss: 3.6072286139995924e-10, G Loss: 0.1886470913887024 - 17.71 s\n",
      "Epoch 2961/3000, 60/60 - D Loss: 3.416602338960267e-10, G Loss: 0.18838991224765778 - 17.75 s\n",
      "Epoch 2962/3000, 60/60 - D Loss: 4.0412792787574933e-10, G Loss: 0.18704207241535187 - 17.78 s\n",
      "Epoch 2963/3000, 60/60 - D Loss: 5.691701482038903e-10, G Loss: 0.19090063869953156 - 17.85 s\n",
      "Epoch 2964/3000, 60/60 - D Loss: 3.3495528893445753e-10, G Loss: 0.18953533470630646 - 17.71 s\n",
      "Epoch 2965/3000, 60/60 - D Loss: 2.7626952320549346e-10, G Loss: 0.1872435212135315 - 17.76 s\n",
      "Epoch 2966/3000, 60/60 - D Loss: 3.5214141773543147e-10, G Loss: 0.18784521520137787 - 17.72 s\n",
      "Epoch 2967/3000, 60/60 - D Loss: 3.713496864466898e-10, G Loss: 0.1880454570055008 - 17.72 s\n",
      "Epoch 2968/3000, 60/60 - D Loss: 2.3817384735725245e-10, G Loss: 0.18805952370166779 - 17.80 s\n",
      "Epoch 2969/3000, 60/60 - D Loss: 3.407298971083299e-10, G Loss: 0.18753528594970703 - 17.62 s\n",
      "Epoch 2970/3000, 60/60 - D Loss: 2.5790271500654704e-10, G Loss: 0.18786214292049408 - 17.87 s\n",
      "Epoch 2971/3000, 60/60 - D Loss: 2.818151466646211e-10, G Loss: 0.1879233419895172 - 17.63 s\n",
      "Epoch 2972/3000, 60/60 - D Loss: 4.205222518035598e-10, G Loss: 0.18967120349407196 - 17.73 s\n",
      "Epoch 2973/3000, 60/60 - D Loss: 2.6989105133863063e-10, G Loss: 0.1881166398525238 - 17.66 s\n",
      "Epoch 2974/3000, 60/60 - D Loss: 2.48333767111478e-10, G Loss: 0.18875986337661743 - 17.76 s\n",
      "Epoch 2975/3000, 60/60 - D Loss: 2.0044388337492967e-10, G Loss: 0.1894543170928955 - 17.64 s\n",
      "Epoch 2976/3000, 60/60 - D Loss: 2.511442311379665e-10, G Loss: 0.18787099421024323 - 17.55 s\n",
      "Epoch 2977/3000, 60/60 - D Loss: 2.72411512539424e-10, G Loss: 0.18916967511177063 - 17.49 s\n",
      "Epoch 2978/3000, 60/60 - D Loss: 3.0859611724807064e-10, G Loss: 0.18771250545978546 - 17.68 s\n",
      "Epoch 2979/3000, 60/60 - D Loss: 2.8043129957827027e-10, G Loss: 0.18770626187324524 - 17.60 s\n",
      "Epoch 2980/3000, 60/60 - D Loss: 1.979189146396031e-10, G Loss: 0.19109319150447845 - 17.89 s\n",
      "Epoch 2981/3000, 60/60 - D Loss: 1.8640876141045982e-10, G Loss: 0.1858690232038498 - 17.82 s\n",
      "Epoch 2982/3000, 60/60 - D Loss: 1.9979956446807706e-10, G Loss: 0.18802787363529205 - 17.66 s\n",
      "Epoch 2983/3000, 60/60 - D Loss: 2.022496038891957e-10, G Loss: 0.18735487759113312 - 17.86 s\n",
      "Epoch 2984/3000, 60/60 - D Loss: 1.7711196570426942e-10, G Loss: 0.1878148764371872 - 17.65 s\n",
      "Epoch 2985/3000, 60/60 - D Loss: 1.752139317144226e-10, G Loss: 0.18800167739391327 - 18.07 s\n",
      "Epoch 2986/3000, 60/60 - D Loss: 1.3919750267170128e-10, G Loss: 0.18983247876167297 - 17.57 s\n",
      "Epoch 2987/3000, 60/60 - D Loss: 1.805045540967875e-10, G Loss: 0.18757177889347076 - 17.56 s\n",
      "Epoch 2988/3000, 60/60 - D Loss: 1.4579741550715334e-10, G Loss: 0.18760976195335388 - 17.79 s\n",
      "Epoch 2989/3000, 60/60 - D Loss: 2.1453893900031438e-10, G Loss: 0.18743132054805756 - 17.62 s\n",
      "Epoch 2990/3000, 60/60 - D Loss: 1.3787432496923653e-10, G Loss: 0.18874602019786835 - 17.26 s\n",
      "Epoch 2991/3000, 60/60 - D Loss: 1.3371550218171207e-10, G Loss: 0.1881422996520996 - 17.08 s\n",
      "Epoch 2992/3000, 60/60 - D Loss: 1.0356409859248319e-10, G Loss: 0.18819576501846313 - 17.79 s\n",
      "Epoch 2993/3000, 60/60 - D Loss: 1.2774472607405894e-10, G Loss: 0.1878442019224167 - 17.69 s\n",
      "Epoch 2994/3000, 60/60 - D Loss: 9.51847949518635e-11, G Loss: 0.18638405203819275 - 17.84 s\n",
      "Epoch 2995/3000, 60/60 - D Loss: 1.1252689800455464e-10, G Loss: 0.18743996322155 - 17.44 s\n",
      "Epoch 2996/3000, 60/60 - D Loss: 1.0307657374025042e-10, G Loss: 0.1882704347372055 - 17.50 s\n",
      "Epoch 2997/3000, 60/60 - D Loss: 1.4897886719382345e-10, G Loss: 0.18887290358543396 - 17.78 s\n",
      "Epoch 2998/3000, 60/60 - D Loss: 1.5092727450663513e-10, G Loss: 0.1891363561153412 - 17.66 s\n",
      "Epoch 2999/3000, 60/60 - D Loss: 1.1313492328455264e-10, G Loss: 0.1875431090593338 - 17.64 s\n",
      "Epoch 3000/3000, 60/60 - D Loss: 1.4814798478333428e-10, G Loss: 0.18854910135269165 - 17.54 s\n",
      "Saved model at epoch 3000 to saved_mode\\model_3000.h5\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Training loop\n",
    "epochs = 200\n",
    "batch_size = 64\n",
    "\n",
    "total_batches = int(np.ceil(len(X_train_noisy) / batch_size))\n",
    "\n",
    "# Create a directory to save the models\n",
    "save_dir = \"saved_mode\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    start_time = time.time()\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\", end=' ')\n",
    "\n",
    "    for i in range(0, len(X_train_noisy), batch_size):\n",
    "        current_batch_size = min(batch_size, len(X_train_noisy) - i)\n",
    "\n",
    "        # Train the discriminator\n",
    "        real_data = y_train_clean[i:i + current_batch_size]\n",
    "        real_labels = tf.ones((current_batch_size, 1))\n",
    "        d_loss_real = discriminator.train_on_batch(real_data, real_labels)\n",
    "\n",
    "        generated_data = generator.predict(X_train_noisy[i:i + current_batch_size])\n",
    "        generate_labels = tf.zeros((current_batch_size, 1))\n",
    "        d_loss_generate = discriminator.train_on_batch(generated_data, generate_labels)\n",
    "\n",
    "        # Calculate total discriminator loss\n",
    "        d_loss = 0.5 * np.add(d_loss_real, d_loss_generate)\n",
    "\n",
    "        # Train the generator\n",
    "        g_loss = generator.train_on_batch(X_train_noisy[i:i + current_batch_size], y_train_clean[i:i + current_batch_size])\n",
    "\n",
    "        print(f\"\\rEpoch {epoch + 1}/{epochs}, {i//batch_size + 1}/{total_batches} - D Loss: {d_loss}, G Loss: {g_loss}\", end='')\n",
    "        \n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\" - {elapsed_time:.2f} s\")\n",
    "    \n",
    "    # Save the model starting from epoch 1500 and every 500 epochs thereafter\n",
    "    if (epoch + 1) >= 100 and (epoch + 1) % 50 == 0:\n",
    "        model_name = f'model_{epoch + 1}.h5'\n",
    "        model_path = os.path.join(save_dir, model_name)\n",
    "        generator.save(model_path)\n",
    "        print(f\"Saved model at epoch {epoch + 1} to {model_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc5ce0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fgtg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6e42db27",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Train the model\n",
    "\n",
    "# initial_learning_rate = 0.0001  # Define your initial learning rate 0.0001  best\n",
    "# adam_optimizer = Adam(learning_rate=initial_learning_rate)\n",
    "\n",
    "# model.compile(loss='mean_squared_error', optimizer=adam_optimizer,metrics=\"accuracy\")\n",
    "\n",
    "# model.fit(X_train_noisy,y_train_clean , epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcad50c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "29e8000e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = generator.predict(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "37362646",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.576876194388265"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pred = model.predict(X_valid)\n",
    "10 * np.log10(np.sum(np.expand_dims(y_valid, axis=-1)**2) / np.sum((np.expand_dims(y_valid, axis=-1) - pred)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "88994b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# si_sdr = ScaleInvariantSignalDistortionRatio()\n",
    "# target = tensor([np.expand_dims(y_valid,axis=-1)])\n",
    "# preds = tensor([pred])\n",
    "# si_sdr(preds, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e47cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "%aug data\n",
    "9.9473 %wiithout generator\n",
    "11.4689 %with generator \n",
    "\n",
    "%without aug\n",
    "9.3017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b514e3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7e65b8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.save(\"generator_3000_wgn0.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca146e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# p = generator.predict(np.expand_dims(X_train_noisy[2][np.newaxis, :, :], axis=-1))\n",
    "# p= p.reshape((128, 64))\n",
    "# scipy.io.savemat('New_AS_002_denoise_model0001_aug_cv250_gg.mat', {'denoise_spe':p})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70dc8e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3af177",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f9bf4e08",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9db1b439",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "# Load the model from the file\n",
    "modell = load_model(\"generator_3000.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3eab2d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list to store the denoised spectrograms\n",
    "denoised_spectrograms = []\n",
    "\n",
    "# Specify the directory containing the noisy data files\n",
    "noisy_data_directory = 'test_noisy_data/'\n",
    "\n",
    "# Loop through all files in the directory\n",
    "for file_name in os.listdir(noisy_data_directory):\n",
    "    if file_name.endswith('.mat'):\n",
    "        # Load the noisy data from the MAT file\n",
    "        file_path = os.path.join(noisy_data_directory, file_name)\n",
    "        test_noisy_data = scipy.io.loadmat(file_path)['Segment_noisy']\n",
    "\n",
    "        noisy_data_real = np.real(test_noisy_data)\n",
    "        noisy_data_imag = np.imag(test_noisy_data)\n",
    "        \n",
    "        \n",
    "        # Expand dimensions\n",
    "        noisy_data_real = np.expand_dims(noisy_data_real[np.newaxis, :, :], axis=-1)\n",
    "   \n",
    "\n",
    "        # Apply the model to denoise the data\n",
    "        denoised_data_real = modell.predict(noisy_data_real)\n",
    "\n",
    "        denoised_data_real = denoised_data_real.reshape((128, 64))\n",
    "        \n",
    "        # Reshape to (128, 64)\n",
    "        #denoised_data = np.vectorize(complex)(denoised_data_real, noisy_data_imag)\n",
    "        \n",
    "\n",
    "        # Append the denoised data to the list\n",
    "        denoised_spectrograms.append(denoised_data_real)\n",
    "\n",
    "# Stack the denoised spectrograms along a new axis (4D array)\n",
    "merged_denoised_data = np.concatenate(denoised_spectrograms, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "466a094b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 320)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_denoised_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ed2e69b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.io.savemat('New_AS_002_denoise_model0001_aug_cv250.mat', {'denoise_spe': merged_denoised_data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "968a3642",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.5413128e-02,  7.7895157e-02,  1.9338399e-01, ...,\n",
       "         0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
       "       [-2.9260597e-01,  2.1329612e-01, -5.5941917e-02, ...,\n",
       "         0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
       "       [ 4.7727647e-01, -8.1839895e-01,  1.8247497e-01, ...,\n",
       "         0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
       "       ...,\n",
       "       [ 2.0739461e-05,  1.1372420e-03, -1.6909323e-03, ...,\n",
       "         0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
       "       [-3.3129403e-05, -1.3230698e-03, -1.5605466e-03, ...,\n",
       "         0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
       "       [-2.8804841e-04, -5.4520206e-04,  1.0511472e-03, ...,\n",
       "         0.0000000e+00,  0.0000000e+00,  0.0000000e+00]], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_denoised_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4946aa88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13eeeb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
